{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for a Dog Identification App \n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'(IMPLEMENTATION)'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section, and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! \n",
    "\n",
    "> **Note**: Once you have completed all of the code implementations, you need to finalize your work by exporting the iPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n",
    "\n",
    "The rubric contains _optional_ \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. If you decide to pursue the \"Stand Out Suggestions\", you should include the code in this IPython notebook.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Why We're Here \n",
    "\n",
    "In this notebook, you will make the first steps towards developing an algorithm that could be used as part of a mobile or web app.  At the end of this project, your code will accept any user-supplied image as input.  If a dog is detected in the image, it will provide an estimate of the dog's breed.  If a human is detected, it will provide an estimate of the dog breed that is most resembling.  The image below displays potential sample output of your finished project (... but we expect that each student's algorithm will behave differently!). \n",
    "\n",
    "![Sample Dog Output](images/sample_dog_output.png)\n",
    "\n",
    "In this real-world setting, you will need to piece together a series of models to perform different tasks; for instance, the algorithm that detects humans in an image will be different from the CNN that infers dog breed.  There are many points of possible failure, and no perfect algorithm exists.  Your imperfect solution will nonetheless create a fun user experience!\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step1): Detect Humans\n",
    "* [Step 2](#step2): Detect Dogs\n",
    "* [Step 3](#step3): Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "* [Step 4](#step4): Use a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 5](#step5): Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 6](#step6): Write your Algorithm\n",
    "* [Step 7](#step7): Test Your Algorithm\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n",
    "\n",
    "### Import Dog Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of dog images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `dog_names` - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\johnc\\Anaconda3\\envs\\dog-project\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Human Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of human images, where the file paths are stored in the numpy array `human_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13234 total human images.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(8675309)\n",
    "\n",
    "# load filenames in shuffled human dataset\n",
    "human_files = np.array(glob(\"lfw/*/*\"))\n",
    "random.shuffle(human_files)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total human images.' % len(human_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Detect Humans\n",
    "\n",
    "We use OpenCV's implementation of [Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) to detect human faces in images.  OpenCV provides many pre-trained face detectors, stored as XML files on [github](https://github.com/opencv/opencv/tree/master/data/haarcascades).  We have downloaded one of these detectors and stored it in the `haarcascades` directory.\n",
    "\n",
    "In the next code cell, we demonstrate how to use this detector to find human faces in a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of faces detected: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQsAAAD8CAYAAABgtYFHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE2bG4VK+TL+MAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x224bc4429b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline                               \n",
    "\n",
    "# extract pre-trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# load color (BGR) image\n",
    "img = cv2.imread(human_files[3])\n",
    "# convert BGR image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# find faces in image\n",
    "faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "# print number of faces detected in the image\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# get bounding box for each detected face\n",
    "for (x,y,w,h) in faces:\n",
    "    # add bounding box to color image\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "# convert BGR image to RGB for plotting\n",
    "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# display the image, along with bounding box\n",
    "plt.imshow(cv_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using any of the face detectors, it is standard procedure to convert the images to grayscale.  The `detectMultiScale` function executes the classifier stored in `face_cascade` and takes the grayscale image as a parameter.  \n",
    "\n",
    "In the above code, `faces` is a numpy array of detected faces, where each row corresponds to a detected face.  Each detected face is a 1D array with four entries that specifies the bounding box of the detected face.  The first two entries in the array (extracted in the above code as `x` and `y`) specify the horizontal and vertical positions of the top left corner of the bounding box.  The last two entries in the array (extracted here as `w` and `h`) specify the width and height of the box.\n",
    "\n",
    "### Write a Human Face Detector\n",
    "\n",
    "We can use this procedure to write a function that returns `True` if a human face is detected in an image and `False` otherwise.  This function, aptly named `face_detector`, takes a string-valued file path to an image as input and appears in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Human Face Detector\n",
    "\n",
    "__Question 1:__ Use the code cell below to test the performance of the `face_detector` function.  \n",
    "- What percentage of the first 100 images in `human_files` have a detected human face?  \n",
    "- What percentage of the first 100 images in `dog_files` have a detected human face? \n",
    "\n",
    "Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face.  You will see that our algorithm falls short of this goal, but still gives acceptable performance.  We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays `human_files_short` and `dog_files_short`.\n",
    "\n",
    "__Answer:__ 97% of the first 100 human images have a detected human face, and 11% of the first 100 dog images have a detected human face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97 0.11\n"
     ]
    }
   ],
   "source": [
    "human_files_short = human_files[:100]\n",
    "dog_files_short = train_files[:100]\n",
    "# Do NOT modify the code above this line.\n",
    "\n",
    "## TODO: Test the performance of the face_detector algorithm \n",
    "## on the images in human_files_short and dog_files_short.\n",
    "human_as_human = len(list(x for x in map(face_detector, human_files_short) if x))/100\n",
    "dog_as_human = len(list(x for x in map(face_detector, dog_files_short) if x))/100\n",
    "print(human_as_human, dog_as_human)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2:__ This algorithmic choice necessitates that we communicate to the user that we accept human images only when they provide a clear view of a face (otherwise, we risk having unneccessarily frustrated users!). In your opinion, is this a reasonable expectation to pose on the user? If not, can you think of a way to detect humans in images that does not necessitate an image with a clearly presented face?\n",
    "\n",
    "__Answer:__ Given that the function is only meant to detect whether a human face is present, I think this is not a reasonable expectation to pose on the user. We can further train the detector with blurred face images such that it could recognise other (more abstract) features, for example the shape of the head. \n",
    "\n",
    "We suggest the face detector from OpenCV as a potential way to detect human images in your algorithm, but you are free to explore other approaches, especially approaches that make use of deep learning :).  Please use the code cell below to design and test your own face detection algorithm.  If you decide to pursue this _optional_ task, report performance on each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (Optional) TODO: Report the performance of another  \n",
    "## face detection algorithm on the LFW dataset\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Detect Dogs\n",
    "\n",
    "In this section, we use a pre-trained [ResNet-50](http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006) model to detect dogs in images.  Our first line of code downloads the ResNet-50 model, along with weights that have been trained on [ImageNet](http://www.image-net.org/), a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  Given an image, this pre-trained ResNet-50 model returns a prediction (derived from the available categories in ImageNet) for the object that is contained in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\johnc\\Anaconda3\\envs\\dog-project\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1062: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# define ResNet50 model\n",
    "ResNet50_model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with ResNet-50\n",
    "\n",
    "Getting the 4D tensor ready for ResNet-50, and for any other pre-trained model in Keras, requires some additional processing.  First, the RGB image is converted to BGR by reordering the channels.  All pre-trained models have the additional normalization step that the mean pixel (expressed in RGB as $[103.939, 116.779, 123.68]$ and calculated from all pixels in all images in ImageNet) must be subtracted from every pixel in each image.  This is implemented in the imported function `preprocess_input`.  If you're curious, you can check the code for `preprocess_input` [here](https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py).\n",
    "\n",
    "Now that we have a way to format our image for supplying to ResNet-50, we are now ready to use the model to extract the predictions.  This is accomplished with the `predict` method, which returns an array whose $i$-th entry is the model's predicted probability that the image belongs to the $i$-th ImageNet category.  This is implemented in the `ResNet50_predict_labels` function below.\n",
    "\n",
    "By taking the argmax of the predicted probability vector, we obtain an integer corresponding to the model's predicted object class, which we can identify with an object category through the use of this [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def ResNet50_predict_labels(img_path):\n",
    "    # returns prediction vector for image located at img_path\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Dog Detector\n",
    "\n",
    "While looking at the [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from `'Chihuahua'` to `'Mexican hairless'`.  Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained ResNet-50 model, we need only check if the `ResNet50_predict_labels` function above returns a value between 151 and 268 (inclusive).\n",
    "\n",
    "We use these ideas to complete the `dog_detector` function below, which returns `True` if a dog is detected in an image (and `False` if not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Dog Detector\n",
    "\n",
    "__Question 3:__ Use the code cell below to test the performance of your `dog_detector` function.  \n",
    "- What percentage of the images in `human_files_short` have a detected dog?  \n",
    "- What percentage of the images in `dog_files_short` have a detected dog?\n",
    "\n",
    "__Answer:__ A dog is detected in 2% of the human_files_short images and 100% of the dog_files_short images respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02 1.0\n"
     ]
    }
   ],
   "source": [
    "### TODO: Test the performance of the dog_detector function\n",
    "### on the images in human_files_short and dog_files_short.\n",
    "human_as_dog = sum(1 for x in map(dog_detector, human_files_short) if x)/100\n",
    "dog_as_dog = sum(1 for x in map(dog_detector, dog_files_short) if x)/100\n",
    "print(human_as_dog, dog_as_dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "\n",
    "Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images.  In this step, you will create a CNN that classifies dog breeds.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 1%.  In Step 5 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.\n",
    "\n",
    "Be careful with adding too many trainable layers!  More parameters means longer training, which means you are more likely to need a GPU to accelerate the training process.  Thankfully, Keras provides a handy estimate of the time that each epoch is likely to take; you can extrapolate this estimate to figure out how long it will take for your algorithm to train. \n",
    "\n",
    "We mention that the task of assigning breed to dogs from images is considered exceptionally challenging.  To see why, consider that *even a human* would have great difficulty in distinguishing between a Brittany and a Welsh Springer Spaniel.  \n",
    "\n",
    "Brittany | Welsh Springer Spaniel\n",
    "- | - \n",
    "<img src=\"images/Brittany_02625.jpg\" width=\"100\"> | <img src=\"images/Welsh_springer_spaniel_08203.jpg\" width=\"200\">\n",
    "\n",
    "It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).  \n",
    "\n",
    "Curly-Coated Retriever | American Water Spaniel\n",
    "- | -\n",
    "<img src=\"images/Curly-coated_retriever_03896.jpg\" width=\"200\"> | <img src=\"images/American_water_spaniel_00648.jpg\" width=\"200\">\n",
    "\n",
    "\n",
    "Likewise, recall that labradors come in yellow, chocolate, and black.  Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.  \n",
    "\n",
    "Yellow Labrador | Chocolate Labrador | Black Labrador\n",
    "- | -\n",
    "<img src=\"images/Labrador_retriever_06457.jpg\" width=\"150\"> | <img src=\"images/Labrador_retriever_06455.jpg\" width=\"240\"> | <img src=\"images/Labrador_retriever_06449.jpg\" width=\"220\">\n",
    "\n",
    "We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.  \n",
    "\n",
    "Remember that the practice is far ahead of the theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun! \n",
    "\n",
    "### Pre-process the Data\n",
    "\n",
    "We rescale the images by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6680/6680 [00:49<00:00, 135.25it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 835/835 [00:05<00:00, 147.07it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 836/836 [00:05<00:00, 149.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        model.summary()\n",
    "\n",
    "We have imported some Python modules to get you started, but feel free to import as many modules as you need.  If you end up getting stuck, here's a hint that specifies a model that trains relatively fast on CPU and attains >1% test accuracy in 5 epochs:\n",
    "\n",
    "![Sample CNN](images/sample_cnn.png)\n",
    "           \n",
    "__Question 4:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  If you chose to use the hinted architecture above, describe why you think that CNN architecture should work well for the image classification task.\n",
    "\n",
    "__Answer:__ I started with an architecture with the one hinted above because convolutional neural networks are generally good at image classification, the convolutional layers can each specialise on recognising specific features in the images. In addition, the layers in my CNN has more filters and my CNN contains an extra layer to increase the number of trainable parameters, but at the same time have added dropout layers an an attempt to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\johnc\\Anaconda3\\envs\\dog-project\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1123: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 32)      416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 64)      8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 56, 64)        16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 32)        8224      \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 133)               4389      \n",
      "=================================================================\n",
      "Total params: 37,733.0\n",
      "Trainable params: 37,733.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu', \n",
    "                        input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.\n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/5\n",
      "6650/6680 [============================>.] - ETA: 250s - loss: 4.9059 - acc: 0.0000e+0 - ETA: 248s - loss: 4.8948 - acc: 0.0100    - ETA: 245s - loss: 4.8927 - acc: 0.006 - ETA: 241s - loss: 4.8971 - acc: 0.005 - ETA: 238s - loss: 4.8962 - acc: 0.004 - ETA: 237s - loss: 4.8968 - acc: 0.003 - ETA: 235s - loss: 4.8964 - acc: 0.002 - ETA: 233s - loss: 4.8955 - acc: 0.002 - ETA: 232s - loss: 4.8941 - acc: 0.002 - ETA: 231s - loss: 4.8931 - acc: 0.002 - ETA: 229s - loss: 4.8928 - acc: 0.005 - ETA: 226s - loss: 4.8916 - acc: 0.006 - ETA: 225s - loss: 4.8924 - acc: 0.006 - ETA: 223s - loss: 4.8927 - acc: 0.005 - ETA: 221s - loss: 4.8922 - acc: 0.008 - ETA: 219s - loss: 4.8918 - acc: 0.007 - ETA: 217s - loss: 4.8910 - acc: 0.007 - ETA: 215s - loss: 4.8915 - acc: 0.006 - ETA: 213s - loss: 4.8920 - acc: 0.006 - ETA: 212s - loss: 4.8914 - acc: 0.006 - ETA: 210s - loss: 4.8915 - acc: 0.005 - ETA: 208s - loss: 4.8914 - acc: 0.005 - ETA: 207s - loss: 4.8911 - acc: 0.005 - ETA: 205s - loss: 4.8911 - acc: 0.005 - ETA: 203s - loss: 4.8904 - acc: 0.005 - ETA: 201s - loss: 4.8905 - acc: 0.005 - ETA: 199s - loss: 4.8904 - acc: 0.005 - ETA: 197s - loss: 4.8908 - acc: 0.005 - ETA: 195s - loss: 4.8906 - acc: 0.005 - ETA: 193s - loss: 4.8905 - acc: 0.005 - ETA: 191s - loss: 4.8899 - acc: 0.005 - ETA: 190s - loss: 4.8900 - acc: 0.005 - ETA: 188s - loss: 4.8896 - acc: 0.004 - ETA: 186s - loss: 4.8900 - acc: 0.004 - ETA: 184s - loss: 4.8900 - acc: 0.004 - ETA: 182s - loss: 4.8899 - acc: 0.005 - ETA: 180s - loss: 4.8895 - acc: 0.006 - ETA: 178s - loss: 4.8896 - acc: 0.006 - ETA: 176s - loss: 4.8897 - acc: 0.007 - ETA: 174s - loss: 4.8899 - acc: 0.007 - ETA: 173s - loss: 4.8898 - acc: 0.007 - ETA: 171s - loss: 4.8896 - acc: 0.007 - ETA: 169s - loss: 4.8895 - acc: 0.007 - ETA: 167s - loss: 4.8896 - acc: 0.007 - ETA: 165s - loss: 4.8895 - acc: 0.007 - ETA: 163s - loss: 4.8895 - acc: 0.007 - ETA: 162s - loss: 4.8892 - acc: 0.006 - ETA: 160s - loss: 4.8895 - acc: 0.006 - ETA: 158s - loss: 4.8896 - acc: 0.006 - ETA: 156s - loss: 4.8894 - acc: 0.006 - ETA: 154s - loss: 4.8892 - acc: 0.006 - ETA: 152s - loss: 4.8890 - acc: 0.006 - ETA: 150s - loss: 4.8891 - acc: 0.006 - ETA: 148s - loss: 4.8890 - acc: 0.005 - ETA: 146s - loss: 4.8887 - acc: 0.006 - ETA: 144s - loss: 4.8888 - acc: 0.006 - ETA: 142s - loss: 4.8886 - acc: 0.006 - ETA: 140s - loss: 4.8889 - acc: 0.006 - ETA: 139s - loss: 4.8891 - acc: 0.006 - ETA: 137s - loss: 4.8893 - acc: 0.006 - ETA: 135s - loss: 4.8893 - acc: 0.005 - ETA: 133s - loss: 4.8892 - acc: 0.005 - ETA: 131s - loss: 4.8889 - acc: 0.005 - ETA: 129s - loss: 4.8890 - acc: 0.005 - ETA: 127s - loss: 4.8894 - acc: 0.005 - ETA: 125s - loss: 4.8895 - acc: 0.005 - ETA: 124s - loss: 4.8894 - acc: 0.005 - ETA: 122s - loss: 4.8894 - acc: 0.005 - ETA: 120s - loss: 4.8895 - acc: 0.005 - ETA: 118s - loss: 4.8893 - acc: 0.006 - ETA: 116s - loss: 4.8893 - acc: 0.006 - ETA: 114s - loss: 4.8893 - acc: 0.006 - ETA: 112s - loss: 4.8893 - acc: 0.006 - ETA: 110s - loss: 4.8894 - acc: 0.006 - ETA: 108s - loss: 4.8893 - acc: 0.006 - ETA: 107s - loss: 4.8892 - acc: 0.006 - ETA: 105s - loss: 4.8893 - acc: 0.006 - ETA: 103s - loss: 4.8894 - acc: 0.005 - ETA: 101s - loss: 4.8892 - acc: 0.006 - ETA: 99s - loss: 4.8892 - acc: 0.006 - ETA: 97s - loss: 4.8893 - acc: 0.00 - ETA: 95s - loss: 4.8893 - acc: 0.00 - ETA: 94s - loss: 4.8894 - acc: 0.00 - ETA: 92s - loss: 4.8893 - acc: 0.00 - ETA: 90s - loss: 4.8892 - acc: 0.00 - ETA: 88s - loss: 4.8892 - acc: 0.00 - ETA: 86s - loss: 4.8891 - acc: 0.00 - ETA: 84s - loss: 4.8890 - acc: 0.00 - ETA: 82s - loss: 4.8890 - acc: 0.00 - ETA: 81s - loss: 4.8889 - acc: 0.00 - ETA: 79s - loss: 4.8890 - acc: 0.00 - ETA: 77s - loss: 4.8890 - acc: 0.00 - ETA: 75s - loss: 4.8890 - acc: 0.00 - ETA: 73s - loss: 4.8891 - acc: 0.00 - ETA: 71s - loss: 4.8891 - acc: 0.00 - ETA: 69s - loss: 4.8890 - acc: 0.00 - ETA: 68s - loss: 4.8890 - acc: 0.00 - ETA: 66s - loss: 4.8889 - acc: 0.00 - ETA: 64s - loss: 4.8889 - acc: 0.00 - ETA: 62s - loss: 4.8890 - acc: 0.00 - ETA: 60s - loss: 4.8889 - acc: 0.00 - ETA: 58s - loss: 4.8889 - acc: 0.00 - ETA: 56s - loss: 4.8888 - acc: 0.00 - ETA: 55s - loss: 4.8887 - acc: 0.00 - ETA: 53s - loss: 4.8885 - acc: 0.00 - ETA: 51s - loss: 4.8884 - acc: 0.00 - ETA: 49s - loss: 4.8884 - acc: 0.00 - ETA: 47s - loss: 4.8882 - acc: 0.00 - ETA: 45s - loss: 4.8882 - acc: 0.00 - ETA: 43s - loss: 4.8883 - acc: 0.00 - ETA: 41s - loss: 4.8883 - acc: 0.00 - ETA: 40s - loss: 4.8883 - acc: 0.00 - ETA: 38s - loss: 4.8881 - acc: 0.00 - ETA: 36s - loss: 4.8880 - acc: 0.00 - ETA: 34s - loss: 4.8879 - acc: 0.00 - ETA: 32s - loss: 4.8880 - acc: 0.00 - ETA: 30s - loss: 4.8883 - acc: 0.00 - ETA: 28s - loss: 4.8883 - acc: 0.00 - ETA: 27s - loss: 4.8882 - acc: 0.00 - ETA: 25s - loss: 4.8881 - acc: 0.00 - ETA: 23s - loss: 4.8881 - acc: 0.00 - ETA: 21s - loss: 4.8882 - acc: 0.00 - ETA: 19s - loss: 4.8882 - acc: 0.00 - ETA: 17s - loss: 4.8881 - acc: 0.00 - ETA: 15s - loss: 4.8881 - acc: 0.00 - ETA: 14s - loss: 4.8881 - acc: 0.00 - ETA: 12s - loss: 4.8880 - acc: 0.00 - ETA: 10s - loss: 4.8881 - acc: 0.00 - ETA: 8s - loss: 4.8881 - acc: 0.0068 - ETA: 6s - loss: 4.8882 - acc: 0.006 - ETA: 4s - loss: 4.8881 - acc: 0.006 - ETA: 2s - loss: 4.8880 - acc: 0.007 - ETA: 1s - loss: 4.8880 - acc: 0.0072Epoch 00000: val_loss improved from inf to 4.88086, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 259s - loss: 4.8879 - acc: 0.0072 - val_loss: 4.8809 - val_acc: 0.0096\n",
      "Epoch 2/5\n",
      "6650/6680 [============================>.] - ETA: 237s - loss: 4.8561 - acc: 0.020 - ETA: 232s - loss: 4.8592 - acc: 0.010 - ETA: 232s - loss: 4.8804 - acc: 0.006 - ETA: 234s - loss: 4.8824 - acc: 0.010 - ETA: 232s - loss: 4.8801 - acc: 0.012 - ETA: 231s - loss: 4.8779 - acc: 0.010 - ETA: 227s - loss: 4.8792 - acc: 0.011 - ETA: 226s - loss: 4.8780 - acc: 0.010 - ETA: 223s - loss: 4.8781 - acc: 0.008 - ETA: 220s - loss: 4.8769 - acc: 0.008 - ETA: 219s - loss: 4.8795 - acc: 0.007 - ETA: 217s - loss: 4.8803 - acc: 0.006 - ETA: 215s - loss: 4.8803 - acc: 0.006 - ETA: 213s - loss: 4.8814 - acc: 0.005 - ETA: 211s - loss: 4.8834 - acc: 0.005 - ETA: 210s - loss: 4.8827 - acc: 0.005 - ETA: 208s - loss: 4.8832 - acc: 0.005 - ETA: 206s - loss: 4.8830 - acc: 0.005 - ETA: 204s - loss: 4.8828 - acc: 0.006 - ETA: 202s - loss: 4.8837 - acc: 0.006 - ETA: 200s - loss: 4.8830 - acc: 0.005 - ETA: 198s - loss: 4.8824 - acc: 0.006 - ETA: 196s - loss: 4.8831 - acc: 0.007 - ETA: 194s - loss: 4.8826 - acc: 0.006 - ETA: 193s - loss: 4.8829 - acc: 0.006 - ETA: 191s - loss: 4.8833 - acc: 0.006 - ETA: 189s - loss: 4.8836 - acc: 0.006 - ETA: 187s - loss: 4.8836 - acc: 0.006 - ETA: 185s - loss: 4.8835 - acc: 0.006 - ETA: 183s - loss: 4.8838 - acc: 0.006 - ETA: 181s - loss: 4.8840 - acc: 0.006 - ETA: 180s - loss: 4.8837 - acc: 0.006 - ETA: 178s - loss: 4.8838 - acc: 0.006 - ETA: 176s - loss: 4.8831 - acc: 0.007 - ETA: 174s - loss: 4.8825 - acc: 0.006 - ETA: 172s - loss: 4.8819 - acc: 0.007 - ETA: 171s - loss: 4.8816 - acc: 0.007 - ETA: 169s - loss: 4.8802 - acc: 0.007 - ETA: 167s - loss: 4.8809 - acc: 0.008 - ETA: 165s - loss: 4.8805 - acc: 0.008 - ETA: 163s - loss: 4.8805 - acc: 0.008 - ETA: 162s - loss: 4.8799 - acc: 0.008 - ETA: 160s - loss: 4.8801 - acc: 0.007 - ETA: 158s - loss: 4.8799 - acc: 0.007 - ETA: 156s - loss: 4.8808 - acc: 0.007 - ETA: 154s - loss: 4.8807 - acc: 0.007 - ETA: 152s - loss: 4.8807 - acc: 0.007 - ETA: 151s - loss: 4.8810 - acc: 0.007 - ETA: 149s - loss: 4.8803 - acc: 0.006 - ETA: 147s - loss: 4.8797 - acc: 0.007 - ETA: 146s - loss: 4.8802 - acc: 0.007 - ETA: 144s - loss: 4.8800 - acc: 0.007 - ETA: 142s - loss: 4.8796 - acc: 0.007 - ETA: 140s - loss: 4.8806 - acc: 0.007 - ETA: 138s - loss: 4.8806 - acc: 0.007 - ETA: 136s - loss: 4.8805 - acc: 0.007 - ETA: 135s - loss: 4.8809 - acc: 0.007 - ETA: 133s - loss: 4.8810 - acc: 0.007 - ETA: 131s - loss: 4.8809 - acc: 0.007 - ETA: 130s - loss: 4.8812 - acc: 0.007 - ETA: 128s - loss: 4.8814 - acc: 0.007 - ETA: 126s - loss: 4.8815 - acc: 0.007 - ETA: 124s - loss: 4.8815 - acc: 0.007 - ETA: 122s - loss: 4.8811 - acc: 0.007 - ETA: 121s - loss: 4.8809 - acc: 0.007 - ETA: 119s - loss: 4.8810 - acc: 0.007 - ETA: 117s - loss: 4.8810 - acc: 0.007 - ETA: 116s - loss: 4.8810 - acc: 0.007 - ETA: 114s - loss: 4.8810 - acc: 0.007 - ETA: 112s - loss: 4.8811 - acc: 0.008 - ETA: 110s - loss: 4.8806 - acc: 0.007 - ETA: 108s - loss: 4.8805 - acc: 0.008 - ETA: 107s - loss: 4.8804 - acc: 0.008 - ETA: 105s - loss: 4.8804 - acc: 0.008 - ETA: 103s - loss: 4.8802 - acc: 0.008 - ETA: 101s - loss: 4.8804 - acc: 0.007 - ETA: 100s - loss: 4.8803 - acc: 0.008 - ETA: 98s - loss: 4.8804 - acc: 0.008 - ETA: 96s - loss: 4.8805 - acc: 0.00 - ETA: 94s - loss: 4.8804 - acc: 0.00 - ETA: 93s - loss: 4.8804 - acc: 0.00 - ETA: 91s - loss: 4.8802 - acc: 0.00 - ETA: 89s - loss: 4.8801 - acc: 0.00 - ETA: 87s - loss: 4.8801 - acc: 0.00 - ETA: 85s - loss: 4.8804 - acc: 0.00 - ETA: 84s - loss: 4.8805 - acc: 0.00 - ETA: 82s - loss: 4.8807 - acc: 0.00 - ETA: 80s - loss: 4.8811 - acc: 0.00 - ETA: 78s - loss: 4.8811 - acc: 0.00 - ETA: 77s - loss: 4.8810 - acc: 0.00 - ETA: 75s - loss: 4.8808 - acc: 0.00 - ETA: 73s - loss: 4.8809 - acc: 0.00 - ETA: 71s - loss: 4.8811 - acc: 0.00 - ETA: 70s - loss: 4.8808 - acc: 0.00 - ETA: 68s - loss: 4.8810 - acc: 0.00 - ETA: 66s - loss: 4.8806 - acc: 0.00 - ETA: 64s - loss: 4.8804 - acc: 0.00 - ETA: 62s - loss: 4.8801 - acc: 0.00 - ETA: 61s - loss: 4.8801 - acc: 0.00 - ETA: 59s - loss: 4.8800 - acc: 0.00 - ETA: 57s - loss: 4.8791 - acc: 0.00 - ETA: 55s - loss: 4.8800 - acc: 0.00 - ETA: 54s - loss: 4.8798 - acc: 0.00 - ETA: 52s - loss: 4.8798 - acc: 0.00 - ETA: 50s - loss: 4.8801 - acc: 0.00 - ETA: 48s - loss: 4.8802 - acc: 0.00 - ETA: 47s - loss: 4.8799 - acc: 0.00 - ETA: 45s - loss: 4.8801 - acc: 0.00 - ETA: 43s - loss: 4.8799 - acc: 0.00 - ETA: 41s - loss: 4.8801 - acc: 0.00 - ETA: 39s - loss: 4.8801 - acc: 0.00 - ETA: 38s - loss: 4.8801 - acc: 0.00 - ETA: 36s - loss: 4.8801 - acc: 0.00 - ETA: 34s - loss: 4.8801 - acc: 0.00 - ETA: 32s - loss: 4.8800 - acc: 0.00 - ETA: 31s - loss: 4.8800 - acc: 0.00 - ETA: 29s - loss: 4.8797 - acc: 0.00 - ETA: 27s - loss: 4.8800 - acc: 0.00 - ETA: 25s - loss: 4.8802 - acc: 0.00 - ETA: 24s - loss: 4.8801 - acc: 0.00 - ETA: 22s - loss: 4.8803 - acc: 0.00 - ETA: 20s - loss: 4.8804 - acc: 0.00 - ETA: 18s - loss: 4.8804 - acc: 0.00 - ETA: 16s - loss: 4.8803 - acc: 0.00 - ETA: 15s - loss: 4.8803 - acc: 0.00 - ETA: 13s - loss: 4.8803 - acc: 0.00 - ETA: 11s - loss: 4.8801 - acc: 0.00 - ETA: 9s - loss: 4.8804 - acc: 0.0086 - ETA: 8s - loss: 4.8801 - acc: 0.008 - ETA: 6s - loss: 4.8801 - acc: 0.008 - ETA: 4s - loss: 4.8802 - acc: 0.008 - ETA: 2s - loss: 4.8803 - acc: 0.008 - ETA: 1s - loss: 4.8804 - acc: 0.0087Epoch 00001: val_loss improved from 4.88086 to 4.87483, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 246s - loss: 4.8805 - acc: 0.0087 - val_loss: 4.8748 - val_acc: 0.0168\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6650/6680 [============================>.] - ETA: 248s - loss: 4.8646 - acc: 0.0000e+0 - ETA: 244s - loss: 4.8660 - acc: 0.0000e+0 - ETA: 237s - loss: 4.8582 - acc: 0.0000e+0 - ETA: 233s - loss: 4.8621 - acc: 0.0050    - ETA: 231s - loss: 4.8644 - acc: 0.008 - ETA: 230s - loss: 4.8656 - acc: 0.010 - ETA: 227s - loss: 4.8690 - acc: 0.008 - ETA: 226s - loss: 4.8694 - acc: 0.010 - ETA: 223s - loss: 4.8664 - acc: 0.008 - ETA: 227s - loss: 4.8692 - acc: 0.010 - ETA: 226s - loss: 4.8679 - acc: 0.012 - ETA: 223s - loss: 4.8660 - acc: 0.013 - ETA: 223s - loss: 4.8676 - acc: 0.012 - ETA: 221s - loss: 4.8671 - acc: 0.011 - ETA: 219s - loss: 4.8671 - acc: 0.010 - ETA: 217s - loss: 4.8693 - acc: 0.011 - ETA: 215s - loss: 4.8709 - acc: 0.010 - ETA: 212s - loss: 4.8704 - acc: 0.011 - ETA: 213s - loss: 4.8692 - acc: 0.011 - ETA: 212s - loss: 4.8710 - acc: 0.012 - ETA: 211s - loss: 4.8698 - acc: 0.012 - ETA: 208s - loss: 4.8707 - acc: 0.011 - ETA: 206s - loss: 4.8697 - acc: 0.011 - ETA: 205s - loss: 4.8702 - acc: 0.010 - ETA: 205s - loss: 4.8701 - acc: 0.010 - ETA: 204s - loss: 4.8704 - acc: 0.011 - ETA: 201s - loss: 4.8698 - acc: 0.011 - ETA: 199s - loss: 4.8711 - acc: 0.011 - ETA: 196s - loss: 4.8713 - acc: 0.011 - ETA: 194s - loss: 4.8719 - acc: 0.010 - ETA: 192s - loss: 4.8722 - acc: 0.011 - ETA: 190s - loss: 4.8728 - acc: 0.010 - ETA: 188s - loss: 4.8734 - acc: 0.010 - ETA: 186s - loss: 4.8735 - acc: 0.010 - ETA: 184s - loss: 4.8735 - acc: 0.009 - ETA: 182s - loss: 4.8731 - acc: 0.009 - ETA: 180s - loss: 4.8732 - acc: 0.009 - ETA: 178s - loss: 4.8735 - acc: 0.009 - ETA: 175s - loss: 4.8742 - acc: 0.009 - ETA: 173s - loss: 4.8741 - acc: 0.010 - ETA: 171s - loss: 4.8744 - acc: 0.010 - ETA: 169s - loss: 4.8744 - acc: 0.010 - ETA: 167s - loss: 4.8747 - acc: 0.009 - ETA: 165s - loss: 4.8750 - acc: 0.009 - ETA: 163s - loss: 4.8748 - acc: 0.009 - ETA: 162s - loss: 4.8745 - acc: 0.009 - ETA: 159s - loss: 4.8737 - acc: 0.009 - ETA: 158s - loss: 4.8739 - acc: 0.009 - ETA: 156s - loss: 4.8739 - acc: 0.009 - ETA: 154s - loss: 4.8737 - acc: 0.009 - ETA: 152s - loss: 4.8738 - acc: 0.009 - ETA: 150s - loss: 4.8737 - acc: 0.008 - ETA: 148s - loss: 4.8736 - acc: 0.009 - ETA: 146s - loss: 4.8736 - acc: 0.009 - ETA: 144s - loss: 4.8737 - acc: 0.009 - ETA: 142s - loss: 4.8730 - acc: 0.010 - ETA: 140s - loss: 4.8730 - acc: 0.010 - ETA: 138s - loss: 4.8734 - acc: 0.010 - ETA: 136s - loss: 4.8724 - acc: 0.011 - ETA: 134s - loss: 4.8725 - acc: 0.011 - ETA: 132s - loss: 4.8723 - acc: 0.011 - ETA: 130s - loss: 4.8726 - acc: 0.011 - ETA: 128s - loss: 4.8731 - acc: 0.011 - ETA: 127s - loss: 4.8739 - acc: 0.010 - ETA: 125s - loss: 4.8738 - acc: 0.011 - ETA: 123s - loss: 4.8738 - acc: 0.010 - ETA: 121s - loss: 4.8738 - acc: 0.010 - ETA: 119s - loss: 4.8741 - acc: 0.010 - ETA: 117s - loss: 4.8744 - acc: 0.010 - ETA: 115s - loss: 4.8740 - acc: 0.010 - ETA: 113s - loss: 4.8739 - acc: 0.010 - ETA: 111s - loss: 4.8741 - acc: 0.010 - ETA: 110s - loss: 4.8739 - acc: 0.010 - ETA: 108s - loss: 4.8736 - acc: 0.010 - ETA: 106s - loss: 4.8730 - acc: 0.010 - ETA: 104s - loss: 4.8735 - acc: 0.010 - ETA: 102s - loss: 4.8730 - acc: 0.011 - ETA: 100s - loss: 4.8733 - acc: 0.011 - ETA: 98s - loss: 4.8738 - acc: 0.010 - ETA: 97s - loss: 4.8735 - acc: 0.01 - ETA: 95s - loss: 4.8730 - acc: 0.01 - ETA: 93s - loss: 4.8729 - acc: 0.01 - ETA: 91s - loss: 4.8730 - acc: 0.01 - ETA: 89s - loss: 4.8729 - acc: 0.01 - ETA: 87s - loss: 4.8730 - acc: 0.01 - ETA: 86s - loss: 4.8730 - acc: 0.01 - ETA: 84s - loss: 4.8734 - acc: 0.01 - ETA: 82s - loss: 4.8735 - acc: 0.01 - ETA: 80s - loss: 4.8735 - acc: 0.01 - ETA: 78s - loss: 4.8731 - acc: 0.01 - ETA: 76s - loss: 4.8731 - acc: 0.01 - ETA: 75s - loss: 4.8727 - acc: 0.01 - ETA: 73s - loss: 4.8723 - acc: 0.00 - ETA: 71s - loss: 4.8722 - acc: 0.01 - ETA: 69s - loss: 4.8721 - acc: 0.01 - ETA: 67s - loss: 4.8721 - acc: 0.01 - ETA: 65s - loss: 4.8724 - acc: 0.00 - ETA: 64s - loss: 4.8722 - acc: 0.01 - ETA: 62s - loss: 4.8718 - acc: 0.00 - ETA: 60s - loss: 4.8717 - acc: 0.01 - ETA: 58s - loss: 4.8720 - acc: 0.01 - ETA: 56s - loss: 4.8714 - acc: 0.01 - ETA: 55s - loss: 4.8713 - acc: 0.01 - ETA: 53s - loss: 4.8709 - acc: 0.01 - ETA: 51s - loss: 4.8708 - acc: 0.01 - ETA: 49s - loss: 4.8709 - acc: 0.01 - ETA: 47s - loss: 4.8703 - acc: 0.01 - ETA: 46s - loss: 4.8704 - acc: 0.01 - ETA: 44s - loss: 4.8700 - acc: 0.01 - ETA: 42s - loss: 4.8697 - acc: 0.01 - ETA: 40s - loss: 4.8695 - acc: 0.01 - ETA: 38s - loss: 4.8693 - acc: 0.01 - ETA: 37s - loss: 4.8694 - acc: 0.01 - ETA: 35s - loss: 4.8692 - acc: 0.01 - ETA: 33s - loss: 4.8692 - acc: 0.01 - ETA: 31s - loss: 4.8690 - acc: 0.01 - ETA: 29s - loss: 4.8690 - acc: 0.01 - ETA: 28s - loss: 4.8693 - acc: 0.01 - ETA: 26s - loss: 4.8693 - acc: 0.01 - ETA: 24s - loss: 4.8693 - acc: 0.01 - ETA: 22s - loss: 4.8696 - acc: 0.01 - ETA: 20s - loss: 4.8703 - acc: 0.01 - ETA: 19s - loss: 4.8700 - acc: 0.01 - ETA: 17s - loss: 4.8696 - acc: 0.01 - ETA: 15s - loss: 4.8697 - acc: 0.01 - ETA: 13s - loss: 4.8697 - acc: 0.01 - ETA: 11s - loss: 4.8698 - acc: 0.01 - ETA: 10s - loss: 4.8696 - acc: 0.01 - ETA: 8s - loss: 4.8693 - acc: 0.0105 - ETA: 6s - loss: 4.8696 - acc: 0.010 - ETA: 4s - loss: 4.8694 - acc: 0.010 - ETA: 2s - loss: 4.8693 - acc: 0.010 - ETA: 1s - loss: 4.8693 - acc: 0.0102Epoch 00002: val_loss improved from 4.87483 to 4.85314, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 253s - loss: 4.8693 - acc: 0.0102 - val_loss: 4.8531 - val_acc: 0.0204\n",
      "Epoch 4/5\n",
      "6650/6680 [============================>.] - ETA: 249s - loss: 4.8146 - acc: 0.020 - ETA: 241s - loss: 4.8214 - acc: 0.030 - ETA: 242s - loss: 4.8180 - acc: 0.026 - ETA: 235s - loss: 4.8121 - acc: 0.020 - ETA: 233s - loss: 4.8209 - acc: 0.020 - ETA: 231s - loss: 4.8197 - acc: 0.020 - ETA: 228s - loss: 4.8259 - acc: 0.020 - ETA: 226s - loss: 4.8312 - acc: 0.017 - ETA: 227s - loss: 4.8287 - acc: 0.020 - ETA: 225s - loss: 4.8303 - acc: 0.018 - ETA: 223s - loss: 4.8325 - acc: 0.020 - ETA: 220s - loss: 4.8354 - acc: 0.018 - ETA: 218s - loss: 4.8422 - acc: 0.018 - ETA: 216s - loss: 4.8437 - acc: 0.017 - ETA: 213s - loss: 4.8406 - acc: 0.016 - ETA: 211s - loss: 4.8414 - acc: 0.016 - ETA: 209s - loss: 4.8473 - acc: 0.016 - ETA: 207s - loss: 4.8450 - acc: 0.016 - ETA: 204s - loss: 4.8476 - acc: 0.017 - ETA: 202s - loss: 4.8481 - acc: 0.017 - ETA: 200s - loss: 4.8500 - acc: 0.016 - ETA: 198s - loss: 4.8492 - acc: 0.015 - ETA: 196s - loss: 4.8500 - acc: 0.014 - ETA: 195s - loss: 4.8502 - acc: 0.015 - ETA: 192s - loss: 4.8495 - acc: 0.016 - ETA: 191s - loss: 4.8487 - acc: 0.016 - ETA: 189s - loss: 4.8499 - acc: 0.017 - ETA: 187s - loss: 4.8504 - acc: 0.016 - ETA: 185s - loss: 4.8513 - acc: 0.017 - ETA: 183s - loss: 4.8523 - acc: 0.016 - ETA: 182s - loss: 4.8527 - acc: 0.016 - ETA: 180s - loss: 4.8519 - acc: 0.015 - ETA: 178s - loss: 4.8507 - acc: 0.015 - ETA: 176s - loss: 4.8505 - acc: 0.015 - ETA: 175s - loss: 4.8496 - acc: 0.015 - ETA: 173s - loss: 4.8515 - acc: 0.015 - ETA: 171s - loss: 4.8520 - acc: 0.015 - ETA: 169s - loss: 4.8515 - acc: 0.014 - ETA: 167s - loss: 4.8519 - acc: 0.015 - ETA: 165s - loss: 4.8513 - acc: 0.015 - ETA: 164s - loss: 4.8517 - acc: 0.014 - ETA: 162s - loss: 4.8526 - acc: 0.014 - ETA: 160s - loss: 4.8526 - acc: 0.014 - ETA: 158s - loss: 4.8522 - acc: 0.014 - ETA: 156s - loss: 4.8526 - acc: 0.013 - ETA: 155s - loss: 4.8522 - acc: 0.013 - ETA: 153s - loss: 4.8526 - acc: 0.014 - ETA: 151s - loss: 4.8523 - acc: 0.013 - ETA: 150s - loss: 4.8524 - acc: 0.013 - ETA: 149s - loss: 4.8530 - acc: 0.014 - ETA: 148s - loss: 4.8516 - acc: 0.014 - ETA: 146s - loss: 4.8521 - acc: 0.013 - ETA: 144s - loss: 4.8523 - acc: 0.013 - ETA: 143s - loss: 4.8528 - acc: 0.013 - ETA: 142s - loss: 4.8533 - acc: 0.013 - ETA: 140s - loss: 4.8535 - acc: 0.013 - ETA: 138s - loss: 4.8536 - acc: 0.013 - ETA: 137s - loss: 4.8536 - acc: 0.013 - ETA: 135s - loss: 4.8528 - acc: 0.012 - ETA: 134s - loss: 4.8519 - acc: 0.012 - ETA: 132s - loss: 4.8514 - acc: 0.013 - ETA: 131s - loss: 4.8512 - acc: 0.013 - ETA: 129s - loss: 4.8517 - acc: 0.013 - ETA: 128s - loss: 4.8515 - acc: 0.013 - ETA: 126s - loss: 4.8516 - acc: 0.013 - ETA: 125s - loss: 4.8519 - acc: 0.013 - ETA: 123s - loss: 4.8515 - acc: 0.013 - ETA: 122s - loss: 4.8511 - acc: 0.014 - ETA: 120s - loss: 4.8512 - acc: 0.013 - ETA: 118s - loss: 4.8510 - acc: 0.013 - ETA: 116s - loss: 4.8500 - acc: 0.013 - ETA: 114s - loss: 4.8500 - acc: 0.013 - ETA: 112s - loss: 4.8499 - acc: 0.013 - ETA: 111s - loss: 4.8495 - acc: 0.013 - ETA: 109s - loss: 4.8492 - acc: 0.013 - ETA: 107s - loss: 4.8492 - acc: 0.014 - ETA: 105s - loss: 4.8486 - acc: 0.014 - ETA: 103s - loss: 4.8484 - acc: 0.014 - ETA: 101s - loss: 4.8485 - acc: 0.014 - ETA: 99s - loss: 4.8483 - acc: 0.015 - ETA: 97s - loss: 4.8484 - acc: 0.01 - ETA: 95s - loss: 4.8480 - acc: 0.01 - ETA: 93s - loss: 4.8481 - acc: 0.01 - ETA: 91s - loss: 4.8477 - acc: 0.01 - ETA: 89s - loss: 4.8475 - acc: 0.01 - ETA: 87s - loss: 4.8483 - acc: 0.01 - ETA: 86s - loss: 4.8489 - acc: 0.01 - ETA: 84s - loss: 4.8487 - acc: 0.01 - ETA: 82s - loss: 4.8484 - acc: 0.01 - ETA: 80s - loss: 4.8472 - acc: 0.01 - ETA: 78s - loss: 4.8465 - acc: 0.01 - ETA: 76s - loss: 4.8460 - acc: 0.01 - ETA: 74s - loss: 4.8466 - acc: 0.01 - ETA: 72s - loss: 4.8460 - acc: 0.01 - ETA: 71s - loss: 4.8464 - acc: 0.01 - ETA: 69s - loss: 4.8469 - acc: 0.01 - ETA: 67s - loss: 4.8471 - acc: 0.01 - ETA: 65s - loss: 4.8467 - acc: 0.01 - ETA: 63s - loss: 4.8465 - acc: 0.01 - ETA: 61s - loss: 4.8466 - acc: 0.01 - ETA: 59s - loss: 4.8465 - acc: 0.01 - ETA: 58s - loss: 4.8474 - acc: 0.01 - ETA: 56s - loss: 4.8472 - acc: 0.01 - ETA: 54s - loss: 4.8471 - acc: 0.01 - ETA: 52s - loss: 4.8473 - acc: 0.01 - ETA: 50s - loss: 4.8475 - acc: 0.01 - ETA: 48s - loss: 4.8474 - acc: 0.01 - ETA: 46s - loss: 4.8474 - acc: 0.01 - ETA: 45s - loss: 4.8463 - acc: 0.01 - ETA: 43s - loss: 4.8464 - acc: 0.01 - ETA: 41s - loss: 4.8461 - acc: 0.01 - ETA: 39s - loss: 4.8460 - acc: 0.01 - ETA: 37s - loss: 4.8460 - acc: 0.01 - ETA: 35s - loss: 4.8459 - acc: 0.01 - ETA: 34s - loss: 4.8464 - acc: 0.01 - ETA: 32s - loss: 4.8469 - acc: 0.01 - ETA: 30s - loss: 4.8472 - acc: 0.01 - ETA: 28s - loss: 4.8472 - acc: 0.01 - ETA: 26s - loss: 4.8471 - acc: 0.01 - ETA: 24s - loss: 4.8474 - acc: 0.01 - ETA: 23s - loss: 4.8469 - acc: 0.01 - ETA: 21s - loss: 4.8465 - acc: 0.01 - ETA: 19s - loss: 4.8468 - acc: 0.01 - ETA: 17s - loss: 4.8467 - acc: 0.01 - ETA: 15s - loss: 4.8459 - acc: 0.01 - ETA: 13s - loss: 4.8457 - acc: 0.01 - ETA: 12s - loss: 4.8456 - acc: 0.01 - ETA: 10s - loss: 4.8453 - acc: 0.01 - ETA: 8s - loss: 4.8448 - acc: 0.0144 - ETA: 6s - loss: 4.8458 - acc: 0.014 - ETA: 4s - loss: 4.8455 - acc: 0.014 - ETA: 2s - loss: 4.8449 - acc: 0.014 - ETA: 1s - loss: 4.8453 - acc: 0.0141Epoch 00003: val_loss improved from 4.85314 to 4.82116, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 253s - loss: 4.8449 - acc: 0.0141 - val_loss: 4.8212 - val_acc: 0.0228\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6650/6680 [============================>.] - ETA: 234s - loss: 4.8594 - acc: 0.0000e+0 - ETA: 236s - loss: 4.8589 - acc: 0.0000e+0 - ETA: 239s - loss: 4.8585 - acc: 0.0000e+0 - ETA: 232s - loss: 4.8523 - acc: 0.0050    - ETA: 230s - loss: 4.8440 - acc: 0.012 - ETA: 228s - loss: 4.8324 - acc: 0.010 - ETA: 225s - loss: 4.8317 - acc: 0.011 - ETA: 223s - loss: 4.8346 - acc: 0.012 - ETA: 221s - loss: 4.8345 - acc: 0.011 - ETA: 219s - loss: 4.8345 - acc: 0.012 - ETA: 217s - loss: 4.8306 - acc: 0.010 - ETA: 215s - loss: 4.8263 - acc: 0.010 - ETA: 213s - loss: 4.8260 - acc: 0.009 - ETA: 211s - loss: 4.8238 - acc: 0.010 - ETA: 210s - loss: 4.8248 - acc: 0.009 - ETA: 208s - loss: 4.8263 - acc: 0.008 - ETA: 206s - loss: 4.8209 - acc: 0.010 - ETA: 205s - loss: 4.8229 - acc: 0.011 - ETA: 203s - loss: 4.8213 - acc: 0.011 - ETA: 201s - loss: 4.8232 - acc: 0.012 - ETA: 199s - loss: 4.8237 - acc: 0.013 - ETA: 197s - loss: 4.8247 - acc: 0.012 - ETA: 195s - loss: 4.8252 - acc: 0.012 - ETA: 194s - loss: 4.8267 - acc: 0.012 - ETA: 192s - loss: 4.8284 - acc: 0.012 - ETA: 190s - loss: 4.8277 - acc: 0.012 - ETA: 188s - loss: 4.8257 - acc: 0.013 - ETA: 186s - loss: 4.8221 - acc: 0.014 - ETA: 185s - loss: 4.8262 - acc: 0.013 - ETA: 183s - loss: 4.8259 - acc: 0.013 - ETA: 181s - loss: 4.8260 - acc: 0.012 - ETA: 179s - loss: 4.8252 - acc: 0.013 - ETA: 177s - loss: 4.8249 - acc: 0.012 - ETA: 175s - loss: 4.8247 - acc: 0.013 - ETA: 174s - loss: 4.8271 - acc: 0.013 - ETA: 172s - loss: 4.8273 - acc: 0.013 - ETA: 170s - loss: 4.8271 - acc: 0.014 - ETA: 168s - loss: 4.8270 - acc: 0.014 - ETA: 167s - loss: 4.8272 - acc: 0.014 - ETA: 165s - loss: 4.8261 - acc: 0.014 - ETA: 163s - loss: 4.8242 - acc: 0.014 - ETA: 161s - loss: 4.8268 - acc: 0.014 - ETA: 160s - loss: 4.8274 - acc: 0.014 - ETA: 158s - loss: 4.8267 - acc: 0.014 - ETA: 156s - loss: 4.8248 - acc: 0.014 - ETA: 155s - loss: 4.8254 - acc: 0.013 - ETA: 153s - loss: 4.8252 - acc: 0.014 - ETA: 151s - loss: 4.8246 - acc: 0.015 - ETA: 149s - loss: 4.8259 - acc: 0.015 - ETA: 147s - loss: 4.8266 - acc: 0.015 - ETA: 146s - loss: 4.8254 - acc: 0.015 - ETA: 144s - loss: 4.8256 - acc: 0.015 - ETA: 142s - loss: 4.8268 - acc: 0.014 - ETA: 140s - loss: 4.8268 - acc: 0.014 - ETA: 139s - loss: 4.8257 - acc: 0.014 - ETA: 137s - loss: 4.8259 - acc: 0.015 - ETA: 135s - loss: 4.8254 - acc: 0.014 - ETA: 133s - loss: 4.8245 - acc: 0.015 - ETA: 132s - loss: 4.8236 - acc: 0.015 - ETA: 130s - loss: 4.8240 - acc: 0.015 - ETA: 128s - loss: 4.8235 - acc: 0.015 - ETA: 126s - loss: 4.8236 - acc: 0.015 - ETA: 125s - loss: 4.8240 - acc: 0.015 - ETA: 123s - loss: 4.8236 - acc: 0.015 - ETA: 121s - loss: 4.8229 - acc: 0.015 - ETA: 119s - loss: 4.8228 - acc: 0.015 - ETA: 118s - loss: 4.8230 - acc: 0.015 - ETA: 116s - loss: 4.8234 - acc: 0.015 - ETA: 114s - loss: 4.8227 - acc: 0.016 - ETA: 112s - loss: 4.8239 - acc: 0.016 - ETA: 110s - loss: 4.8224 - acc: 0.016 - ETA: 109s - loss: 4.8226 - acc: 0.016 - ETA: 107s - loss: 4.8226 - acc: 0.016 - ETA: 105s - loss: 4.8225 - acc: 0.016 - ETA: 103s - loss: 4.8222 - acc: 0.016 - ETA: 102s - loss: 4.8224 - acc: 0.016 - ETA: 100s - loss: 4.8224 - acc: 0.016 - ETA: 98s - loss: 4.8229 - acc: 0.016 - ETA: 96s - loss: 4.8226 - acc: 0.01 - ETA: 95s - loss: 4.8222 - acc: 0.01 - ETA: 93s - loss: 4.8224 - acc: 0.01 - ETA: 92s - loss: 4.8210 - acc: 0.01 - ETA: 90s - loss: 4.8216 - acc: 0.01 - ETA: 88s - loss: 4.8218 - acc: 0.01 - ETA: 86s - loss: 4.8210 - acc: 0.01 - ETA: 85s - loss: 4.8213 - acc: 0.01 - ETA: 83s - loss: 4.8203 - acc: 0.01 - ETA: 81s - loss: 4.8198 - acc: 0.01 - ETA: 80s - loss: 4.8205 - acc: 0.01 - ETA: 78s - loss: 4.8196 - acc: 0.01 - ETA: 76s - loss: 4.8200 - acc: 0.01 - ETA: 75s - loss: 4.8205 - acc: 0.01 - ETA: 73s - loss: 4.8200 - acc: 0.01 - ETA: 71s - loss: 4.8194 - acc: 0.01 - ETA: 69s - loss: 4.8191 - acc: 0.01 - ETA: 67s - loss: 4.8195 - acc: 0.01 - ETA: 65s - loss: 4.8192 - acc: 0.01 - ETA: 64s - loss: 4.8204 - acc: 0.01 - ETA: 62s - loss: 4.8212 - acc: 0.01 - ETA: 60s - loss: 4.8214 - acc: 0.01 - ETA: 58s - loss: 4.8216 - acc: 0.01 - ETA: 56s - loss: 4.8209 - acc: 0.01 - ETA: 55s - loss: 4.8211 - acc: 0.01 - ETA: 53s - loss: 4.8204 - acc: 0.01 - ETA: 51s - loss: 4.8208 - acc: 0.01 - ETA: 49s - loss: 4.8198 - acc: 0.01 - ETA: 47s - loss: 4.8203 - acc: 0.01 - ETA: 46s - loss: 4.8197 - acc: 0.01 - ETA: 44s - loss: 4.8197 - acc: 0.01 - ETA: 42s - loss: 4.8201 - acc: 0.01 - ETA: 40s - loss: 4.8196 - acc: 0.01 - ETA: 38s - loss: 4.8196 - acc: 0.01 - ETA: 36s - loss: 4.8199 - acc: 0.01 - ETA: 35s - loss: 4.8201 - acc: 0.01 - ETA: 33s - loss: 4.8197 - acc: 0.01 - ETA: 31s - loss: 4.8197 - acc: 0.01 - ETA: 29s - loss: 4.8201 - acc: 0.01 - ETA: 28s - loss: 4.8199 - acc: 0.01 - ETA: 26s - loss: 4.8190 - acc: 0.01 - ETA: 24s - loss: 4.8185 - acc: 0.01 - ETA: 22s - loss: 4.8187 - acc: 0.01 - ETA: 20s - loss: 4.8180 - acc: 0.01 - ETA: 19s - loss: 4.8191 - acc: 0.01 - ETA: 17s - loss: 4.8189 - acc: 0.01 - ETA: 15s - loss: 4.8187 - acc: 0.01 - ETA: 13s - loss: 4.8189 - acc: 0.01 - ETA: 11s - loss: 4.8188 - acc: 0.01 - ETA: 10s - loss: 4.8185 - acc: 0.01 - ETA: 8s - loss: 4.8181 - acc: 0.0150 - ETA: 6s - loss: 4.8179 - acc: 0.015 - ETA: 4s - loss: 4.8171 - acc: 0.015 - ETA: 2s - loss: 4.8180 - acc: 0.015 - ETA: 1s - loss: 4.8176 - acc: 0.0149Epoch 00004: val_loss improved from 4.82116 to 4.79760, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "6680/6680 [==============================] - 249s - loss: 4.8177 - acc: 0.0151 - val_loss: 4.7976 - val_acc: 0.0180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20ea293cfd0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=50, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 2.6316%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Use a CNN to Classify Dog Breeds\n",
    "\n",
    "To reduce training time without sacrificing accuracy, we show you how to train a CNN using transfer learning.  In the following step, you will get a chance to use transfer learning to train your own CNN.\n",
    "\n",
    "### Obtain Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\n",
    "train_VGG16 = bottleneck_features['train']\n",
    "valid_VGG16 = bottleneck_features['valid']\n",
    "test_VGG16 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model uses the the pre-trained VGG-16 model as a fixed feature extractor, where the last convolutional output of VGG-16 is fed as input to our model.  We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each dog category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_8 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 68,229.0\n",
      "Trainable params: 68,229.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG16_model = Sequential()\n",
    "VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "VGG16_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "VGG16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6660/6680 [============================>.] - ETA: 77s - loss: 14.4242 - acc: 0.0000e+0 - ETA: 7s - loss: 14.6507 - acc: 0.0231    - ETA: 4s - loss: 14.7757 - acc: 0.02 - ETA: 3s - loss: 14.6149 - acc: 0.02 - ETA: 3s - loss: 14.6218 - acc: 0.02 - ETA: 2s - loss: 14.4643 - acc: 0.03 - ETA: 2s - loss: 14.3767 - acc: 0.04 - ETA: 2s - loss: 14.3078 - acc: 0.04 - ETA: 1s - loss: 14.2430 - acc: 0.04 - ETA: 1s - loss: 14.1537 - acc: 0.04 - ETA: 1s - loss: 14.0563 - acc: 0.04 - ETA: 1s - loss: 13.9518 - acc: 0.05 - ETA: 1s - loss: 13.8636 - acc: 0.05 - ETA: 1s - loss: 13.7604 - acc: 0.05 - ETA: 1s - loss: 13.6299 - acc: 0.06 - ETA: 1s - loss: 13.5176 - acc: 0.06 - ETA: 1s - loss: 13.4521 - acc: 0.06 - ETA: 1s - loss: 13.3554 - acc: 0.07 - ETA: 0s - loss: 13.2730 - acc: 0.07 - ETA: 0s - loss: 13.1789 - acc: 0.08 - ETA: 0s - loss: 13.1472 - acc: 0.08 - ETA: 0s - loss: 13.0469 - acc: 0.09 - ETA: 0s - loss: 12.9777 - acc: 0.09 - ETA: 0s - loss: 12.9021 - acc: 0.09 - ETA: 0s - loss: 12.8349 - acc: 0.10 - ETA: 0s - loss: 12.7635 - acc: 0.10 - ETA: 0s - loss: 12.7347 - acc: 0.10 - ETA: 0s - loss: 12.6783 - acc: 0.10 - ETA: 0s - loss: 12.6083 - acc: 0.11 - ETA: 0s - loss: 12.5348 - acc: 0.11 - ETA: 0s - loss: 12.5013 - acc: 0.12 - ETA: 0s - loss: 12.4486 - acc: 0.12 - ETA: 0s - loss: 12.4032 - acc: 0.1269Epoch 00000: val_loss improved from inf to 10.93756, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 2s - loss: 12.3899 - acc: 0.1280 - val_loss: 10.9376 - val_acc: 0.2144\n",
      "Epoch 2/20\n",
      "6640/6680 [============================>.] - ETA: 1s - loss: 9.7967 - acc: 0.350 - ETA: 1s - loss: 11.4517 - acc: 0.21 - ETA: 1s - loss: 11.1223 - acc: 0.23 - ETA: 1s - loss: 11.2944 - acc: 0.21 - ETA: 1s - loss: 10.9623 - acc: 0.22 - ETA: 1s - loss: 10.9762 - acc: 0.22 - ETA: 1s - loss: 10.8305 - acc: 0.23 - ETA: 1s - loss: 10.7907 - acc: 0.23 - ETA: 1s - loss: 10.8227 - acc: 0.23 - ETA: 1s - loss: 10.7113 - acc: 0.24 - ETA: 1s - loss: 10.6625 - acc: 0.24 - ETA: 1s - loss: 10.6559 - acc: 0.24 - ETA: 1s - loss: 10.6892 - acc: 0.24 - ETA: 1s - loss: 10.6311 - acc: 0.25 - ETA: 0s - loss: 10.5869 - acc: 0.25 - ETA: 0s - loss: 10.5380 - acc: 0.25 - ETA: 0s - loss: 10.4415 - acc: 0.26 - ETA: 0s - loss: 10.4443 - acc: 0.26 - ETA: 0s - loss: 10.4146 - acc: 0.26 - ETA: 0s - loss: 10.3799 - acc: 0.26 - ETA: 0s - loss: 10.3691 - acc: 0.26 - ETA: 0s - loss: 10.3662 - acc: 0.27 - ETA: 0s - loss: 10.3349 - acc: 0.27 - ETA: 0s - loss: 10.3210 - acc: 0.27 - ETA: 0s - loss: 10.3219 - acc: 0.27 - ETA: 0s - loss: 10.3200 - acc: 0.27 - ETA: 0s - loss: 10.3483 - acc: 0.27 - ETA: 0s - loss: 10.2974 - acc: 0.27 - ETA: 0s - loss: 10.2778 - acc: 0.28 - ETA: 0s - loss: 10.2659 - acc: 0.28 - ETA: 0s - loss: 10.2655 - acc: 0.28 - ETA: 0s - loss: 10.2825 - acc: 0.28 - ETA: 0s - loss: 10.2958 - acc: 0.2833Epoch 00001: val_loss improved from 10.93756 to 10.31645, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 10.2975 - acc: 0.2832 - val_loss: 10.3164 - val_acc: 0.2814\n",
      "Epoch 3/20\n",
      "6480/6680 [============================>.] - ETA: 1s - loss: 9.1805 - acc: 0.400 - ETA: 1s - loss: 10.3968 - acc: 0.31 - ETA: 1s - loss: 10.4874 - acc: 0.30 - ETA: 1s - loss: 10.1092 - acc: 0.32 - ETA: 1s - loss: 10.1142 - acc: 0.32 - ETA: 1s - loss: 10.1942 - acc: 0.32 - ETA: 1s - loss: 10.0782 - acc: 0.32 - ETA: 1s - loss: 9.9297 - acc: 0.3405 - ETA: 1s - loss: 9.7731 - acc: 0.349 - ETA: 1s - loss: 9.7786 - acc: 0.348 - ETA: 1s - loss: 9.7829 - acc: 0.346 - ETA: 1s - loss: 9.7894 - acc: 0.344 - ETA: 0s - loss: 9.7454 - acc: 0.345 - ETA: 0s - loss: 9.7073 - acc: 0.347 - ETA: 0s - loss: 9.7639 - acc: 0.343 - ETA: 0s - loss: 9.7841 - acc: 0.342 - ETA: 0s - loss: 9.8298 - acc: 0.340 - ETA: 0s - loss: 9.8226 - acc: 0.341 - ETA: 0s - loss: 9.8171 - acc: 0.341 - ETA: 0s - loss: 9.8097 - acc: 0.342 - ETA: 0s - loss: 9.8510 - acc: 0.340 - ETA: 0s - loss: 9.8587 - acc: 0.340 - ETA: 0s - loss: 9.8836 - acc: 0.338 - ETA: 0s - loss: 9.8919 - acc: 0.338 - ETA: 0s - loss: 9.9233 - acc: 0.336 - ETA: 0s - loss: 9.8825 - acc: 0.339 - ETA: 0s - loss: 9.8699 - acc: 0.340 - ETA: 0s - loss: 9.9122 - acc: 0.338 - ETA: 0s - loss: 9.8941 - acc: 0.339 - ETA: 0s - loss: 9.8986 - acc: 0.339 - ETA: 0s - loss: 9.9204 - acc: 0.3386Epoch 00002: val_loss improved from 10.31645 to 10.20370, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 9.9207 - acc: 0.3389 - val_loss: 10.2037 - val_acc: 0.3174\n",
      "Epoch 4/20\n",
      "6540/6680 [============================>.] - ETA: 1s - loss: 11.4614 - acc: 0.20 - ETA: 1s - loss: 9.6817 - acc: 0.3577 - ETA: 1s - loss: 9.6228 - acc: 0.370 - ETA: 1s - loss: 9.7135 - acc: 0.365 - ETA: 1s - loss: 9.7699 - acc: 0.360 - ETA: 1s - loss: 9.6807 - acc: 0.364 - ETA: 1s - loss: 9.6904 - acc: 0.365 - ETA: 1s - loss: 9.6079 - acc: 0.371 - ETA: 1s - loss: 9.5117 - acc: 0.375 - ETA: 1s - loss: 9.5597 - acc: 0.372 - ETA: 1s - loss: 9.5449 - acc: 0.374 - ETA: 1s - loss: 9.5912 - acc: 0.372 - ETA: 1s - loss: 9.5791 - acc: 0.372 - ETA: 0s - loss: 9.6163 - acc: 0.369 - ETA: 0s - loss: 9.6570 - acc: 0.365 - ETA: 0s - loss: 9.6655 - acc: 0.364 - ETA: 0s - loss: 9.6992 - acc: 0.361 - ETA: 0s - loss: 9.6810 - acc: 0.363 - ETA: 0s - loss: 9.6862 - acc: 0.363 - ETA: 0s - loss: 9.6464 - acc: 0.366 - ETA: 0s - loss: 9.6470 - acc: 0.366 - ETA: 0s - loss: 9.7116 - acc: 0.361 - ETA: 0s - loss: 9.6992 - acc: 0.362 - ETA: 0s - loss: 9.7447 - acc: 0.360 - ETA: 0s - loss: 9.7519 - acc: 0.359 - ETA: 0s - loss: 9.7757 - acc: 0.358 - ETA: 0s - loss: 9.7737 - acc: 0.358 - ETA: 0s - loss: 9.7704 - acc: 0.359 - ETA: 0s - loss: 9.7709 - acc: 0.359 - ETA: 0s - loss: 9.7648 - acc: 0.359 - ETA: 0s - loss: 9.7515 - acc: 0.360 - ETA: 0s - loss: 9.7707 - acc: 0.3595Epoch 00003: val_loss improved from 10.20370 to 10.07325, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 9.7931 - acc: 0.3581 - val_loss: 10.0732 - val_acc: 0.3114\n",
      "Epoch 5/20\n",
      "6580/6680 [============================>.] - ETA: 1s - loss: 6.5247 - acc: 0.550 - ETA: 1s - loss: 9.4755 - acc: 0.384 - ETA: 1s - loss: 9.7147 - acc: 0.379 - ETA: 1s - loss: 9.7664 - acc: 0.376 - ETA: 1s - loss: 9.6741 - acc: 0.381 - ETA: 1s - loss: 9.4187 - acc: 0.392 - ETA: 1s - loss: 9.3838 - acc: 0.392 - ETA: 1s - loss: 9.3952 - acc: 0.391 - ETA: 1s - loss: 9.4339 - acc: 0.387 - ETA: 1s - loss: 9.5003 - acc: 0.382 - ETA: 1s - loss: 9.4051 - acc: 0.389 - ETA: 1s - loss: 9.4632 - acc: 0.386 - ETA: 1s - loss: 9.4346 - acc: 0.388 - ETA: 0s - loss: 9.3652 - acc: 0.391 - ETA: 0s - loss: 9.4659 - acc: 0.386 - ETA: 0s - loss: 9.4794 - acc: 0.385 - ETA: 0s - loss: 9.5171 - acc: 0.382 - ETA: 0s - loss: 9.5333 - acc: 0.382 - ETA: 0s - loss: 9.5155 - acc: 0.383 - ETA: 0s - loss: 9.4712 - acc: 0.386 - ETA: 0s - loss: 9.4902 - acc: 0.383 - ETA: 0s - loss: 9.4720 - acc: 0.384 - ETA: 0s - loss: 9.5130 - acc: 0.381 - ETA: 0s - loss: 9.5403 - acc: 0.380 - ETA: 0s - loss: 9.5347 - acc: 0.380 - ETA: 0s - loss: 9.5592 - acc: 0.379 - ETA: 0s - loss: 9.5410 - acc: 0.379 - ETA: 0s - loss: 9.5710 - acc: 0.377 - ETA: 0s - loss: 9.5962 - acc: 0.376 - ETA: 0s - loss: 9.5759 - acc: 0.376 - ETA: 0s - loss: 9.5639 - acc: 0.377 - ETA: 0s - loss: 9.5742 - acc: 0.3764Epoch 00004: val_loss improved from 10.07325 to 9.90868, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 9.5737 - acc: 0.3760 - val_loss: 9.9087 - val_acc: 0.3269\n",
      "Epoch 6/20\n",
      "6460/6680 [============================>.] - ETA: 1s - loss: 11.5438 - acc: 0.25 - ETA: 1s - loss: 9.3665 - acc: 0.3958 - ETA: 1s - loss: 9.2245 - acc: 0.400 - ETA: 1s - loss: 9.4813 - acc: 0.389 - ETA: 1s - loss: 9.6369 - acc: 0.380 - ETA: 1s - loss: 9.8496 - acc: 0.367 - ETA: 1s - loss: 9.6526 - acc: 0.380 - ETA: 1s - loss: 9.6500 - acc: 0.378 - ETA: 1s - loss: 9.6816 - acc: 0.376 - ETA: 1s - loss: 9.6624 - acc: 0.377 - ETA: 1s - loss: 9.6313 - acc: 0.381 - ETA: 1s - loss: 9.6178 - acc: 0.382 - ETA: 1s - loss: 9.6017 - acc: 0.383 - ETA: 0s - loss: 9.6068 - acc: 0.381 - ETA: 0s - loss: 9.5409 - acc: 0.386 - ETA: 0s - loss: 9.5097 - acc: 0.387 - ETA: 0s - loss: 9.5411 - acc: 0.386 - ETA: 0s - loss: 9.5539 - acc: 0.385 - ETA: 0s - loss: 9.5374 - acc: 0.386 - ETA: 0s - loss: 9.4954 - acc: 0.389 - ETA: 0s - loss: 9.4928 - acc: 0.389 - ETA: 0s - loss: 9.5258 - acc: 0.387 - ETA: 0s - loss: 9.5293 - acc: 0.387 - ETA: 0s - loss: 9.5527 - acc: 0.386 - ETA: 0s - loss: 9.5729 - acc: 0.385 - ETA: 0s - loss: 9.5282 - acc: 0.387 - ETA: 0s - loss: 9.4847 - acc: 0.389 - ETA: 0s - loss: 9.4495 - acc: 0.392 - ETA: 0s - loss: 9.4190 - acc: 0.393 - ETA: 0s - loss: 9.4408 - acc: 0.392 - ETA: 0s - loss: 9.4380 - acc: 0.3921Epoch 00005: val_loss improved from 9.90868 to 9.88517, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 9.4185 - acc: 0.3927 - val_loss: 9.8852 - val_acc: 0.3198\n",
      "Epoch 7/20\n",
      "6660/6680 [============================>.] - ETA: 1s - loss: 8.8691 - acc: 0.450 - ETA: 1s - loss: 9.9849 - acc: 0.361 - ETA: 1s - loss: 9.8251 - acc: 0.370 - ETA: 1s - loss: 9.5754 - acc: 0.386 - ETA: 1s - loss: 9.4004 - acc: 0.394 - ETA: 1s - loss: 9.3308 - acc: 0.400 - ETA: 1s - loss: 9.3050 - acc: 0.403 - ETA: 1s - loss: 9.2097 - acc: 0.408 - ETA: 1s - loss: 9.2375 - acc: 0.406 - ETA: 0s - loss: 9.1783 - acc: 0.411 - ETA: 0s - loss: 9.1815 - acc: 0.412 - ETA: 0s - loss: 9.1567 - acc: 0.413 - ETA: 0s - loss: 9.1819 - acc: 0.410 - ETA: 0s - loss: 9.1584 - acc: 0.412 - ETA: 0s - loss: 9.1939 - acc: 0.410 - ETA: 0s - loss: 9.2000 - acc: 0.408 - ETA: 0s - loss: 9.2615 - acc: 0.404 - ETA: 0s - loss: 9.2717 - acc: 0.403 - ETA: 0s - loss: 9.2906 - acc: 0.403 - ETA: 0s - loss: 9.2606 - acc: 0.405 - ETA: 0s - loss: 9.2699 - acc: 0.405 - ETA: 0s - loss: 9.3103 - acc: 0.403 - ETA: 0s - loss: 9.3148 - acc: 0.403 - ETA: 0s - loss: 9.2831 - acc: 0.404 - ETA: 0s - loss: 9.2810 - acc: 0.404 - ETA: 0s - loss: 9.2880 - acc: 0.404 - ETA: 0s - loss: 9.3109 - acc: 0.402 - ETA: 0s - loss: 9.2933 - acc: 0.404 - ETA: 0s - loss: 9.2856 - acc: 0.4050Epoch 00006: val_loss improved from 9.88517 to 9.75081, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 9.2825 - acc: 0.4051 - val_loss: 9.7508 - val_acc: 0.3341\n",
      "Epoch 8/20\n",
      "6600/6680 [============================>.] - ETA: 1s - loss: 9.6709 - acc: 0.400 - ETA: 1s - loss: 9.9763 - acc: 0.365 - ETA: 1s - loss: 9.8095 - acc: 0.380 - ETA: 1s - loss: 9.5159 - acc: 0.400 - ETA: 1s - loss: 9.4670 - acc: 0.403 - ETA: 1s - loss: 9.5684 - acc: 0.394 - ETA: 1s - loss: 9.6157 - acc: 0.389 - ETA: 1s - loss: 9.4382 - acc: 0.400 - ETA: 1s - loss: 9.4464 - acc: 0.400 - ETA: 1s - loss: 9.4883 - acc: 0.398 - ETA: 0s - loss: 9.4440 - acc: 0.400 - ETA: 0s - loss: 9.4186 - acc: 0.402 - ETA: 0s - loss: 9.4283 - acc: 0.401 - ETA: 0s - loss: 9.4083 - acc: 0.403 - ETA: 0s - loss: 9.3347 - acc: 0.407 - ETA: 0s - loss: 9.3682 - acc: 0.405 - ETA: 0s - loss: 9.3622 - acc: 0.405 - ETA: 0s - loss: 9.3252 - acc: 0.408 - ETA: 0s - loss: 9.3307 - acc: 0.407 - ETA: 0s - loss: 9.3350 - acc: 0.407 - ETA: 0s - loss: 9.3238 - acc: 0.408 - ETA: 0s - loss: 9.3114 - acc: 0.408 - ETA: 0s - loss: 9.3235 - acc: 0.408 - ETA: 0s - loss: 9.2941 - acc: 0.410 - ETA: 0s - loss: 9.3159 - acc: 0.408 - ETA: 0s - loss: 9.2831 - acc: 0.410 - ETA: 0s - loss: 9.2497 - acc: 0.412 - ETA: 0s - loss: 9.2344 - acc: 0.413 - ETA: 0s - loss: 9.2355 - acc: 0.4135Epoch 00007: val_loss improved from 9.75081 to 9.70676, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 9.2388 - acc: 0.4132 - val_loss: 9.7068 - val_acc: 0.3401\n",
      "Epoch 9/20\n",
      "6600/6680 [============================>.] - ETA: 1s - loss: 8.8660 - acc: 0.450 - ETA: 1s - loss: 9.6047 - acc: 0.404 - ETA: 1s - loss: 9.0200 - acc: 0.436 - ETA: 1s - loss: 9.3212 - acc: 0.418 - ETA: 1s - loss: 9.2503 - acc: 0.421 - ETA: 1s - loss: 9.3647 - acc: 0.415 - ETA: 1s - loss: 9.3098 - acc: 0.418 - ETA: 1s - loss: 9.2989 - acc: 0.417 - ETA: 1s - loss: 9.3334 - acc: 0.415 - ETA: 1s - loss: 9.2827 - acc: 0.418 - ETA: 1s - loss: 9.2722 - acc: 0.419 - ETA: 1s - loss: 9.2668 - acc: 0.419 - ETA: 1s - loss: 9.2235 - acc: 0.422 - ETA: 0s - loss: 9.1906 - acc: 0.424 - ETA: 0s - loss: 9.1483 - acc: 0.426 - ETA: 0s - loss: 9.1073 - acc: 0.428 - ETA: 0s - loss: 9.1366 - acc: 0.426 - ETA: 0s - loss: 9.1956 - acc: 0.422 - ETA: 0s - loss: 9.2016 - acc: 0.421 - ETA: 0s - loss: 9.1506 - acc: 0.424 - ETA: 0s - loss: 9.1337 - acc: 0.424 - ETA: 0s - loss: 9.1074 - acc: 0.425 - ETA: 0s - loss: 9.1394 - acc: 0.424 - ETA: 0s - loss: 9.1414 - acc: 0.423 - ETA: 0s - loss: 9.1204 - acc: 0.424 - ETA: 0s - loss: 9.1191 - acc: 0.424 - ETA: 0s - loss: 9.0962 - acc: 0.425 - ETA: 0s - loss: 9.1070 - acc: 0.424 - ETA: 0s - loss: 9.1159 - acc: 0.424 - ETA: 0s - loss: 9.1278 - acc: 0.422 - ETA: 0s - loss: 9.1459 - acc: 0.4214Epoch 00008: val_loss improved from 9.70676 to 9.60305, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 9.1537 - acc: 0.4210 - val_loss: 9.6031 - val_acc: 0.3521\n",
      "Epoch 10/20\n",
      "6620/6680 [============================>.] - ETA: 1s - loss: 4.8443 - acc: 0.700 - ETA: 1s - loss: 8.9508 - acc: 0.433 - ETA: 1s - loss: 8.9168 - acc: 0.434 - ETA: 1s - loss: 8.8882 - acc: 0.435 - ETA: 1s - loss: 8.8726 - acc: 0.437 - ETA: 1s - loss: 8.9149 - acc: 0.436 - ETA: 1s - loss: 9.0217 - acc: 0.431 - ETA: 1s - loss: 8.9443 - acc: 0.434 - ETA: 1s - loss: 8.9705 - acc: 0.433 - ETA: 1s - loss: 8.9693 - acc: 0.434 - ETA: 1s - loss: 8.9703 - acc: 0.432 - ETA: 1s - loss: 9.0220 - acc: 0.428 - ETA: 0s - loss: 9.0486 - acc: 0.427 - ETA: 0s - loss: 9.0566 - acc: 0.426 - ETA: 0s - loss: 9.0779 - acc: 0.424 - ETA: 0s - loss: 9.1612 - acc: 0.420 - ETA: 0s - loss: 9.1766 - acc: 0.419 - ETA: 0s - loss: 9.1849 - acc: 0.418 - ETA: 0s - loss: 9.1217 - acc: 0.422 - ETA: 0s - loss: 9.0876 - acc: 0.423 - ETA: 0s - loss: 9.1055 - acc: 0.422 - ETA: 0s - loss: 9.0802 - acc: 0.423 - ETA: 0s - loss: 9.0685 - acc: 0.423 - ETA: 0s - loss: 9.0728 - acc: 0.423 - ETA: 0s - loss: 9.0534 - acc: 0.424 - ETA: 0s - loss: 9.0504 - acc: 0.423 - ETA: 0s - loss: 9.0524 - acc: 0.423 - ETA: 0s - loss: 9.0346 - acc: 0.424 - ETA: 0s - loss: 9.0314 - acc: 0.424 - ETA: 0s - loss: 9.0588 - acc: 0.421 - ETA: 0s - loss: 9.0625 - acc: 0.4219Epoch 00009: val_loss improved from 9.60305 to 9.40749, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 9.0559 - acc: 0.4225 - val_loss: 9.4075 - val_acc: 0.3629\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6600/6680 [============================>.] - ETA: 1s - loss: 4.1966 - acc: 0.650 - ETA: 1s - loss: 8.1058 - acc: 0.483 - ETA: 1s - loss: 8.5078 - acc: 0.463 - ETA: 1s - loss: 8.7697 - acc: 0.447 - ETA: 1s - loss: 9.0187 - acc: 0.431 - ETA: 1s - loss: 8.9889 - acc: 0.433 - ETA: 1s - loss: 8.9169 - acc: 0.437 - ETA: 1s - loss: 8.8226 - acc: 0.444 - ETA: 1s - loss: 8.8351 - acc: 0.443 - ETA: 1s - loss: 8.8353 - acc: 0.442 - ETA: 1s - loss: 8.9250 - acc: 0.435 - ETA: 1s - loss: 8.9103 - acc: 0.436 - ETA: 0s - loss: 9.0063 - acc: 0.430 - ETA: 0s - loss: 8.9994 - acc: 0.431 - ETA: 0s - loss: 8.9950 - acc: 0.430 - ETA: 0s - loss: 9.0320 - acc: 0.427 - ETA: 0s - loss: 9.0513 - acc: 0.426 - ETA: 0s - loss: 9.0038 - acc: 0.428 - ETA: 0s - loss: 8.9697 - acc: 0.430 - ETA: 0s - loss: 8.9963 - acc: 0.429 - ETA: 0s - loss: 9.0157 - acc: 0.428 - ETA: 0s - loss: 9.0308 - acc: 0.428 - ETA: 0s - loss: 9.0055 - acc: 0.429 - ETA: 0s - loss: 8.9688 - acc: 0.432 - ETA: 0s - loss: 8.9576 - acc: 0.433 - ETA: 0s - loss: 8.9568 - acc: 0.433 - ETA: 0s - loss: 8.9624 - acc: 0.433 - ETA: 0s - loss: 8.9861 - acc: 0.432 - ETA: 0s - loss: 8.9750 - acc: 0.432 - ETA: 0s - loss: 8.9560 - acc: 0.433 - ETA: 0s - loss: 8.9426 - acc: 0.4345Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 8.9340 - acc: 0.4349 - val_loss: 9.4450 - val_acc: 0.3725\n",
      "Epoch 12/20\n",
      "6560/6680 [============================>.] - ETA: 1s - loss: 8.8670 - acc: 0.450 - ETA: 1s - loss: 8.9417 - acc: 0.438 - ETA: 1s - loss: 8.9091 - acc: 0.441 - ETA: 1s - loss: 8.8961 - acc: 0.444 - ETA: 1s - loss: 9.0241 - acc: 0.436 - ETA: 1s - loss: 9.0896 - acc: 0.431 - ETA: 1s - loss: 8.9648 - acc: 0.438 - ETA: 1s - loss: 8.9836 - acc: 0.436 - ETA: 1s - loss: 8.9871 - acc: 0.436 - ETA: 1s - loss: 8.8625 - acc: 0.443 - ETA: 0s - loss: 8.8414 - acc: 0.445 - ETA: 0s - loss: 8.8360 - acc: 0.444 - ETA: 0s - loss: 8.9202 - acc: 0.438 - ETA: 0s - loss: 8.9182 - acc: 0.438 - ETA: 0s - loss: 8.8888 - acc: 0.439 - ETA: 0s - loss: 8.8991 - acc: 0.439 - ETA: 0s - loss: 8.8975 - acc: 0.439 - ETA: 0s - loss: 8.9012 - acc: 0.439 - ETA: 0s - loss: 8.9024 - acc: 0.438 - ETA: 0s - loss: 8.9142 - acc: 0.437 - ETA: 0s - loss: 8.8889 - acc: 0.439 - ETA: 0s - loss: 8.8410 - acc: 0.441 - ETA: 0s - loss: 8.8399 - acc: 0.442 - ETA: 0s - loss: 8.8454 - acc: 0.441 - ETA: 0s - loss: 8.8654 - acc: 0.440 - ETA: 0s - loss: 8.8872 - acc: 0.438 - ETA: 0s - loss: 8.9051 - acc: 0.437 - ETA: 0s - loss: 8.8729 - acc: 0.439 - ETA: 0s - loss: 8.8713 - acc: 0.4395Epoch 00011: val_loss improved from 9.40749 to 9.40342, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 8.8721 - acc: 0.4394 - val_loss: 9.4034 - val_acc: 0.3557\n",
      "Epoch 13/20\n",
      "6640/6680 [============================>.] - ETA: 2s - loss: 8.2415 - acc: 0.400 - ETA: 1s - loss: 8.2353 - acc: 0.475 - ETA: 1s - loss: 8.6142 - acc: 0.452 - ETA: 1s - loss: 8.5369 - acc: 0.456 - ETA: 1s - loss: 8.7076 - acc: 0.446 - ETA: 1s - loss: 8.6297 - acc: 0.451 - ETA: 1s - loss: 8.5538 - acc: 0.458 - ETA: 1s - loss: 8.5912 - acc: 0.457 - ETA: 1s - loss: 8.5784 - acc: 0.458 - ETA: 1s - loss: 8.6658 - acc: 0.454 - ETA: 1s - loss: 8.7247 - acc: 0.450 - ETA: 0s - loss: 8.6706 - acc: 0.453 - ETA: 0s - loss: 8.8052 - acc: 0.445 - ETA: 0s - loss: 8.7544 - acc: 0.449 - ETA: 0s - loss: 8.7888 - acc: 0.445 - ETA: 0s - loss: 8.7633 - acc: 0.447 - ETA: 0s - loss: 8.7760 - acc: 0.447 - ETA: 0s - loss: 8.7779 - acc: 0.446 - ETA: 0s - loss: 8.7741 - acc: 0.447 - ETA: 0s - loss: 8.7649 - acc: 0.447 - ETA: 0s - loss: 8.7460 - acc: 0.448 - ETA: 0s - loss: 8.7588 - acc: 0.447 - ETA: 0s - loss: 8.7400 - acc: 0.449 - ETA: 0s - loss: 8.7682 - acc: 0.447 - ETA: 0s - loss: 8.7597 - acc: 0.448 - ETA: 0s - loss: 8.7534 - acc: 0.448 - ETA: 0s - loss: 8.7672 - acc: 0.447 - ETA: 0s - loss: 8.7803 - acc: 0.446 - ETA: 0s - loss: 8.8074 - acc: 0.444 - ETA: 0s - loss: 8.8067 - acc: 0.4443Epoch 00012: val_loss improved from 9.40342 to 9.27171, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 8.8023 - acc: 0.4446 - val_loss: 9.2717 - val_acc: 0.3641\n",
      "Epoch 14/20\n",
      "6640/6680 [============================>.] - ETA: 1s - loss: 8.0591 - acc: 0.500 - ETA: 1s - loss: 8.5532 - acc: 0.466 - ETA: 1s - loss: 8.8764 - acc: 0.444 - ETA: 1s - loss: 8.7660 - acc: 0.445 - ETA: 1s - loss: 8.8464 - acc: 0.440 - ETA: 1s - loss: 8.7339 - acc: 0.449 - ETA: 1s - loss: 8.5308 - acc: 0.462 - ETA: 1s - loss: 8.7159 - acc: 0.451 - ETA: 1s - loss: 8.6049 - acc: 0.455 - ETA: 1s - loss: 8.6298 - acc: 0.454 - ETA: 0s - loss: 8.6334 - acc: 0.454 - ETA: 0s - loss: 8.6347 - acc: 0.453 - ETA: 0s - loss: 8.6607 - acc: 0.451 - ETA: 0s - loss: 8.6733 - acc: 0.451 - ETA: 0s - loss: 8.7102 - acc: 0.448 - ETA: 0s - loss: 8.7072 - acc: 0.449 - ETA: 0s - loss: 8.7396 - acc: 0.447 - ETA: 0s - loss: 8.7598 - acc: 0.446 - ETA: 0s - loss: 8.7348 - acc: 0.448 - ETA: 0s - loss: 8.7396 - acc: 0.448 - ETA: 0s - loss: 8.7266 - acc: 0.449 - ETA: 0s - loss: 8.7192 - acc: 0.450 - ETA: 0s - loss: 8.7242 - acc: 0.449 - ETA: 0s - loss: 8.7410 - acc: 0.447 - ETA: 0s - loss: 8.7487 - acc: 0.447 - ETA: 0s - loss: 8.7352 - acc: 0.448 - ETA: 0s - loss: 8.7027 - acc: 0.449 - ETA: 0s - loss: 8.6736 - acc: 0.451 - ETA: 0s - loss: 8.6878 - acc: 0.449 - ETA: 0s - loss: 8.7136 - acc: 0.448 - ETA: 0s - loss: 8.7311 - acc: 0.4476Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 8.7250 - acc: 0.4479 - val_loss: 9.2884 - val_acc: 0.3677\n",
      "Epoch 15/20\n",
      "6640/6680 [============================>.] - ETA: 1s - loss: 6.4473 - acc: 0.600 - ETA: 1s - loss: 8.9580 - acc: 0.434 - ETA: 1s - loss: 9.2189 - acc: 0.420 - ETA: 1s - loss: 9.5161 - acc: 0.402 - ETA: 1s - loss: 9.3063 - acc: 0.416 - ETA: 1s - loss: 9.1618 - acc: 0.425 - ETA: 1s - loss: 9.1786 - acc: 0.422 - ETA: 1s - loss: 9.1243 - acc: 0.425 - ETA: 1s - loss: 9.0851 - acc: 0.427 - ETA: 1s - loss: 8.9836 - acc: 0.435 - ETA: 1s - loss: 8.8734 - acc: 0.442 - ETA: 0s - loss: 8.8410 - acc: 0.445 - ETA: 0s - loss: 8.8282 - acc: 0.445 - ETA: 0s - loss: 8.8390 - acc: 0.444 - ETA: 0s - loss: 8.8114 - acc: 0.446 - ETA: 0s - loss: 8.7582 - acc: 0.450 - ETA: 0s - loss: 8.7492 - acc: 0.451 - ETA: 0s - loss: 8.7776 - acc: 0.449 - ETA: 0s - loss: 8.7981 - acc: 0.448 - ETA: 0s - loss: 8.8493 - acc: 0.444 - ETA: 0s - loss: 8.8143 - acc: 0.446 - ETA: 0s - loss: 8.8305 - acc: 0.445 - ETA: 0s - loss: 8.8291 - acc: 0.445 - ETA: 0s - loss: 8.8087 - acc: 0.447 - ETA: 0s - loss: 8.8012 - acc: 0.447 - ETA: 0s - loss: 8.7818 - acc: 0.448 - ETA: 0s - loss: 8.7427 - acc: 0.451 - ETA: 0s - loss: 8.7356 - acc: 0.451 - ETA: 0s - loss: 8.7224 - acc: 0.452 - ETA: 0s - loss: 8.7019 - acc: 0.452 - ETA: 0s - loss: 8.6979 - acc: 0.4532Epoch 00014: val_loss improved from 9.27171 to 9.19524, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 8.6965 - acc: 0.4533 - val_loss: 9.1952 - val_acc: 0.3772\n",
      "Epoch 16/20\n",
      "6600/6680 [============================>.] - ETA: 1s - loss: 4.0301 - acc: 0.750 - ETA: 1s - loss: 8.0595 - acc: 0.500 - ETA: 1s - loss: 8.4680 - acc: 0.470 - ETA: 1s - loss: 8.3542 - acc: 0.475 - ETA: 1s - loss: 8.2085 - acc: 0.483 - ETA: 1s - loss: 8.2939 - acc: 0.480 - ETA: 1s - loss: 8.3675 - acc: 0.475 - ETA: 1s - loss: 8.4012 - acc: 0.474 - ETA: 1s - loss: 8.5210 - acc: 0.467 - ETA: 1s - loss: 8.5411 - acc: 0.466 - ETA: 1s - loss: 8.5949 - acc: 0.462 - ETA: 1s - loss: 8.5808 - acc: 0.463 - ETA: 1s - loss: 8.5679 - acc: 0.464 - ETA: 0s - loss: 8.5735 - acc: 0.463 - ETA: 0s - loss: 8.5806 - acc: 0.462 - ETA: 0s - loss: 8.5624 - acc: 0.463 - ETA: 0s - loss: 8.5325 - acc: 0.464 - ETA: 0s - loss: 8.5639 - acc: 0.463 - ETA: 0s - loss: 8.6121 - acc: 0.460 - ETA: 0s - loss: 8.6215 - acc: 0.460 - ETA: 0s - loss: 8.6238 - acc: 0.459 - ETA: 0s - loss: 8.6821 - acc: 0.455 - ETA: 0s - loss: 8.6637 - acc: 0.456 - ETA: 0s - loss: 8.6660 - acc: 0.456 - ETA: 0s - loss: 8.6641 - acc: 0.456 - ETA: 0s - loss: 8.6544 - acc: 0.457 - ETA: 0s - loss: 8.6878 - acc: 0.454 - ETA: 0s - loss: 8.7066 - acc: 0.453 - ETA: 0s - loss: 8.7145 - acc: 0.452 - ETA: 0s - loss: 8.6987 - acc: 0.452 - ETA: 0s - loss: 8.6689 - acc: 0.454 - ETA: 0s - loss: 8.6447 - acc: 0.4553Epoch 00015: val_loss did not improve\n",
      "6680/6680 [==============================] - 1s - loss: 8.6549 - acc: 0.4546 - val_loss: 9.1953 - val_acc: 0.3641\n",
      "Epoch 17/20\n",
      "6640/6680 [============================>.] - ETA: 1s - loss: 7.2548 - acc: 0.550 - ETA: 1s - loss: 8.7423 - acc: 0.438 - ETA: 1s - loss: 8.3443 - acc: 0.466 - ETA: 1s - loss: 8.5512 - acc: 0.455 - ETA: 1s - loss: 8.3755 - acc: 0.464 - ETA: 1s - loss: 8.5296 - acc: 0.456 - ETA: 1s - loss: 8.5392 - acc: 0.455 - ETA: 1s - loss: 8.4907 - acc: 0.459 - ETA: 1s - loss: 8.5085 - acc: 0.459 - ETA: 1s - loss: 8.5800 - acc: 0.454 - ETA: 0s - loss: 8.5593 - acc: 0.456 - ETA: 0s - loss: 8.5792 - acc: 0.455 - ETA: 0s - loss: 8.5921 - acc: 0.454 - ETA: 0s - loss: 8.5714 - acc: 0.455 - ETA: 0s - loss: 8.5434 - acc: 0.456 - ETA: 0s - loss: 8.4571 - acc: 0.461 - ETA: 0s - loss: 8.5525 - acc: 0.456 - ETA: 0s - loss: 8.5758 - acc: 0.454 - ETA: 0s - loss: 8.5710 - acc: 0.455 - ETA: 0s - loss: 8.5546 - acc: 0.456 - ETA: 0s - loss: 8.5357 - acc: 0.457 - ETA: 0s - loss: 8.5310 - acc: 0.458 - ETA: 0s - loss: 8.5284 - acc: 0.458 - ETA: 0s - loss: 8.4883 - acc: 0.460 - ETA: 0s - loss: 8.4763 - acc: 0.461 - ETA: 0s - loss: 8.4991 - acc: 0.460 - ETA: 0s - loss: 8.4921 - acc: 0.461 - ETA: 0s - loss: 8.4968 - acc: 0.460 - ETA: 0s - loss: 8.4964 - acc: 0.4604Epoch 00016: val_loss improved from 9.19524 to 9.15774, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 8.4861 - acc: 0.4611 - val_loss: 9.1577 - val_acc: 0.3737\n",
      "Epoch 18/20\n",
      "6640/6680 [============================>.] - ETA: 1s - loss: 8.3073 - acc: 0.450 - ETA: 1s - loss: 7.2080 - acc: 0.541 - ETA: 1s - loss: 8.1784 - acc: 0.475 - ETA: 1s - loss: 8.2245 - acc: 0.471 - ETA: 1s - loss: 8.3567 - acc: 0.467 - ETA: 1s - loss: 8.4170 - acc: 0.461 - ETA: 1s - loss: 8.4440 - acc: 0.459 - ETA: 1s - loss: 8.4805 - acc: 0.456 - ETA: 1s - loss: 8.4916 - acc: 0.455 - ETA: 1s - loss: 8.3848 - acc: 0.462 - ETA: 0s - loss: 8.2678 - acc: 0.469 - ETA: 0s - loss: 8.2927 - acc: 0.469 - ETA: 0s - loss: 8.2467 - acc: 0.472 - ETA: 0s - loss: 8.2329 - acc: 0.473 - ETA: 0s - loss: 8.1689 - acc: 0.476 - ETA: 0s - loss: 8.2160 - acc: 0.474 - ETA: 0s - loss: 8.2197 - acc: 0.473 - ETA: 0s - loss: 8.2231 - acc: 0.473 - ETA: 0s - loss: 8.2610 - acc: 0.471 - ETA: 0s - loss: 8.2368 - acc: 0.472 - ETA: 0s - loss: 8.2688 - acc: 0.470 - ETA: 0s - loss: 8.2689 - acc: 0.470 - ETA: 0s - loss: 8.2697 - acc: 0.470 - ETA: 0s - loss: 8.2673 - acc: 0.470 - ETA: 0s - loss: 8.2514 - acc: 0.471 - ETA: 0s - loss: 8.2549 - acc: 0.470 - ETA: 0s - loss: 8.2541 - acc: 0.470 - ETA: 0s - loss: 8.2233 - acc: 0.472 - ETA: 0s - loss: 8.2430 - acc: 0.4703Epoch 00017: val_loss improved from 9.15774 to 8.80875, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 8.2456 - acc: 0.4702 - val_loss: 8.8088 - val_acc: 0.3928\n",
      "Epoch 19/20\n",
      "6540/6680 [============================>.] - ETA: 1s - loss: 8.7238 - acc: 0.400 - ETA: 1s - loss: 8.6088 - acc: 0.445 - ETA: 1s - loss: 8.4441 - acc: 0.458 - ETA: 1s - loss: 8.1452 - acc: 0.475 - ETA: 1s - loss: 8.1449 - acc: 0.472 - ETA: 1s - loss: 8.1188 - acc: 0.473 - ETA: 1s - loss: 8.0612 - acc: 0.475 - ETA: 1s - loss: 8.1304 - acc: 0.472 - ETA: 1s - loss: 8.0730 - acc: 0.476 - ETA: 1s - loss: 8.0994 - acc: 0.475 - ETA: 1s - loss: 8.0867 - acc: 0.477 - ETA: 0s - loss: 8.1243 - acc: 0.475 - ETA: 0s - loss: 8.1902 - acc: 0.472 - ETA: 0s - loss: 8.2080 - acc: 0.471 - ETA: 0s - loss: 8.1343 - acc: 0.476 - ETA: 0s - loss: 8.1688 - acc: 0.475 - ETA: 0s - loss: 8.2053 - acc: 0.473 - ETA: 0s - loss: 8.2034 - acc: 0.472 - ETA: 0s - loss: 8.1572 - acc: 0.475 - ETA: 0s - loss: 8.1013 - acc: 0.478 - ETA: 0s - loss: 8.1011 - acc: 0.479 - ETA: 0s - loss: 8.0661 - acc: 0.481 - ETA: 0s - loss: 8.0725 - acc: 0.481 - ETA: 0s - loss: 8.0609 - acc: 0.483 - ETA: 0s - loss: 8.0314 - acc: 0.485 - ETA: 0s - loss: 8.0394 - acc: 0.484 - ETA: 0s - loss: 8.0111 - acc: 0.486 - ETA: 0s - loss: 8.0075 - acc: 0.486 - ETA: 0s - loss: 8.0157 - acc: 0.486 - ETA: 0s - loss: 8.0347 - acc: 0.4850Epoch 00018: val_loss improved from 8.80875 to 8.65667, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 8.0459 - acc: 0.4844 - val_loss: 8.6567 - val_acc: 0.4120\n",
      "Epoch 20/20\n",
      "6560/6680 [============================>.] - ETA: 1s - loss: 7.2534 - acc: 0.550 - ETA: 1s - loss: 8.3089 - acc: 0.479 - ETA: 1s - loss: 8.0704 - acc: 0.489 - ETA: 1s - loss: 8.2166 - acc: 0.479 - ETA: 1s - loss: 8.2310 - acc: 0.473 - ETA: 1s - loss: 8.0369 - acc: 0.485 - ETA: 1s - loss: 8.0136 - acc: 0.487 - ETA: 1s - loss: 7.9001 - acc: 0.495 - ETA: 1s - loss: 7.9225 - acc: 0.493 - ETA: 1s - loss: 7.9529 - acc: 0.493 - ETA: 1s - loss: 7.9528 - acc: 0.493 - ETA: 1s - loss: 7.9496 - acc: 0.492 - ETA: 0s - loss: 7.9352 - acc: 0.493 - ETA: 0s - loss: 7.8993 - acc: 0.494 - ETA: 0s - loss: 7.9345 - acc: 0.493 - ETA: 0s - loss: 8.0085 - acc: 0.489 - ETA: 0s - loss: 7.9773 - acc: 0.491 - ETA: 0s - loss: 7.9079 - acc: 0.495 - ETA: 0s - loss: 7.9085 - acc: 0.495 - ETA: 0s - loss: 7.8941 - acc: 0.496 - ETA: 0s - loss: 7.9062 - acc: 0.494 - ETA: 0s - loss: 7.9053 - acc: 0.493 - ETA: 0s - loss: 7.8834 - acc: 0.495 - ETA: 0s - loss: 7.8975 - acc: 0.494 - ETA: 0s - loss: 7.8615 - acc: 0.495 - ETA: 0s - loss: 7.8690 - acc: 0.494 - ETA: 0s - loss: 7.8537 - acc: 0.495 - ETA: 0s - loss: 7.8553 - acc: 0.495 - ETA: 0s - loss: 7.8689 - acc: 0.494 - ETA: 0s - loss: 7.8726 - acc: 0.494 - ETA: 0s - loss: 7.8866 - acc: 0.4931Epoch 00019: val_loss improved from 8.65667 to 8.42148, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "6680/6680 [==============================] - 1s - loss: 7.8843 - acc: 0.4930 - val_loss: 8.4215 - val_acc: 0.4144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20ea2bb7f60>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG16_model.fit(train_VGG16, train_targets, \n",
    "          validation_data=(valid_VGG16, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_model.load_weights('saved_models/weights.best.VGG16.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 41.1483%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Dog Breed with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_bottleneck_features import *\n",
    "\n",
    "def VGG16_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = VGG16_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## Step 5: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "\n",
    "You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
    "\n",
    "In Step 4, we used transfer learning to create a CNN using VGG-16 bottleneck features.  In this section, you must use the bottleneck features from a different pre-trained model.  To make things easier for you, we have pre-computed the features for all of the networks that are currently available in Keras:\n",
    "- [VGG-19](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz) bottleneck features\n",
    "- [ResNet-50](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz) bottleneck features\n",
    "- [Inception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz) bottleneck features\n",
    "- [Xception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz) bottleneck features\n",
    "\n",
    "The files are encoded as such:\n",
    "\n",
    "    Dog{network}Data.npz\n",
    "    \n",
    "where `{network}`, in the above filename, can be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`.  Pick one of the above architectures, download the corresponding bottleneck features, and store the downloaded file in the `bottleneck_features/` folder in the repository.\n",
    "\n",
    "### (IMPLEMENTATION) Obtain Bottleneck Features\n",
    "\n",
    "In the code block below, extract the bottleneck features corresponding to the train, test, and validation sets by running the following:\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{network}Data.npz')\n",
    "    train_{network} = bottleneck_features['train']\n",
    "    valid_{network} = bottleneck_features['valid']\n",
    "    test_{network} = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Obtain bottleneck features from another pre-trained CNN.\n",
    "bottleneck_features = np.load('bottleneck_features/DogXceptionData.npz')\n",
    "train_Xception = bottleneck_features['train']\n",
    "valid_Xception = bottleneck_features['valid']\n",
    "test_Xception = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        <your model's name>.summary()\n",
    "   \n",
    "__Question 5:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem.\n",
    "\n",
    "__Answer:__ We started with the Xception model and the output is being fed to a global average pooling layer, which is further connected to a dense layer. The Xception model is pretrained with data from ImageNet, it is highly likely that the first few convolution layer is capable of identifying simple features (e.g. lines, shapes) within an image, which should be relevant to our task. I believe one dense layer would be sufficient provided that our dog images are possibly similar to subsets from ImageNet. The dense layer has 133 nodes which correspond to the number of dog categories of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_2 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 133)               272517    \n",
      "=================================================================\n",
      "Total params: 272,517.0\n",
      "Trainable params: 272,517.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### TODO: Define your architecture.\n",
    "Xception_model = Sequential()\n",
    "Xception_model.add(GlobalAveragePooling2D(input_shape=train_Xception.shape[1:]))\n",
    "Xception_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "Xception_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Compile the model.\n",
    "Xception_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.  \n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6600/6680 [============================>.] - ETA: 81s - loss: 5.0702 - acc: 0.05 - ETA: 20s - loss: 4.9133 - acc: 0.07 - ETA: 14s - loss: 4.7058 - acc: 0.10 - ETA: 11s - loss: 4.5643 - acc: 0.16 - ETA: 9s - loss: 4.2989 - acc: 0.2156 - ETA: 8s - loss: 4.1149 - acc: 0.237 - ETA: 7s - loss: 3.9385 - acc: 0.270 - ETA: 7s - loss: 3.7996 - acc: 0.289 - ETA: 6s - loss: 3.6525 - acc: 0.309 - ETA: 6s - loss: 3.4978 - acc: 0.330 - ETA: 5s - loss: 3.3436 - acc: 0.348 - ETA: 5s - loss: 3.1977 - acc: 0.377 - ETA: 5s - loss: 3.0799 - acc: 0.395 - ETA: 5s - loss: 2.9577 - acc: 0.414 - ETA: 5s - loss: 2.8262 - acc: 0.436 - ETA: 4s - loss: 2.7267 - acc: 0.449 - ETA: 4s - loss: 2.6273 - acc: 0.467 - ETA: 4s - loss: 2.5367 - acc: 0.485 - ETA: 4s - loss: 2.4553 - acc: 0.498 - ETA: 4s - loss: 2.3818 - acc: 0.510 - ETA: 4s - loss: 2.3159 - acc: 0.518 - ETA: 4s - loss: 2.2542 - acc: 0.530 - ETA: 4s - loss: 2.1962 - acc: 0.536 - ETA: 3s - loss: 2.1302 - acc: 0.547 - ETA: 3s - loss: 2.0802 - acc: 0.556 - ETA: 3s - loss: 2.0278 - acc: 0.566 - ETA: 3s - loss: 1.9856 - acc: 0.573 - ETA: 3s - loss: 1.9464 - acc: 0.580 - ETA: 3s - loss: 1.9171 - acc: 0.585 - ETA: 3s - loss: 1.8796 - acc: 0.591 - ETA: 3s - loss: 1.8400 - acc: 0.599 - ETA: 3s - loss: 1.8046 - acc: 0.605 - ETA: 3s - loss: 1.7692 - acc: 0.610 - ETA: 3s - loss: 1.7420 - acc: 0.614 - ETA: 3s - loss: 1.7190 - acc: 0.618 - ETA: 3s - loss: 1.6878 - acc: 0.623 - ETA: 2s - loss: 1.6577 - acc: 0.630 - ETA: 2s - loss: 1.6302 - acc: 0.632 - ETA: 2s - loss: 1.6107 - acc: 0.635 - ETA: 2s - loss: 1.5859 - acc: 0.638 - ETA: 2s - loss: 1.5515 - acc: 0.644 - ETA: 2s - loss: 1.5306 - acc: 0.648 - ETA: 2s - loss: 1.5110 - acc: 0.651 - ETA: 2s - loss: 1.4918 - acc: 0.654 - ETA: 2s - loss: 1.4763 - acc: 0.657 - ETA: 2s - loss: 1.4576 - acc: 0.660 - ETA: 2s - loss: 1.4429 - acc: 0.662 - ETA: 2s - loss: 1.4279 - acc: 0.663 - ETA: 2s - loss: 1.4137 - acc: 0.665 - ETA: 2s - loss: 1.3993 - acc: 0.667 - ETA: 2s - loss: 1.3839 - acc: 0.671 - ETA: 2s - loss: 1.3655 - acc: 0.675 - ETA: 1s - loss: 1.3481 - acc: 0.679 - ETA: 1s - loss: 1.3374 - acc: 0.679 - ETA: 1s - loss: 1.3285 - acc: 0.681 - ETA: 1s - loss: 1.3221 - acc: 0.681 - ETA: 1s - loss: 1.3119 - acc: 0.683 - ETA: 1s - loss: 1.3012 - acc: 0.686 - ETA: 1s - loss: 1.2902 - acc: 0.687 - ETA: 1s - loss: 1.2763 - acc: 0.691 - ETA: 1s - loss: 1.2629 - acc: 0.693 - ETA: 1s - loss: 1.2512 - acc: 0.695 - ETA: 1s - loss: 1.2362 - acc: 0.698 - ETA: 1s - loss: 1.2206 - acc: 0.702 - ETA: 1s - loss: 1.2093 - acc: 0.704 - ETA: 1s - loss: 1.2007 - acc: 0.705 - ETA: 1s - loss: 1.1890 - acc: 0.708 - ETA: 1s - loss: 1.1792 - acc: 0.710 - ETA: 0s - loss: 1.1700 - acc: 0.711 - ETA: 0s - loss: 1.1606 - acc: 0.713 - ETA: 0s - loss: 1.1540 - acc: 0.715 - ETA: 0s - loss: 1.1449 - acc: 0.717 - ETA: 0s - loss: 1.1349 - acc: 0.718 - ETA: 0s - loss: 1.1275 - acc: 0.720 - ETA: 0s - loss: 1.1189 - acc: 0.722 - ETA: 0s - loss: 1.1112 - acc: 0.724 - ETA: 0s - loss: 1.1022 - acc: 0.726 - ETA: 0s - loss: 1.0943 - acc: 0.726 - ETA: 0s - loss: 1.0875 - acc: 0.728 - ETA: 0s - loss: 1.0834 - acc: 0.729 - ETA: 0s - loss: 1.0786 - acc: 0.730 - ETA: 0s - loss: 1.0730 - acc: 0.731 - ETA: 0s - loss: 1.0701 - acc: 0.731 - ETA: 0s - loss: 1.0671 - acc: 0.731 - ETA: 0s - loss: 1.0614 - acc: 0.732 - ETA: 0s - loss: 1.0538 - acc: 0.7341Epoch 00000: val_loss improved from inf to 0.52909, saving model to saved_models/weights.best.Xception.hdf5\n",
      "6680/6680 [==============================] - 5s - loss: 1.0499 - acc: 0.7346 - val_loss: 0.5291 - val_acc: 0.8275\n",
      "Epoch 2/20\n",
      "6640/6680 [============================>.] - ETA: 3s - loss: 0.7151 - acc: 0.750 - ETA: 3s - loss: 0.3880 - acc: 0.883 - ETA: 4s - loss: 0.4719 - acc: 0.855 - ETA: 4s - loss: 0.4296 - acc: 0.869 - ETA: 4s - loss: 0.4283 - acc: 0.865 - ETA: 4s - loss: 0.4564 - acc: 0.862 - ETA: 4s - loss: 0.4231 - acc: 0.868 - ETA: 4s - loss: 0.4096 - acc: 0.873 - ETA: 4s - loss: 0.4009 - acc: 0.875 - ETA: 4s - loss: 0.4101 - acc: 0.875 - ETA: 4s - loss: 0.4311 - acc: 0.865 - ETA: 4s - loss: 0.4334 - acc: 0.858 - ETA: 4s - loss: 0.4329 - acc: 0.857 - ETA: 3s - loss: 0.4183 - acc: 0.861 - ETA: 3s - loss: 0.4154 - acc: 0.861 - ETA: 3s - loss: 0.4139 - acc: 0.861 - ETA: 3s - loss: 0.4080 - acc: 0.861 - ETA: 3s - loss: 0.4061 - acc: 0.862 - ETA: 3s - loss: 0.4014 - acc: 0.864 - ETA: 3s - loss: 0.4021 - acc: 0.863 - ETA: 3s - loss: 0.3994 - acc: 0.865 - ETA: 3s - loss: 0.3933 - acc: 0.868 - ETA: 3s - loss: 0.3890 - acc: 0.869 - ETA: 3s - loss: 0.3903 - acc: 0.867 - ETA: 3s - loss: 0.3963 - acc: 0.865 - ETA: 3s - loss: 0.3986 - acc: 0.864 - ETA: 3s - loss: 0.4001 - acc: 0.863 - ETA: 3s - loss: 0.4008 - acc: 0.862 - ETA: 3s - loss: 0.4038 - acc: 0.862 - ETA: 3s - loss: 0.4014 - acc: 0.864 - ETA: 3s - loss: 0.3967 - acc: 0.866 - ETA: 2s - loss: 0.4012 - acc: 0.864 - ETA: 2s - loss: 0.4026 - acc: 0.865 - ETA: 2s - loss: 0.4034 - acc: 0.864 - ETA: 2s - loss: 0.4011 - acc: 0.865 - ETA: 2s - loss: 0.3966 - acc: 0.868 - ETA: 2s - loss: 0.3944 - acc: 0.869 - ETA: 2s - loss: 0.3939 - acc: 0.869 - ETA: 2s - loss: 0.3910 - acc: 0.869 - ETA: 2s - loss: 0.3933 - acc: 0.869 - ETA: 2s - loss: 0.3974 - acc: 0.867 - ETA: 2s - loss: 0.3975 - acc: 0.869 - ETA: 2s - loss: 0.3987 - acc: 0.870 - ETA: 2s - loss: 0.3931 - acc: 0.871 - ETA: 2s - loss: 0.3929 - acc: 0.872 - ETA: 2s - loss: 0.3936 - acc: 0.871 - ETA: 2s - loss: 0.3927 - acc: 0.872 - ETA: 2s - loss: 0.3930 - acc: 0.872 - ETA: 1s - loss: 0.3908 - acc: 0.872 - ETA: 1s - loss: 0.3905 - acc: 0.872 - ETA: 1s - loss: 0.3880 - acc: 0.872 - ETA: 1s - loss: 0.3897 - acc: 0.872 - ETA: 1s - loss: 0.3872 - acc: 0.874 - ETA: 1s - loss: 0.3896 - acc: 0.872 - ETA: 1s - loss: 0.3886 - acc: 0.873 - ETA: 1s - loss: 0.3885 - acc: 0.873 - ETA: 1s - loss: 0.3922 - acc: 0.872 - ETA: 1s - loss: 0.3915 - acc: 0.872 - ETA: 1s - loss: 0.3910 - acc: 0.873 - ETA: 1s - loss: 0.3913 - acc: 0.872 - ETA: 1s - loss: 0.3896 - acc: 0.873 - ETA: 1s - loss: 0.3924 - acc: 0.872 - ETA: 1s - loss: 0.3929 - acc: 0.872 - ETA: 1s - loss: 0.3946 - acc: 0.871 - ETA: 1s - loss: 0.3949 - acc: 0.872 - ETA: 0s - loss: 0.3961 - acc: 0.872 - ETA: 0s - loss: 0.3966 - acc: 0.872 - ETA: 0s - loss: 0.3966 - acc: 0.872 - ETA: 0s - loss: 0.3959 - acc: 0.872 - ETA: 0s - loss: 0.3961 - acc: 0.872 - ETA: 0s - loss: 0.3961 - acc: 0.872 - ETA: 0s - loss: 0.3959 - acc: 0.871 - ETA: 0s - loss: 0.3957 - acc: 0.872 - ETA: 0s - loss: 0.3942 - acc: 0.872 - ETA: 0s - loss: 0.3939 - acc: 0.872 - ETA: 0s - loss: 0.3951 - acc: 0.872 - ETA: 0s - loss: 0.3921 - acc: 0.873 - ETA: 0s - loss: 0.3910 - acc: 0.874 - ETA: 0s - loss: 0.3918 - acc: 0.874 - ETA: 0s - loss: 0.3916 - acc: 0.873 - ETA: 0s - loss: 0.3926 - acc: 0.873 - ETA: 0s - loss: 0.3921 - acc: 0.8735Epoch 00001: val_loss improved from 0.52909 to 0.46906, saving model to saved_models/weights.best.Xception.hdf5\n",
      "6680/6680 [==============================] - 4s - loss: 0.3935 - acc: 0.8729 - val_loss: 0.4691 - val_acc: 0.8551\n",
      "Epoch 3/20\n",
      "6640/6680 [============================>.] - ETA: 6s - loss: 0.5524 - acc: 0.900 - ETA: 5s - loss: 0.3926 - acc: 0.900 - ETA: 5s - loss: 0.3250 - acc: 0.912 - ETA: 5s - loss: 0.3180 - acc: 0.918 - ETA: 5s - loss: 0.3272 - acc: 0.896 - ETA: 5s - loss: 0.3482 - acc: 0.889 - ETA: 4s - loss: 0.3571 - acc: 0.889 - ETA: 4s - loss: 0.3559 - acc: 0.890 - ETA: 4s - loss: 0.3705 - acc: 0.887 - ETA: 4s - loss: 0.3581 - acc: 0.890 - ETA: 4s - loss: 0.3632 - acc: 0.886 - ETA: 4s - loss: 0.3630 - acc: 0.890 - ETA: 4s - loss: 0.3483 - acc: 0.892 - ETA: 4s - loss: 0.3450 - acc: 0.890 - ETA: 4s - loss: 0.3288 - acc: 0.897 - ETA: 4s - loss: 0.3185 - acc: 0.900 - ETA: 4s - loss: 0.3260 - acc: 0.897 - ETA: 4s - loss: 0.3165 - acc: 0.900 - ETA: 4s - loss: 0.3182 - acc: 0.900 - ETA: 4s - loss: 0.3156 - acc: 0.900 - ETA: 3s - loss: 0.3165 - acc: 0.900 - ETA: 3s - loss: 0.3217 - acc: 0.900 - ETA: 3s - loss: 0.3200 - acc: 0.898 - ETA: 3s - loss: 0.3091 - acc: 0.902 - ETA: 3s - loss: 0.3117 - acc: 0.900 - ETA: 3s - loss: 0.3103 - acc: 0.901 - ETA: 3s - loss: 0.3125 - acc: 0.900 - ETA: 3s - loss: 0.3107 - acc: 0.900 - ETA: 3s - loss: 0.3079 - acc: 0.901 - ETA: 3s - loss: 0.3063 - acc: 0.902 - ETA: 3s - loss: 0.3076 - acc: 0.902 - ETA: 3s - loss: 0.3087 - acc: 0.900 - ETA: 3s - loss: 0.3140 - acc: 0.898 - ETA: 3s - loss: 0.3128 - acc: 0.898 - ETA: 3s - loss: 0.3164 - acc: 0.897 - ETA: 2s - loss: 0.3185 - acc: 0.896 - ETA: 2s - loss: 0.3170 - acc: 0.896 - ETA: 2s - loss: 0.3140 - acc: 0.898 - ETA: 2s - loss: 0.3118 - acc: 0.899 - ETA: 2s - loss: 0.3115 - acc: 0.899 - ETA: 2s - loss: 0.3097 - acc: 0.900 - ETA: 2s - loss: 0.3120 - acc: 0.899 - ETA: 2s - loss: 0.3143 - acc: 0.899 - ETA: 2s - loss: 0.3159 - acc: 0.899 - ETA: 2s - loss: 0.3155 - acc: 0.900 - ETA: 2s - loss: 0.3166 - acc: 0.900 - ETA: 2s - loss: 0.3171 - acc: 0.900 - ETA: 2s - loss: 0.3186 - acc: 0.900 - ETA: 2s - loss: 0.3185 - acc: 0.899 - ETA: 2s - loss: 0.3184 - acc: 0.900 - ETA: 2s - loss: 0.3174 - acc: 0.900 - ETA: 2s - loss: 0.3148 - acc: 0.901 - ETA: 2s - loss: 0.3146 - acc: 0.900 - ETA: 1s - loss: 0.3162 - acc: 0.900 - ETA: 1s - loss: 0.3154 - acc: 0.900 - ETA: 1s - loss: 0.3164 - acc: 0.900 - ETA: 1s - loss: 0.3184 - acc: 0.899 - ETA: 1s - loss: 0.3192 - acc: 0.898 - ETA: 1s - loss: 0.3194 - acc: 0.898 - ETA: 1s - loss: 0.3184 - acc: 0.899 - ETA: 1s - loss: 0.3177 - acc: 0.899 - ETA: 1s - loss: 0.3177 - acc: 0.898 - ETA: 1s - loss: 0.3198 - acc: 0.898 - ETA: 1s - loss: 0.3174 - acc: 0.899 - ETA: 1s - loss: 0.3161 - acc: 0.900 - ETA: 1s - loss: 0.3144 - acc: 0.900 - ETA: 1s - loss: 0.3213 - acc: 0.900 - ETA: 1s - loss: 0.3191 - acc: 0.900 - ETA: 1s - loss: 0.3176 - acc: 0.900 - ETA: 1s - loss: 0.3163 - acc: 0.901 - ETA: 1s - loss: 0.3146 - acc: 0.901 - ETA: 1s - loss: 0.3139 - acc: 0.901 - ETA: 0s - loss: 0.3135 - acc: 0.901 - ETA: 0s - loss: 0.3151 - acc: 0.901 - ETA: 0s - loss: 0.3150 - acc: 0.900 - ETA: 0s - loss: 0.3159 - acc: 0.900 - ETA: 0s - loss: 0.3143 - acc: 0.901 - ETA: 0s - loss: 0.3151 - acc: 0.900 - ETA: 0s - loss: 0.3164 - acc: 0.900 - ETA: 0s - loss: 0.3166 - acc: 0.900 - ETA: 0s - loss: 0.3174 - acc: 0.899 - ETA: 0s - loss: 0.3166 - acc: 0.899 - ETA: 0s - loss: 0.3168 - acc: 0.899 - ETA: 0s - loss: 0.3176 - acc: 0.899 - ETA: 0s - loss: 0.3205 - acc: 0.898 - ETA: 0s - loss: 0.3216 - acc: 0.898 - ETA: 0s - loss: 0.3208 - acc: 0.898 - ETA: 0s - loss: 0.3251 - acc: 0.897 - ETA: 0s - loss: 0.3260 - acc: 0.8973Epoch 00002: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.3256 - acc: 0.8975 - val_loss: 0.4703 - val_acc: 0.8539\n",
      "Epoch 4/20\n",
      "6620/6680 [============================>.] - ETA: 3s - loss: 0.0648 - acc: 1.000 - ETA: 4s - loss: 0.1360 - acc: 0.970 - ETA: 4s - loss: 0.1511 - acc: 0.966 - ETA: 4s - loss: 0.1982 - acc: 0.950 - ETA: 4s - loss: 0.1859 - acc: 0.952 - ETA: 4s - loss: 0.2227 - acc: 0.940 - ETA: 4s - loss: 0.2187 - acc: 0.942 - ETA: 4s - loss: 0.2232 - acc: 0.939 - ETA: 4s - loss: 0.2471 - acc: 0.931 - ETA: 4s - loss: 0.2587 - acc: 0.928 - ETA: 4s - loss: 0.2685 - acc: 0.928 - ETA: 4s - loss: 0.2597 - acc: 0.930 - ETA: 4s - loss: 0.2698 - acc: 0.929 - ETA: 3s - loss: 0.2843 - acc: 0.919 - ETA: 3s - loss: 0.2790 - acc: 0.921 - ETA: 3s - loss: 0.2730 - acc: 0.923 - ETA: 3s - loss: 0.2665 - acc: 0.923 - ETA: 3s - loss: 0.2718 - acc: 0.923 - ETA: 3s - loss: 0.2729 - acc: 0.923 - ETA: 3s - loss: 0.2775 - acc: 0.920 - ETA: 3s - loss: 0.2746 - acc: 0.921 - ETA: 3s - loss: 0.2799 - acc: 0.919 - ETA: 3s - loss: 0.2824 - acc: 0.917 - ETA: 3s - loss: 0.2788 - acc: 0.917 - ETA: 3s - loss: 0.2749 - acc: 0.917 - ETA: 3s - loss: 0.2735 - acc: 0.917 - ETA: 3s - loss: 0.2718 - acc: 0.917 - ETA: 2s - loss: 0.2685 - acc: 0.918 - ETA: 2s - loss: 0.2692 - acc: 0.917 - ETA: 2s - loss: 0.2665 - acc: 0.918 - ETA: 2s - loss: 0.2647 - acc: 0.918 - ETA: 2s - loss: 0.2648 - acc: 0.916 - ETA: 2s - loss: 0.2626 - acc: 0.917 - ETA: 2s - loss: 0.2636 - acc: 0.918 - ETA: 2s - loss: 0.2637 - acc: 0.917 - ETA: 2s - loss: 0.2650 - acc: 0.917 - ETA: 2s - loss: 0.2640 - acc: 0.917 - ETA: 2s - loss: 0.2591 - acc: 0.919 - ETA: 2s - loss: 0.2605 - acc: 0.918 - ETA: 2s - loss: 0.2572 - acc: 0.919 - ETA: 2s - loss: 0.2569 - acc: 0.919 - ETA: 2s - loss: 0.2563 - acc: 0.920 - ETA: 2s - loss: 0.2601 - acc: 0.918 - ETA: 2s - loss: 0.2587 - acc: 0.919 - ETA: 2s - loss: 0.2571 - acc: 0.919 - ETA: 2s - loss: 0.2550 - acc: 0.920 - ETA: 1s - loss: 0.2529 - acc: 0.920 - ETA: 1s - loss: 0.2534 - acc: 0.920 - ETA: 1s - loss: 0.2570 - acc: 0.920 - ETA: 1s - loss: 0.2578 - acc: 0.919 - ETA: 1s - loss: 0.2561 - acc: 0.919 - ETA: 1s - loss: 0.2556 - acc: 0.920 - ETA: 1s - loss: 0.2538 - acc: 0.921 - ETA: 1s - loss: 0.2560 - acc: 0.920 - ETA: 1s - loss: 0.2578 - acc: 0.921 - ETA: 1s - loss: 0.2558 - acc: 0.921 - ETA: 1s - loss: 0.2593 - acc: 0.921 - ETA: 1s - loss: 0.2576 - acc: 0.921 - ETA: 1s - loss: 0.2575 - acc: 0.921 - ETA: 1s - loss: 0.2587 - acc: 0.919 - ETA: 1s - loss: 0.2591 - acc: 0.919 - ETA: 1s - loss: 0.2595 - acc: 0.919 - ETA: 1s - loss: 0.2585 - acc: 0.919 - ETA: 1s - loss: 0.2636 - acc: 0.917 - ETA: 0s - loss: 0.2643 - acc: 0.917 - ETA: 0s - loss: 0.2660 - acc: 0.917 - ETA: 0s - loss: 0.2667 - acc: 0.916 - ETA: 0s - loss: 0.2673 - acc: 0.916 - ETA: 0s - loss: 0.2679 - acc: 0.916 - ETA: 0s - loss: 0.2673 - acc: 0.916 - ETA: 0s - loss: 0.2676 - acc: 0.916 - ETA: 0s - loss: 0.2677 - acc: 0.915 - ETA: 0s - loss: 0.2691 - acc: 0.915 - ETA: 0s - loss: 0.2681 - acc: 0.916 - ETA: 0s - loss: 0.2678 - acc: 0.916 - ETA: 0s - loss: 0.2673 - acc: 0.915 - ETA: 0s - loss: 0.2660 - acc: 0.916 - ETA: 0s - loss: 0.2676 - acc: 0.915 - ETA: 0s - loss: 0.2679 - acc: 0.915 - ETA: 0s - loss: 0.2667 - acc: 0.915 - ETA: 0s - loss: 0.2672 - acc: 0.915 - ETA: 0s - loss: 0.2693 - acc: 0.9150Epoch 00003: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.2696 - acc: 0.9147 - val_loss: 0.4905 - val_acc: 0.8515\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6600/6680 [============================>.] - ETA: 3s - loss: 0.1583 - acc: 0.950 - ETA: 4s - loss: 0.1594 - acc: 0.960 - ETA: 4s - loss: 0.1840 - acc: 0.944 - ETA: 4s - loss: 0.2383 - acc: 0.930 - ETA: 4s - loss: 0.2314 - acc: 0.929 - ETA: 4s - loss: 0.2300 - acc: 0.923 - ETA: 4s - loss: 0.2475 - acc: 0.920 - ETA: 4s - loss: 0.2446 - acc: 0.919 - ETA: 4s - loss: 0.2518 - acc: 0.919 - ETA: 4s - loss: 0.2418 - acc: 0.923 - ETA: 3s - loss: 0.2371 - acc: 0.926 - ETA: 3s - loss: 0.2338 - acc: 0.929 - ETA: 3s - loss: 0.2274 - acc: 0.930 - ETA: 3s - loss: 0.2419 - acc: 0.924 - ETA: 3s - loss: 0.2340 - acc: 0.926 - ETA: 3s - loss: 0.2312 - acc: 0.925 - ETA: 3s - loss: 0.2317 - acc: 0.925 - ETA: 3s - loss: 0.2287 - acc: 0.926 - ETA: 3s - loss: 0.2250 - acc: 0.928 - ETA: 3s - loss: 0.2184 - acc: 0.930 - ETA: 3s - loss: 0.2166 - acc: 0.930 - ETA: 3s - loss: 0.2148 - acc: 0.930 - ETA: 3s - loss: 0.2178 - acc: 0.930 - ETA: 3s - loss: 0.2255 - acc: 0.929 - ETA: 3s - loss: 0.2286 - acc: 0.929 - ETA: 3s - loss: 0.2255 - acc: 0.930 - ETA: 3s - loss: 0.2253 - acc: 0.931 - ETA: 2s - loss: 0.2290 - acc: 0.930 - ETA: 2s - loss: 0.2266 - acc: 0.931 - ETA: 2s - loss: 0.2364 - acc: 0.926 - ETA: 2s - loss: 0.2406 - acc: 0.926 - ETA: 2s - loss: 0.2381 - acc: 0.927 - ETA: 2s - loss: 0.2358 - acc: 0.927 - ETA: 2s - loss: 0.2392 - acc: 0.926 - ETA: 2s - loss: 0.2391 - acc: 0.925 - ETA: 2s - loss: 0.2363 - acc: 0.927 - ETA: 2s - loss: 0.2347 - acc: 0.927 - ETA: 2s - loss: 0.2359 - acc: 0.927 - ETA: 2s - loss: 0.2387 - acc: 0.926 - ETA: 2s - loss: 0.2401 - acc: 0.925 - ETA: 2s - loss: 0.2445 - acc: 0.925 - ETA: 2s - loss: 0.2434 - acc: 0.925 - ETA: 2s - loss: 0.2443 - acc: 0.925 - ETA: 1s - loss: 0.2403 - acc: 0.926 - ETA: 1s - loss: 0.2398 - acc: 0.927 - ETA: 1s - loss: 0.2408 - acc: 0.925 - ETA: 1s - loss: 0.2423 - acc: 0.925 - ETA: 1s - loss: 0.2433 - acc: 0.924 - ETA: 1s - loss: 0.2440 - acc: 0.924 - ETA: 1s - loss: 0.2428 - acc: 0.924 - ETA: 1s - loss: 0.2439 - acc: 0.923 - ETA: 1s - loss: 0.2459 - acc: 0.924 - ETA: 1s - loss: 0.2449 - acc: 0.924 - ETA: 1s - loss: 0.2429 - acc: 0.924 - ETA: 1s - loss: 0.2446 - acc: 0.923 - ETA: 1s - loss: 0.2431 - acc: 0.923 - ETA: 1s - loss: 0.2407 - acc: 0.924 - ETA: 1s - loss: 0.2402 - acc: 0.924 - ETA: 1s - loss: 0.2379 - acc: 0.925 - ETA: 1s - loss: 0.2379 - acc: 0.924 - ETA: 1s - loss: 0.2370 - acc: 0.924 - ETA: 0s - loss: 0.2380 - acc: 0.924 - ETA: 0s - loss: 0.2383 - acc: 0.924 - ETA: 0s - loss: 0.2381 - acc: 0.924 - ETA: 0s - loss: 0.2384 - acc: 0.924 - ETA: 0s - loss: 0.2384 - acc: 0.924 - ETA: 0s - loss: 0.2387 - acc: 0.924 - ETA: 0s - loss: 0.2388 - acc: 0.924 - ETA: 0s - loss: 0.2397 - acc: 0.923 - ETA: 0s - loss: 0.2408 - acc: 0.923 - ETA: 0s - loss: 0.2414 - acc: 0.923 - ETA: 0s - loss: 0.2432 - acc: 0.923 - ETA: 0s - loss: 0.2421 - acc: 0.923 - ETA: 0s - loss: 0.2450 - acc: 0.923 - ETA: 0s - loss: 0.2448 - acc: 0.922 - ETA: 0s - loss: 0.2441 - acc: 0.922 - ETA: 0s - loss: 0.2443 - acc: 0.923 - ETA: 0s - loss: 0.2446 - acc: 0.922 - ETA: 0s - loss: 0.2430 - acc: 0.9235Epoch 00004: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.2418 - acc: 0.9237 - val_loss: 0.5112 - val_acc: 0.8551\n",
      "Epoch 6/20\n",
      "6600/6680 [============================>.] - ETA: 4s - loss: 0.1436 - acc: 0.950 - ETA: 4s - loss: 0.1347 - acc: 0.970 - ETA: 4s - loss: 0.2433 - acc: 0.944 - ETA: 4s - loss: 0.2049 - acc: 0.946 - ETA: 4s - loss: 0.1820 - acc: 0.950 - ETA: 4s - loss: 0.1728 - acc: 0.950 - ETA: 4s - loss: 0.1551 - acc: 0.956 - ETA: 4s - loss: 0.1659 - acc: 0.946 - ETA: 4s - loss: 0.1679 - acc: 0.947 - ETA: 3s - loss: 0.1653 - acc: 0.946 - ETA: 3s - loss: 0.1786 - acc: 0.942 - ETA: 3s - loss: 0.1893 - acc: 0.940 - ETA: 3s - loss: 0.1954 - acc: 0.938 - ETA: 3s - loss: 0.2025 - acc: 0.936 - ETA: 3s - loss: 0.2016 - acc: 0.935 - ETA: 3s - loss: 0.2031 - acc: 0.934 - ETA: 3s - loss: 0.2022 - acc: 0.934 - ETA: 3s - loss: 0.1982 - acc: 0.935 - ETA: 3s - loss: 0.1991 - acc: 0.935 - ETA: 3s - loss: 0.1916 - acc: 0.938 - ETA: 3s - loss: 0.1904 - acc: 0.939 - ETA: 3s - loss: 0.1911 - acc: 0.937 - ETA: 3s - loss: 0.1871 - acc: 0.939 - ETA: 3s - loss: 0.1851 - acc: 0.939 - ETA: 3s - loss: 0.1837 - acc: 0.939 - ETA: 3s - loss: 0.1841 - acc: 0.939 - ETA: 2s - loss: 0.1800 - acc: 0.940 - ETA: 2s - loss: 0.1811 - acc: 0.940 - ETA: 2s - loss: 0.1787 - acc: 0.940 - ETA: 2s - loss: 0.1769 - acc: 0.941 - ETA: 2s - loss: 0.1746 - acc: 0.942 - ETA: 2s - loss: 0.1734 - acc: 0.943 - ETA: 2s - loss: 0.1721 - acc: 0.943 - ETA: 2s - loss: 0.1819 - acc: 0.942 - ETA: 2s - loss: 0.1844 - acc: 0.941 - ETA: 2s - loss: 0.1884 - acc: 0.940 - ETA: 2s - loss: 0.1910 - acc: 0.939 - ETA: 2s - loss: 0.1943 - acc: 0.938 - ETA: 2s - loss: 0.1961 - acc: 0.936 - ETA: 2s - loss: 0.1979 - acc: 0.936 - ETA: 2s - loss: 0.1974 - acc: 0.936 - ETA: 2s - loss: 0.1963 - acc: 0.937 - ETA: 2s - loss: 0.1947 - acc: 0.937 - ETA: 2s - loss: 0.1964 - acc: 0.936 - ETA: 2s - loss: 0.1962 - acc: 0.937 - ETA: 1s - loss: 0.1972 - acc: 0.936 - ETA: 1s - loss: 0.1976 - acc: 0.936 - ETA: 1s - loss: 0.1980 - acc: 0.936 - ETA: 1s - loss: 0.2022 - acc: 0.935 - ETA: 1s - loss: 0.2052 - acc: 0.935 - ETA: 1s - loss: 0.2058 - acc: 0.935 - ETA: 1s - loss: 0.2064 - acc: 0.935 - ETA: 1s - loss: 0.2060 - acc: 0.934 - ETA: 1s - loss: 0.2075 - acc: 0.934 - ETA: 1s - loss: 0.2089 - acc: 0.934 - ETA: 1s - loss: 0.2064 - acc: 0.935 - ETA: 1s - loss: 0.2081 - acc: 0.935 - ETA: 1s - loss: 0.2084 - acc: 0.934 - ETA: 1s - loss: 0.2084 - acc: 0.934 - ETA: 1s - loss: 0.2115 - acc: 0.933 - ETA: 1s - loss: 0.2099 - acc: 0.933 - ETA: 1s - loss: 0.2106 - acc: 0.933 - ETA: 1s - loss: 0.2097 - acc: 0.934 - ETA: 0s - loss: 0.2088 - acc: 0.934 - ETA: 0s - loss: 0.2097 - acc: 0.933 - ETA: 0s - loss: 0.2085 - acc: 0.934 - ETA: 0s - loss: 0.2080 - acc: 0.934 - ETA: 0s - loss: 0.2073 - acc: 0.935 - ETA: 0s - loss: 0.2079 - acc: 0.935 - ETA: 0s - loss: 0.2102 - acc: 0.934 - ETA: 0s - loss: 0.2107 - acc: 0.934 - ETA: 0s - loss: 0.2132 - acc: 0.933 - ETA: 0s - loss: 0.2129 - acc: 0.933 - ETA: 0s - loss: 0.2116 - acc: 0.933 - ETA: 0s - loss: 0.2113 - acc: 0.933 - ETA: 0s - loss: 0.2130 - acc: 0.933 - ETA: 0s - loss: 0.2131 - acc: 0.933 - ETA: 0s - loss: 0.2141 - acc: 0.933 - ETA: 0s - loss: 0.2134 - acc: 0.933 - ETA: 0s - loss: 0.2140 - acc: 0.933 - ETA: 0s - loss: 0.2122 - acc: 0.9335Epoch 00005: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.2125 - acc: 0.9332 - val_loss: 0.5035 - val_acc: 0.8587\n",
      "Epoch 7/20\n",
      "6600/6680 [============================>.] - ETA: 3s - loss: 0.3345 - acc: 0.900 - ETA: 4s - loss: 0.1732 - acc: 0.940 - ETA: 4s - loss: 0.1576 - acc: 0.938 - ETA: 4s - loss: 0.1534 - acc: 0.938 - ETA: 4s - loss: 0.1344 - acc: 0.950 - ETA: 4s - loss: 0.1638 - acc: 0.938 - ETA: 4s - loss: 0.1801 - acc: 0.930 - ETA: 4s - loss: 0.1812 - acc: 0.934 - ETA: 4s - loss: 0.1788 - acc: 0.936 - ETA: 4s - loss: 0.1780 - acc: 0.940 - ETA: 3s - loss: 0.1730 - acc: 0.941 - ETA: 3s - loss: 0.1862 - acc: 0.940 - ETA: 3s - loss: 0.1936 - acc: 0.939 - ETA: 3s - loss: 0.1945 - acc: 0.940 - ETA: 3s - loss: 0.1898 - acc: 0.940 - ETA: 3s - loss: 0.1946 - acc: 0.936 - ETA: 3s - loss: 0.2063 - acc: 0.934 - ETA: 3s - loss: 0.2070 - acc: 0.934 - ETA: 3s - loss: 0.1982 - acc: 0.937 - ETA: 3s - loss: 0.1914 - acc: 0.939 - ETA: 3s - loss: 0.1915 - acc: 0.939 - ETA: 3s - loss: 0.1885 - acc: 0.939 - ETA: 3s - loss: 0.1832 - acc: 0.941 - ETA: 3s - loss: 0.1861 - acc: 0.940 - ETA: 3s - loss: 0.1857 - acc: 0.940 - ETA: 3s - loss: 0.1823 - acc: 0.940 - ETA: 3s - loss: 0.1860 - acc: 0.939 - ETA: 2s - loss: 0.1890 - acc: 0.938 - ETA: 2s - loss: 0.1921 - acc: 0.938 - ETA: 2s - loss: 0.1911 - acc: 0.938 - ETA: 2s - loss: 0.1890 - acc: 0.938 - ETA: 2s - loss: 0.1889 - acc: 0.937 - ETA: 2s - loss: 0.1887 - acc: 0.937 - ETA: 2s - loss: 0.1884 - acc: 0.938 - ETA: 2s - loss: 0.1910 - acc: 0.938 - ETA: 2s - loss: 0.1934 - acc: 0.937 - ETA: 2s - loss: 0.1964 - acc: 0.937 - ETA: 2s - loss: 0.1971 - acc: 0.937 - ETA: 2s - loss: 0.1949 - acc: 0.937 - ETA: 2s - loss: 0.1936 - acc: 0.938 - ETA: 2s - loss: 0.1937 - acc: 0.938 - ETA: 2s - loss: 0.1931 - acc: 0.938 - ETA: 2s - loss: 0.1907 - acc: 0.939 - ETA: 2s - loss: 0.1923 - acc: 0.938 - ETA: 2s - loss: 0.1899 - acc: 0.939 - ETA: 2s - loss: 0.1881 - acc: 0.940 - ETA: 1s - loss: 0.1858 - acc: 0.941 - ETA: 1s - loss: 0.1843 - acc: 0.941 - ETA: 1s - loss: 0.1841 - acc: 0.942 - ETA: 1s - loss: 0.1839 - acc: 0.942 - ETA: 1s - loss: 0.1830 - acc: 0.942 - ETA: 1s - loss: 0.1861 - acc: 0.942 - ETA: 1s - loss: 0.1880 - acc: 0.942 - ETA: 1s - loss: 0.1872 - acc: 0.942 - ETA: 1s - loss: 0.1867 - acc: 0.943 - ETA: 1s - loss: 0.1849 - acc: 0.943 - ETA: 1s - loss: 0.1827 - acc: 0.944 - ETA: 1s - loss: 0.1817 - acc: 0.944 - ETA: 1s - loss: 0.1824 - acc: 0.944 - ETA: 1s - loss: 0.1815 - acc: 0.944 - ETA: 1s - loss: 0.1810 - acc: 0.945 - ETA: 1s - loss: 0.1835 - acc: 0.944 - ETA: 1s - loss: 0.1856 - acc: 0.944 - ETA: 1s - loss: 0.1867 - acc: 0.943 - ETA: 0s - loss: 0.1861 - acc: 0.942 - ETA: 0s - loss: 0.1873 - acc: 0.942 - ETA: 0s - loss: 0.1864 - acc: 0.942 - ETA: 0s - loss: 0.1866 - acc: 0.942 - ETA: 0s - loss: 0.1912 - acc: 0.942 - ETA: 0s - loss: 0.1956 - acc: 0.941 - ETA: 0s - loss: 0.1938 - acc: 0.941 - ETA: 0s - loss: 0.1923 - acc: 0.941 - ETA: 0s - loss: 0.1932 - acc: 0.941 - ETA: 0s - loss: 0.1940 - acc: 0.941 - ETA: 0s - loss: 0.1945 - acc: 0.941 - ETA: 0s - loss: 0.1945 - acc: 0.940 - ETA: 0s - loss: 0.1949 - acc: 0.940 - ETA: 0s - loss: 0.1957 - acc: 0.940 - ETA: 0s - loss: 0.1957 - acc: 0.940 - ETA: 0s - loss: 0.1940 - acc: 0.940 - ETA: 0s - loss: 0.1950 - acc: 0.9405Epoch 00006: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.1951 - acc: 0.9407 - val_loss: 0.5337 - val_acc: 0.8491\n",
      "Epoch 8/20\n",
      "6620/6680 [============================>.] - ETA: 4s - loss: 0.2372 - acc: 0.950 - ETA: 4s - loss: 0.1021 - acc: 0.970 - ETA: 4s - loss: 0.1682 - acc: 0.938 - ETA: 4s - loss: 0.1672 - acc: 0.938 - ETA: 4s - loss: 0.1466 - acc: 0.947 - ETA: 4s - loss: 0.1294 - acc: 0.952 - ETA: 4s - loss: 0.1352 - acc: 0.950 - ETA: 4s - loss: 0.1577 - acc: 0.946 - ETA: 4s - loss: 0.1693 - acc: 0.948 - ETA: 4s - loss: 0.1607 - acc: 0.950 - ETA: 3s - loss: 0.1519 - acc: 0.952 - ETA: 3s - loss: 0.1524 - acc: 0.950 - ETA: 3s - loss: 0.1496 - acc: 0.951 - ETA: 3s - loss: 0.1623 - acc: 0.950 - ETA: 3s - loss: 0.1596 - acc: 0.950 - ETA: 3s - loss: 0.1618 - acc: 0.950 - ETA: 3s - loss: 0.1591 - acc: 0.952 - ETA: 3s - loss: 0.1558 - acc: 0.954 - ETA: 3s - loss: 0.1565 - acc: 0.954 - ETA: 3s - loss: 0.1563 - acc: 0.953 - ETA: 3s - loss: 0.1567 - acc: 0.954 - ETA: 3s - loss: 0.1579 - acc: 0.954 - ETA: 3s - loss: 0.1558 - acc: 0.953 - ETA: 3s - loss: 0.1552 - acc: 0.953 - ETA: 3s - loss: 0.1528 - acc: 0.953 - ETA: 3s - loss: 0.1555 - acc: 0.953 - ETA: 3s - loss: 0.1525 - acc: 0.953 - ETA: 3s - loss: 0.1504 - acc: 0.954 - ETA: 2s - loss: 0.1517 - acc: 0.953 - ETA: 2s - loss: 0.1491 - acc: 0.955 - ETA: 2s - loss: 0.1515 - acc: 0.953 - ETA: 2s - loss: 0.1592 - acc: 0.953 - ETA: 2s - loss: 0.1582 - acc: 0.953 - ETA: 2s - loss: 0.1635 - acc: 0.951 - ETA: 2s - loss: 0.1640 - acc: 0.951 - ETA: 2s - loss: 0.1640 - acc: 0.951 - ETA: 2s - loss: 0.1604 - acc: 0.952 - ETA: 2s - loss: 0.1589 - acc: 0.953 - ETA: 2s - loss: 0.1615 - acc: 0.952 - ETA: 2s - loss: 0.1615 - acc: 0.953 - ETA: 2s - loss: 0.1604 - acc: 0.953 - ETA: 2s - loss: 0.1607 - acc: 0.953 - ETA: 2s - loss: 0.1595 - acc: 0.954 - ETA: 2s - loss: 0.1622 - acc: 0.953 - ETA: 2s - loss: 0.1594 - acc: 0.954 - ETA: 2s - loss: 0.1608 - acc: 0.953 - ETA: 1s - loss: 0.1634 - acc: 0.951 - ETA: 1s - loss: 0.1649 - acc: 0.950 - ETA: 1s - loss: 0.1671 - acc: 0.949 - ETA: 1s - loss: 0.1687 - acc: 0.948 - ETA: 1s - loss: 0.1668 - acc: 0.949 - ETA: 1s - loss: 0.1698 - acc: 0.948 - ETA: 1s - loss: 0.1707 - acc: 0.948 - ETA: 1s - loss: 0.1708 - acc: 0.948 - ETA: 1s - loss: 0.1740 - acc: 0.948 - ETA: 1s - loss: 0.1719 - acc: 0.949 - ETA: 1s - loss: 0.1721 - acc: 0.949 - ETA: 1s - loss: 0.1720 - acc: 0.949 - ETA: 1s - loss: 0.1710 - acc: 0.949 - ETA: 1s - loss: 0.1711 - acc: 0.948 - ETA: 1s - loss: 0.1705 - acc: 0.948 - ETA: 1s - loss: 0.1684 - acc: 0.949 - ETA: 1s - loss: 0.1664 - acc: 0.950 - ETA: 1s - loss: 0.1667 - acc: 0.950 - ETA: 0s - loss: 0.1652 - acc: 0.950 - ETA: 0s - loss: 0.1651 - acc: 0.950 - ETA: 0s - loss: 0.1664 - acc: 0.949 - ETA: 0s - loss: 0.1658 - acc: 0.950 - ETA: 0s - loss: 0.1649 - acc: 0.950 - ETA: 0s - loss: 0.1644 - acc: 0.950 - ETA: 0s - loss: 0.1635 - acc: 0.950 - ETA: 0s - loss: 0.1638 - acc: 0.950 - ETA: 0s - loss: 0.1629 - acc: 0.950 - ETA: 0s - loss: 0.1657 - acc: 0.950 - ETA: 0s - loss: 0.1657 - acc: 0.949 - ETA: 0s - loss: 0.1684 - acc: 0.949 - ETA: 0s - loss: 0.1692 - acc: 0.949 - ETA: 0s - loss: 0.1709 - acc: 0.948 - ETA: 0s - loss: 0.1726 - acc: 0.948 - ETA: 0s - loss: 0.1730 - acc: 0.948 - ETA: 0s - loss: 0.1721 - acc: 0.948 - ETA: 0s - loss: 0.1751 - acc: 0.9480Epoch 00007: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.1743 - acc: 0.9482 - val_loss: 0.5340 - val_acc: 0.8635\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6640/6680 [============================>.] - ETA: 3s - loss: 0.0432 - acc: 1.000 - ETA: 4s - loss: 0.1009 - acc: 0.980 - ETA: 4s - loss: 0.1014 - acc: 0.970 - ETA: 4s - loss: 0.1249 - acc: 0.960 - ETA: 4s - loss: 0.1157 - acc: 0.961 - ETA: 4s - loss: 0.1155 - acc: 0.961 - ETA: 4s - loss: 0.1050 - acc: 0.963 - ETA: 4s - loss: 0.1100 - acc: 0.958 - ETA: 4s - loss: 0.1219 - acc: 0.955 - ETA: 4s - loss: 0.1226 - acc: 0.955 - ETA: 4s - loss: 0.1246 - acc: 0.952 - ETA: 4s - loss: 0.1218 - acc: 0.953 - ETA: 4s - loss: 0.1171 - acc: 0.955 - ETA: 4s - loss: 0.1175 - acc: 0.955 - ETA: 4s - loss: 0.1117 - acc: 0.958 - ETA: 3s - loss: 0.1078 - acc: 0.960 - ETA: 3s - loss: 0.1091 - acc: 0.960 - ETA: 3s - loss: 0.1058 - acc: 0.961 - ETA: 3s - loss: 0.1046 - acc: 0.962 - ETA: 3s - loss: 0.1013 - acc: 0.963 - ETA: 3s - loss: 0.1061 - acc: 0.961 - ETA: 3s - loss: 0.1042 - acc: 0.962 - ETA: 3s - loss: 0.1045 - acc: 0.963 - ETA: 3s - loss: 0.1018 - acc: 0.964 - ETA: 3s - loss: 0.1088 - acc: 0.962 - ETA: 3s - loss: 0.1132 - acc: 0.961 - ETA: 3s - loss: 0.1122 - acc: 0.961 - ETA: 3s - loss: 0.1131 - acc: 0.961 - ETA: 3s - loss: 0.1127 - acc: 0.961 - ETA: 3s - loss: 0.1115 - acc: 0.961 - ETA: 3s - loss: 0.1110 - acc: 0.961 - ETA: 3s - loss: 0.1110 - acc: 0.961 - ETA: 3s - loss: 0.1099 - acc: 0.962 - ETA: 3s - loss: 0.1112 - acc: 0.961 - ETA: 3s - loss: 0.1125 - acc: 0.961 - ETA: 2s - loss: 0.1101 - acc: 0.962 - ETA: 2s - loss: 0.1097 - acc: 0.962 - ETA: 2s - loss: 0.1118 - acc: 0.961 - ETA: 2s - loss: 0.1189 - acc: 0.960 - ETA: 2s - loss: 0.1175 - acc: 0.961 - ETA: 2s - loss: 0.1166 - acc: 0.961 - ETA: 2s - loss: 0.1184 - acc: 0.960 - ETA: 2s - loss: 0.1210 - acc: 0.959 - ETA: 2s - loss: 0.1246 - acc: 0.958 - ETA: 2s - loss: 0.1287 - acc: 0.957 - ETA: 2s - loss: 0.1303 - acc: 0.957 - ETA: 2s - loss: 0.1300 - acc: 0.957 - ETA: 2s - loss: 0.1295 - acc: 0.958 - ETA: 2s - loss: 0.1300 - acc: 0.958 - ETA: 2s - loss: 0.1304 - acc: 0.957 - ETA: 1s - loss: 0.1315 - acc: 0.957 - ETA: 1s - loss: 0.1298 - acc: 0.957 - ETA: 1s - loss: 0.1303 - acc: 0.956 - ETA: 1s - loss: 0.1303 - acc: 0.956 - ETA: 1s - loss: 0.1315 - acc: 0.956 - ETA: 1s - loss: 0.1374 - acc: 0.954 - ETA: 1s - loss: 0.1421 - acc: 0.954 - ETA: 1s - loss: 0.1427 - acc: 0.954 - ETA: 1s - loss: 0.1452 - acc: 0.954 - ETA: 1s - loss: 0.1473 - acc: 0.953 - ETA: 1s - loss: 0.1495 - acc: 0.953 - ETA: 1s - loss: 0.1506 - acc: 0.952 - ETA: 1s - loss: 0.1511 - acc: 0.952 - ETA: 1s - loss: 0.1510 - acc: 0.952 - ETA: 0s - loss: 0.1501 - acc: 0.952 - ETA: 0s - loss: 0.1490 - acc: 0.953 - ETA: 0s - loss: 0.1513 - acc: 0.952 - ETA: 0s - loss: 0.1546 - acc: 0.952 - ETA: 0s - loss: 0.1541 - acc: 0.952 - ETA: 0s - loss: 0.1540 - acc: 0.952 - ETA: 0s - loss: 0.1538 - acc: 0.952 - ETA: 0s - loss: 0.1561 - acc: 0.952 - ETA: 0s - loss: 0.1563 - acc: 0.951 - ETA: 0s - loss: 0.1553 - acc: 0.951 - ETA: 0s - loss: 0.1543 - acc: 0.952 - ETA: 0s - loss: 0.1532 - acc: 0.952 - ETA: 0s - loss: 0.1536 - acc: 0.952 - ETA: 0s - loss: 0.1556 - acc: 0.952 - ETA: 0s - loss: 0.1554 - acc: 0.9521Epoch 00008: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.1562 - acc: 0.9519 - val_loss: 0.5812 - val_acc: 0.8647\n",
      "Epoch 10/20\n",
      "6640/6680 [============================>.] - ETA: 3s - loss: 0.3678 - acc: 0.900 - ETA: 4s - loss: 0.2455 - acc: 0.940 - ETA: 4s - loss: 0.1681 - acc: 0.961 - ETA: 3s - loss: 0.1151 - acc: 0.975 - ETA: 3s - loss: 0.1150 - acc: 0.968 - ETA: 3s - loss: 0.1175 - acc: 0.965 - ETA: 3s - loss: 0.1112 - acc: 0.968 - ETA: 3s - loss: 0.1116 - acc: 0.971 - ETA: 3s - loss: 0.1190 - acc: 0.966 - ETA: 3s - loss: 0.1162 - acc: 0.967 - ETA: 3s - loss: 0.1166 - acc: 0.967 - ETA: 3s - loss: 0.1205 - acc: 0.963 - ETA: 3s - loss: 0.1210 - acc: 0.962 - ETA: 3s - loss: 0.1321 - acc: 0.960 - ETA: 3s - loss: 0.1333 - acc: 0.960 - ETA: 3s - loss: 0.1328 - acc: 0.960 - ETA: 3s - loss: 0.1301 - acc: 0.960 - ETA: 3s - loss: 0.1331 - acc: 0.958 - ETA: 3s - loss: 0.1298 - acc: 0.958 - ETA: 3s - loss: 0.1384 - acc: 0.957 - ETA: 3s - loss: 0.1345 - acc: 0.957 - ETA: 3s - loss: 0.1301 - acc: 0.958 - ETA: 3s - loss: 0.1304 - acc: 0.958 - ETA: 3s - loss: 0.1301 - acc: 0.959 - ETA: 2s - loss: 0.1314 - acc: 0.958 - ETA: 2s - loss: 0.1338 - acc: 0.958 - ETA: 2s - loss: 0.1321 - acc: 0.958 - ETA: 2s - loss: 0.1278 - acc: 0.960 - ETA: 2s - loss: 0.1279 - acc: 0.960 - ETA: 2s - loss: 0.1284 - acc: 0.960 - ETA: 2s - loss: 0.1313 - acc: 0.959 - ETA: 2s - loss: 0.1285 - acc: 0.960 - ETA: 2s - loss: 0.1287 - acc: 0.959 - ETA: 2s - loss: 0.1277 - acc: 0.960 - ETA: 2s - loss: 0.1254 - acc: 0.961 - ETA: 2s - loss: 0.1250 - acc: 0.961 - ETA: 2s - loss: 0.1308 - acc: 0.959 - ETA: 2s - loss: 0.1287 - acc: 0.960 - ETA: 2s - loss: 0.1294 - acc: 0.960 - ETA: 2s - loss: 0.1307 - acc: 0.960 - ETA: 1s - loss: 0.1291 - acc: 0.960 - ETA: 1s - loss: 0.1284 - acc: 0.960 - ETA: 1s - loss: 0.1307 - acc: 0.959 - ETA: 1s - loss: 0.1325 - acc: 0.958 - ETA: 1s - loss: 0.1341 - acc: 0.958 - ETA: 1s - loss: 0.1328 - acc: 0.958 - ETA: 1s - loss: 0.1319 - acc: 0.958 - ETA: 1s - loss: 0.1325 - acc: 0.958 - ETA: 1s - loss: 0.1348 - acc: 0.957 - ETA: 1s - loss: 0.1370 - acc: 0.957 - ETA: 1s - loss: 0.1366 - acc: 0.956 - ETA: 1s - loss: 0.1370 - acc: 0.956 - ETA: 1s - loss: 0.1367 - acc: 0.956 - ETA: 1s - loss: 0.1375 - acc: 0.956 - ETA: 1s - loss: 0.1402 - acc: 0.955 - ETA: 1s - loss: 0.1404 - acc: 0.955 - ETA: 1s - loss: 0.1390 - acc: 0.956 - ETA: 0s - loss: 0.1371 - acc: 0.956 - ETA: 0s - loss: 0.1412 - acc: 0.956 - ETA: 0s - loss: 0.1425 - acc: 0.955 - ETA: 0s - loss: 0.1421 - acc: 0.955 - ETA: 0s - loss: 0.1423 - acc: 0.955 - ETA: 0s - loss: 0.1419 - acc: 0.955 - ETA: 0s - loss: 0.1429 - acc: 0.954 - ETA: 0s - loss: 0.1433 - acc: 0.954 - ETA: 0s - loss: 0.1458 - acc: 0.953 - ETA: 0s - loss: 0.1469 - acc: 0.953 - ETA: 0s - loss: 0.1462 - acc: 0.953 - ETA: 0s - loss: 0.1466 - acc: 0.953 - ETA: 0s - loss: 0.1463 - acc: 0.953 - ETA: 0s - loss: 0.1478 - acc: 0.953 - ETA: 0s - loss: 0.1485 - acc: 0.952 - ETA: 0s - loss: 0.1504 - acc: 0.952 - ETA: 0s - loss: 0.1498 - acc: 0.952 - ETA: 0s - loss: 0.1493 - acc: 0.9532Epoch 00009: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.1496 - acc: 0.9531 - val_loss: 0.5740 - val_acc: 0.8575\n",
      "Epoch 11/20\n",
      "6640/6680 [============================>.] - ETA: 4s - loss: 0.1451 - acc: 0.900 - ETA: 4s - loss: 0.1097 - acc: 0.940 - ETA: 4s - loss: 0.0930 - acc: 0.950 - ETA: 4s - loss: 0.1202 - acc: 0.946 - ETA: 4s - loss: 0.1404 - acc: 0.944 - ETA: 4s - loss: 0.1282 - acc: 0.945 - ETA: 4s - loss: 0.1123 - acc: 0.951 - ETA: 3s - loss: 0.1144 - acc: 0.954 - ETA: 3s - loss: 0.1079 - acc: 0.958 - ETA: 3s - loss: 0.1019 - acc: 0.962 - ETA: 3s - loss: 0.1088 - acc: 0.960 - ETA: 3s - loss: 0.1057 - acc: 0.962 - ETA: 3s - loss: 0.0994 - acc: 0.964 - ETA: 3s - loss: 0.1039 - acc: 0.961 - ETA: 3s - loss: 0.1080 - acc: 0.960 - ETA: 3s - loss: 0.1096 - acc: 0.961 - ETA: 3s - loss: 0.1085 - acc: 0.961 - ETA: 3s - loss: 0.1096 - acc: 0.961 - ETA: 3s - loss: 0.1091 - acc: 0.962 - ETA: 3s - loss: 0.1129 - acc: 0.963 - ETA: 2s - loss: 0.1121 - acc: 0.963 - ETA: 2s - loss: 0.1114 - acc: 0.963 - ETA: 2s - loss: 0.1140 - acc: 0.963 - ETA: 2s - loss: 0.1169 - acc: 0.963 - ETA: 2s - loss: 0.1151 - acc: 0.963 - ETA: 2s - loss: 0.1167 - acc: 0.963 - ETA: 2s - loss: 0.1177 - acc: 0.963 - ETA: 2s - loss: 0.1221 - acc: 0.963 - ETA: 2s - loss: 0.1252 - acc: 0.963 - ETA: 2s - loss: 0.1253 - acc: 0.961 - ETA: 2s - loss: 0.1264 - acc: 0.961 - ETA: 2s - loss: 0.1270 - acc: 0.961 - ETA: 2s - loss: 0.1288 - acc: 0.960 - ETA: 2s - loss: 0.1270 - acc: 0.961 - ETA: 2s - loss: 0.1250 - acc: 0.961 - ETA: 2s - loss: 0.1221 - acc: 0.962 - ETA: 2s - loss: 0.1223 - acc: 0.961 - ETA: 1s - loss: 0.1260 - acc: 0.960 - ETA: 1s - loss: 0.1317 - acc: 0.958 - ETA: 1s - loss: 0.1303 - acc: 0.958 - ETA: 1s - loss: 0.1291 - acc: 0.959 - ETA: 1s - loss: 0.1291 - acc: 0.959 - ETA: 1s - loss: 0.1275 - acc: 0.959 - ETA: 1s - loss: 0.1292 - acc: 0.958 - ETA: 1s - loss: 0.1274 - acc: 0.959 - ETA: 1s - loss: 0.1259 - acc: 0.959 - ETA: 1s - loss: 0.1266 - acc: 0.959 - ETA: 1s - loss: 0.1288 - acc: 0.958 - ETA: 1s - loss: 0.1286 - acc: 0.958 - ETA: 1s - loss: 0.1293 - acc: 0.958 - ETA: 1s - loss: 0.1284 - acc: 0.958 - ETA: 1s - loss: 0.1336 - acc: 0.957 - ETA: 1s - loss: 0.1330 - acc: 0.958 - ETA: 1s - loss: 0.1319 - acc: 0.958 - ETA: 1s - loss: 0.1328 - acc: 0.958 - ETA: 1s - loss: 0.1343 - acc: 0.957 - ETA: 1s - loss: 0.1354 - acc: 0.957 - ETA: 1s - loss: 0.1356 - acc: 0.957 - ETA: 1s - loss: 0.1349 - acc: 0.957 - ETA: 0s - loss: 0.1339 - acc: 0.957 - ETA: 0s - loss: 0.1325 - acc: 0.958 - ETA: 0s - loss: 0.1340 - acc: 0.957 - ETA: 0s - loss: 0.1340 - acc: 0.957 - ETA: 0s - loss: 0.1332 - acc: 0.957 - ETA: 0s - loss: 0.1326 - acc: 0.957 - ETA: 0s - loss: 0.1314 - acc: 0.958 - ETA: 0s - loss: 0.1341 - acc: 0.957 - ETA: 0s - loss: 0.1366 - acc: 0.958 - ETA: 0s - loss: 0.1356 - acc: 0.958 - ETA: 0s - loss: 0.1350 - acc: 0.958 - ETA: 0s - loss: 0.1338 - acc: 0.958 - ETA: 0s - loss: 0.1334 - acc: 0.958 - ETA: 0s - loss: 0.1327 - acc: 0.958 - ETA: 0s - loss: 0.1318 - acc: 0.959 - ETA: 0s - loss: 0.1310 - acc: 0.959 - ETA: 0s - loss: 0.1317 - acc: 0.959 - ETA: 0s - loss: 0.1317 - acc: 0.959 - ETA: 0s - loss: 0.1320 - acc: 0.959 - ETA: 0s - loss: 0.1339 - acc: 0.959 - ETA: 0s - loss: 0.1333 - acc: 0.9590Epoch 00010: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.1330 - acc: 0.9590 - val_loss: 0.5943 - val_acc: 0.8491\n",
      "Epoch 12/20\n",
      "6660/6680 [============================>.] - ETA: 4s - loss: 0.0216 - acc: 1.000 - ETA: 5s - loss: 0.0503 - acc: 0.990 - ETA: 5s - loss: 0.0449 - acc: 0.987 - ETA: 5s - loss: 0.0401 - acc: 0.987 - ETA: 5s - loss: 0.0580 - acc: 0.976 - ETA: 5s - loss: 0.0643 - acc: 0.976 - ETA: 5s - loss: 0.0629 - acc: 0.976 - ETA: 4s - loss: 0.0682 - acc: 0.975 - ETA: 4s - loss: 0.0694 - acc: 0.975 - ETA: 4s - loss: 0.0634 - acc: 0.978 - ETA: 4s - loss: 0.0716 - acc: 0.975 - ETA: 4s - loss: 0.0871 - acc: 0.972 - ETA: 4s - loss: 0.0834 - acc: 0.972 - ETA: 4s - loss: 0.0943 - acc: 0.969 - ETA: 4s - loss: 0.0930 - acc: 0.969 - ETA: 4s - loss: 0.0889 - acc: 0.970 - ETA: 4s - loss: 0.0919 - acc: 0.970 - ETA: 4s - loss: 0.0921 - acc: 0.970 - ETA: 4s - loss: 0.0941 - acc: 0.968 - ETA: 4s - loss: 0.0940 - acc: 0.968 - ETA: 3s - loss: 0.0962 - acc: 0.968 - ETA: 3s - loss: 0.0987 - acc: 0.967 - ETA: 3s - loss: 0.0961 - acc: 0.966 - ETA: 3s - loss: 0.1021 - acc: 0.965 - ETA: 3s - loss: 0.1057 - acc: 0.964 - ETA: 3s - loss: 0.1104 - acc: 0.963 - ETA: 3s - loss: 0.1174 - acc: 0.962 - ETA: 3s - loss: 0.1164 - acc: 0.962 - ETA: 3s - loss: 0.1198 - acc: 0.959 - ETA: 3s - loss: 0.1201 - acc: 0.960 - ETA: 3s - loss: 0.1207 - acc: 0.960 - ETA: 3s - loss: 0.1188 - acc: 0.960 - ETA: 2s - loss: 0.1243 - acc: 0.959 - ETA: 2s - loss: 0.1232 - acc: 0.960 - ETA: 2s - loss: 0.1249 - acc: 0.959 - ETA: 2s - loss: 0.1218 - acc: 0.960 - ETA: 2s - loss: 0.1238 - acc: 0.960 - ETA: 2s - loss: 0.1218 - acc: 0.960 - ETA: 2s - loss: 0.1213 - acc: 0.961 - ETA: 2s - loss: 0.1206 - acc: 0.961 - ETA: 2s - loss: 0.1209 - acc: 0.961 - ETA: 2s - loss: 0.1187 - acc: 0.961 - ETA: 2s - loss: 0.1192 - acc: 0.961 - ETA: 2s - loss: 0.1236 - acc: 0.960 - ETA: 2s - loss: 0.1227 - acc: 0.960 - ETA: 2s - loss: 0.1256 - acc: 0.960 - ETA: 2s - loss: 0.1242 - acc: 0.961 - ETA: 2s - loss: 0.1242 - acc: 0.960 - ETA: 1s - loss: 0.1219 - acc: 0.961 - ETA: 1s - loss: 0.1214 - acc: 0.961 - ETA: 1s - loss: 0.1199 - acc: 0.961 - ETA: 1s - loss: 0.1198 - acc: 0.961 - ETA: 1s - loss: 0.1211 - acc: 0.961 - ETA: 1s - loss: 0.1209 - acc: 0.961 - ETA: 1s - loss: 0.1213 - acc: 0.961 - ETA: 1s - loss: 0.1212 - acc: 0.961 - ETA: 1s - loss: 0.1229 - acc: 0.961 - ETA: 1s - loss: 0.1219 - acc: 0.961 - ETA: 1s - loss: 0.1223 - acc: 0.961 - ETA: 1s - loss: 0.1227 - acc: 0.961 - ETA: 1s - loss: 0.1243 - acc: 0.960 - ETA: 1s - loss: 0.1250 - acc: 0.960 - ETA: 1s - loss: 0.1234 - acc: 0.961 - ETA: 1s - loss: 0.1233 - acc: 0.961 - ETA: 1s - loss: 0.1259 - acc: 0.961 - ETA: 0s - loss: 0.1266 - acc: 0.961 - ETA: 0s - loss: 0.1261 - acc: 0.961 - ETA: 0s - loss: 0.1255 - acc: 0.961 - ETA: 0s - loss: 0.1252 - acc: 0.961 - ETA: 0s - loss: 0.1238 - acc: 0.962 - ETA: 0s - loss: 0.1227 - acc: 0.962 - ETA: 0s - loss: 0.1216 - acc: 0.962 - ETA: 0s - loss: 0.1221 - acc: 0.962 - ETA: 0s - loss: 0.1214 - acc: 0.962 - ETA: 0s - loss: 0.1201 - acc: 0.962 - ETA: 0s - loss: 0.1222 - acc: 0.962 - ETA: 0s - loss: 0.1216 - acc: 0.962 - ETA: 0s - loss: 0.1219 - acc: 0.962 - ETA: 0s - loss: 0.1214 - acc: 0.962 - ETA: 0s - loss: 0.1220 - acc: 0.962 - ETA: 0s - loss: 0.1216 - acc: 0.962 - ETA: 0s - loss: 0.1226 - acc: 0.961 - ETA: 0s - loss: 0.1219 - acc: 0.9619Epoch 00011: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.1222 - acc: 0.9618 - val_loss: 0.5834 - val_acc: 0.8491\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6600/6680 [============================>.] - ETA: 4s - loss: 0.0183 - acc: 1.000 - ETA: 4s - loss: 0.0459 - acc: 0.980 - ETA: 4s - loss: 0.0831 - acc: 0.961 - ETA: 4s - loss: 0.0699 - acc: 0.969 - ETA: 4s - loss: 0.0784 - acc: 0.964 - ETA: 4s - loss: 0.1274 - acc: 0.961 - ETA: 4s - loss: 0.1190 - acc: 0.964 - ETA: 4s - loss: 0.1064 - acc: 0.967 - ETA: 4s - loss: 0.0970 - acc: 0.971 - ETA: 4s - loss: 0.1000 - acc: 0.968 - ETA: 4s - loss: 0.1082 - acc: 0.965 - ETA: 4s - loss: 0.1044 - acc: 0.967 - ETA: 4s - loss: 0.1050 - acc: 0.964 - ETA: 4s - loss: 0.1090 - acc: 0.965 - ETA: 3s - loss: 0.1062 - acc: 0.965 - ETA: 3s - loss: 0.1081 - acc: 0.966 - ETA: 3s - loss: 0.1070 - acc: 0.966 - ETA: 3s - loss: 0.1070 - acc: 0.966 - ETA: 3s - loss: 0.1064 - acc: 0.966 - ETA: 3s - loss: 0.1064 - acc: 0.965 - ETA: 3s - loss: 0.1048 - acc: 0.965 - ETA: 3s - loss: 0.1082 - acc: 0.964 - ETA: 3s - loss: 0.1043 - acc: 0.965 - ETA: 3s - loss: 0.1018 - acc: 0.965 - ETA: 3s - loss: 0.0998 - acc: 0.965 - ETA: 3s - loss: 0.0973 - acc: 0.966 - ETA: 3s - loss: 0.0942 - acc: 0.967 - ETA: 2s - loss: 0.0935 - acc: 0.968 - ETA: 2s - loss: 0.0937 - acc: 0.967 - ETA: 2s - loss: 0.0933 - acc: 0.967 - ETA: 2s - loss: 0.0929 - acc: 0.967 - ETA: 2s - loss: 0.0944 - acc: 0.966 - ETA: 2s - loss: 0.0970 - acc: 0.965 - ETA: 2s - loss: 0.0988 - acc: 0.964 - ETA: 2s - loss: 0.1014 - acc: 0.964 - ETA: 2s - loss: 0.1003 - acc: 0.964 - ETA: 2s - loss: 0.0987 - acc: 0.964 - ETA: 2s - loss: 0.1000 - acc: 0.964 - ETA: 2s - loss: 0.1048 - acc: 0.964 - ETA: 2s - loss: 0.1050 - acc: 0.964 - ETA: 2s - loss: 0.1080 - acc: 0.964 - ETA: 2s - loss: 0.1081 - acc: 0.964 - ETA: 2s - loss: 0.1089 - acc: 0.964 - ETA: 2s - loss: 0.1133 - acc: 0.964 - ETA: 2s - loss: 0.1168 - acc: 0.963 - ETA: 1s - loss: 0.1152 - acc: 0.964 - ETA: 1s - loss: 0.1143 - acc: 0.964 - ETA: 1s - loss: 0.1123 - acc: 0.965 - ETA: 1s - loss: 0.1121 - acc: 0.965 - ETA: 1s - loss: 0.1121 - acc: 0.965 - ETA: 1s - loss: 0.1130 - acc: 0.964 - ETA: 1s - loss: 0.1144 - acc: 0.964 - ETA: 1s - loss: 0.1139 - acc: 0.964 - ETA: 1s - loss: 0.1151 - acc: 0.963 - ETA: 1s - loss: 0.1145 - acc: 0.963 - ETA: 1s - loss: 0.1156 - acc: 0.964 - ETA: 1s - loss: 0.1172 - acc: 0.963 - ETA: 1s - loss: 0.1184 - acc: 0.963 - ETA: 1s - loss: 0.1213 - acc: 0.963 - ETA: 1s - loss: 0.1199 - acc: 0.963 - ETA: 1s - loss: 0.1188 - acc: 0.964 - ETA: 0s - loss: 0.1176 - acc: 0.964 - ETA: 0s - loss: 0.1167 - acc: 0.965 - ETA: 0s - loss: 0.1183 - acc: 0.964 - ETA: 0s - loss: 0.1174 - acc: 0.964 - ETA: 0s - loss: 0.1173 - acc: 0.964 - ETA: 0s - loss: 0.1186 - acc: 0.964 - ETA: 0s - loss: 0.1174 - acc: 0.964 - ETA: 0s - loss: 0.1164 - acc: 0.965 - ETA: 0s - loss: 0.1153 - acc: 0.965 - ETA: 0s - loss: 0.1143 - acc: 0.965 - ETA: 0s - loss: 0.1139 - acc: 0.965 - ETA: 0s - loss: 0.1139 - acc: 0.965 - ETA: 0s - loss: 0.1145 - acc: 0.965 - ETA: 0s - loss: 0.1152 - acc: 0.965 - ETA: 0s - loss: 0.1144 - acc: 0.965 - ETA: 0s - loss: 0.1141 - acc: 0.965 - ETA: 0s - loss: 0.1154 - acc: 0.964 - ETA: 0s - loss: 0.1146 - acc: 0.9650Epoch 00012: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.1155 - acc: 0.9650 - val_loss: 0.6026 - val_acc: 0.8587\n",
      "Epoch 14/20\n",
      "6620/6680 [============================>.] - ETA: 3s - loss: 0.1066 - acc: 0.950 - ETA: 4s - loss: 0.0747 - acc: 0.970 - ETA: 4s - loss: 0.1296 - acc: 0.962 - ETA: 4s - loss: 0.1223 - acc: 0.958 - ETA: 4s - loss: 0.1507 - acc: 0.946 - ETA: 4s - loss: 0.1328 - acc: 0.952 - ETA: 4s - loss: 0.1209 - acc: 0.954 - ETA: 4s - loss: 0.1099 - acc: 0.958 - ETA: 4s - loss: 0.1060 - acc: 0.957 - ETA: 4s - loss: 0.1127 - acc: 0.955 - ETA: 4s - loss: 0.1027 - acc: 0.960 - ETA: 4s - loss: 0.1015 - acc: 0.961 - ETA: 4s - loss: 0.0955 - acc: 0.964 - ETA: 3s - loss: 0.0923 - acc: 0.965 - ETA: 3s - loss: 0.0873 - acc: 0.967 - ETA: 3s - loss: 0.0861 - acc: 0.968 - ETA: 3s - loss: 0.0901 - acc: 0.968 - ETA: 3s - loss: 0.0915 - acc: 0.967 - ETA: 3s - loss: 0.0883 - acc: 0.968 - ETA: 3s - loss: 0.0915 - acc: 0.969 - ETA: 3s - loss: 0.0921 - acc: 0.969 - ETA: 3s - loss: 0.0937 - acc: 0.969 - ETA: 3s - loss: 0.0904 - acc: 0.970 - ETA: 3s - loss: 0.0896 - acc: 0.971 - ETA: 3s - loss: 0.0906 - acc: 0.970 - ETA: 3s - loss: 0.0901 - acc: 0.971 - ETA: 3s - loss: 0.0888 - acc: 0.970 - ETA: 3s - loss: 0.0869 - acc: 0.971 - ETA: 3s - loss: 0.0846 - acc: 0.972 - ETA: 3s - loss: 0.0893 - acc: 0.972 - ETA: 3s - loss: 0.0927 - acc: 0.971 - ETA: 2s - loss: 0.0913 - acc: 0.972 - ETA: 2s - loss: 0.0892 - acc: 0.973 - ETA: 2s - loss: 0.0910 - acc: 0.972 - ETA: 2s - loss: 0.0901 - acc: 0.973 - ETA: 2s - loss: 0.0882 - acc: 0.973 - ETA: 2s - loss: 0.0881 - acc: 0.973 - ETA: 2s - loss: 0.0878 - acc: 0.973 - ETA: 2s - loss: 0.0888 - acc: 0.973 - ETA: 2s - loss: 0.0913 - acc: 0.972 - ETA: 2s - loss: 0.0897 - acc: 0.973 - ETA: 2s - loss: 0.0880 - acc: 0.973 - ETA: 2s - loss: 0.0872 - acc: 0.973 - ETA: 2s - loss: 0.0861 - acc: 0.974 - ETA: 2s - loss: 0.0879 - acc: 0.973 - ETA: 2s - loss: 0.0921 - acc: 0.972 - ETA: 2s - loss: 0.0908 - acc: 0.973 - ETA: 2s - loss: 0.0950 - acc: 0.972 - ETA: 2s - loss: 0.0961 - acc: 0.972 - ETA: 1s - loss: 0.0959 - acc: 0.972 - ETA: 1s - loss: 0.0966 - acc: 0.971 - ETA: 1s - loss: 0.0966 - acc: 0.971 - ETA: 1s - loss: 0.0975 - acc: 0.970 - ETA: 1s - loss: 0.0980 - acc: 0.970 - ETA: 1s - loss: 0.0965 - acc: 0.971 - ETA: 1s - loss: 0.0954 - acc: 0.971 - ETA: 1s - loss: 0.0955 - acc: 0.971 - ETA: 1s - loss: 0.0959 - acc: 0.970 - ETA: 1s - loss: 0.0963 - acc: 0.970 - ETA: 1s - loss: 0.1005 - acc: 0.969 - ETA: 1s - loss: 0.1015 - acc: 0.969 - ETA: 1s - loss: 0.1025 - acc: 0.969 - ETA: 1s - loss: 0.1019 - acc: 0.969 - ETA: 1s - loss: 0.1040 - acc: 0.968 - ETA: 1s - loss: 0.1041 - acc: 0.968 - ETA: 1s - loss: 0.1071 - acc: 0.967 - ETA: 0s - loss: 0.1071 - acc: 0.968 - ETA: 0s - loss: 0.1063 - acc: 0.968 - ETA: 0s - loss: 0.1055 - acc: 0.968 - ETA: 0s - loss: 0.1063 - acc: 0.968 - ETA: 0s - loss: 0.1053 - acc: 0.968 - ETA: 0s - loss: 0.1055 - acc: 0.968 - ETA: 0s - loss: 0.1052 - acc: 0.968 - ETA: 0s - loss: 0.1072 - acc: 0.968 - ETA: 0s - loss: 0.1062 - acc: 0.968 - ETA: 0s - loss: 0.1070 - acc: 0.967 - ETA: 0s - loss: 0.1071 - acc: 0.967 - ETA: 0s - loss: 0.1076 - acc: 0.967 - ETA: 0s - loss: 0.1084 - acc: 0.967 - ETA: 0s - loss: 0.1074 - acc: 0.967 - ETA: 0s - loss: 0.1071 - acc: 0.967 - ETA: 0s - loss: 0.1072 - acc: 0.967 - ETA: 0s - loss: 0.1071 - acc: 0.9672Epoch 00013: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.1075 - acc: 0.9672 - val_loss: 0.6223 - val_acc: 0.8527\n",
      "Epoch 15/20\n",
      "6600/6680 [============================>.] - ETA: 7s - loss: 0.1607 - acc: 0.950 - ETA: 5s - loss: 0.0673 - acc: 0.980 - ETA: 4s - loss: 0.0500 - acc: 0.983 - ETA: 4s - loss: 0.0458 - acc: 0.984 - ETA: 4s - loss: 0.0507 - acc: 0.983 - ETA: 4s - loss: 0.0512 - acc: 0.981 - ETA: 4s - loss: 0.0763 - acc: 0.975 - ETA: 4s - loss: 0.0793 - acc: 0.972 - ETA: 3s - loss: 0.0932 - acc: 0.969 - ETA: 3s - loss: 0.0856 - acc: 0.972 - ETA: 3s - loss: 0.0864 - acc: 0.971 - ETA: 3s - loss: 0.0870 - acc: 0.970 - ETA: 3s - loss: 0.0861 - acc: 0.970 - ETA: 3s - loss: 0.0907 - acc: 0.970 - ETA: 3s - loss: 0.0873 - acc: 0.971 - ETA: 3s - loss: 0.0857 - acc: 0.972 - ETA: 3s - loss: 0.0845 - acc: 0.973 - ETA: 3s - loss: 0.0934 - acc: 0.973 - ETA: 3s - loss: 0.0899 - acc: 0.974 - ETA: 3s - loss: 0.0866 - acc: 0.975 - ETA: 3s - loss: 0.0898 - acc: 0.974 - ETA: 3s - loss: 0.0928 - acc: 0.973 - ETA: 3s - loss: 0.0980 - acc: 0.971 - ETA: 2s - loss: 0.0960 - acc: 0.971 - ETA: 2s - loss: 0.0980 - acc: 0.970 - ETA: 2s - loss: 0.1016 - acc: 0.970 - ETA: 2s - loss: 0.1027 - acc: 0.970 - ETA: 2s - loss: 0.1013 - acc: 0.970 - ETA: 2s - loss: 0.1032 - acc: 0.970 - ETA: 2s - loss: 0.1059 - acc: 0.970 - ETA: 2s - loss: 0.1080 - acc: 0.970 - ETA: 2s - loss: 0.1103 - acc: 0.970 - ETA: 2s - loss: 0.1090 - acc: 0.970 - ETA: 2s - loss: 0.1063 - acc: 0.971 - ETA: 2s - loss: 0.1054 - acc: 0.971 - ETA: 2s - loss: 0.1060 - acc: 0.970 - ETA: 2s - loss: 0.1052 - acc: 0.970 - ETA: 2s - loss: 0.1045 - acc: 0.970 - ETA: 2s - loss: 0.1060 - acc: 0.969 - ETA: 2s - loss: 0.1060 - acc: 0.969 - ETA: 2s - loss: 0.1046 - acc: 0.969 - ETA: 2s - loss: 0.1059 - acc: 0.969 - ETA: 1s - loss: 0.1040 - acc: 0.969 - ETA: 1s - loss: 0.1029 - acc: 0.970 - ETA: 1s - loss: 0.1013 - acc: 0.970 - ETA: 1s - loss: 0.1038 - acc: 0.970 - ETA: 1s - loss: 0.1044 - acc: 0.970 - ETA: 1s - loss: 0.1047 - acc: 0.969 - ETA: 1s - loss: 0.1049 - acc: 0.968 - ETA: 1s - loss: 0.1039 - acc: 0.968 - ETA: 1s - loss: 0.1027 - acc: 0.969 - ETA: 1s - loss: 0.1023 - acc: 0.969 - ETA: 1s - loss: 0.1011 - acc: 0.969 - ETA: 1s - loss: 0.1013 - acc: 0.969 - ETA: 1s - loss: 0.1019 - acc: 0.968 - ETA: 1s - loss: 0.1013 - acc: 0.969 - ETA: 1s - loss: 0.1019 - acc: 0.968 - ETA: 1s - loss: 0.1043 - acc: 0.968 - ETA: 1s - loss: 0.1050 - acc: 0.968 - ETA: 1s - loss: 0.1039 - acc: 0.968 - ETA: 0s - loss: 0.1031 - acc: 0.968 - ETA: 0s - loss: 0.1037 - acc: 0.968 - ETA: 0s - loss: 0.1029 - acc: 0.968 - ETA: 0s - loss: 0.1020 - acc: 0.969 - ETA: 0s - loss: 0.1017 - acc: 0.969 - ETA: 0s - loss: 0.1012 - acc: 0.969 - ETA: 0s - loss: 0.1001 - acc: 0.969 - ETA: 0s - loss: 0.1024 - acc: 0.969 - ETA: 0s - loss: 0.1014 - acc: 0.970 - ETA: 0s - loss: 0.1006 - acc: 0.970 - ETA: 0s - loss: 0.1001 - acc: 0.970 - ETA: 0s - loss: 0.0992 - acc: 0.970 - ETA: 0s - loss: 0.0983 - acc: 0.970 - ETA: 0s - loss: 0.0990 - acc: 0.970 - ETA: 0s - loss: 0.1003 - acc: 0.969 - ETA: 0s - loss: 0.0994 - acc: 0.970 - ETA: 0s - loss: 0.0984 - acc: 0.970 - ETA: 0s - loss: 0.0990 - acc: 0.9705Epoch 00014: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.1002 - acc: 0.9702 - val_loss: 0.6457 - val_acc: 0.8431\n",
      "Epoch 16/20\n",
      "6600/6680 [============================>.] - ETA: 3s - loss: 0.0425 - acc: 1.000 - ETA: 4s - loss: 0.0357 - acc: 0.980 - ETA: 4s - loss: 0.0401 - acc: 0.983 - ETA: 4s - loss: 0.0991 - acc: 0.975 - ETA: 3s - loss: 0.1293 - acc: 0.973 - ETA: 3s - loss: 0.1269 - acc: 0.972 - ETA: 3s - loss: 0.1223 - acc: 0.969 - ETA: 3s - loss: 0.1160 - acc: 0.969 - ETA: 3s - loss: 0.1117 - acc: 0.968 - ETA: 3s - loss: 0.1062 - acc: 0.969 - ETA: 3s - loss: 0.1036 - acc: 0.968 - ETA: 3s - loss: 0.1008 - acc: 0.968 - ETA: 3s - loss: 0.1022 - acc: 0.968 - ETA: 3s - loss: 0.1018 - acc: 0.968 - ETA: 3s - loss: 0.1031 - acc: 0.968 - ETA: 3s - loss: 0.1005 - acc: 0.968 - ETA: 3s - loss: 0.0962 - acc: 0.970 - ETA: 3s - loss: 0.0942 - acc: 0.970 - ETA: 3s - loss: 0.0905 - acc: 0.971 - ETA: 3s - loss: 0.0868 - acc: 0.972 - ETA: 3s - loss: 0.0841 - acc: 0.973 - ETA: 3s - loss: 0.0823 - acc: 0.973 - ETA: 2s - loss: 0.0804 - acc: 0.974 - ETA: 2s - loss: 0.0795 - acc: 0.974 - ETA: 2s - loss: 0.0780 - acc: 0.974 - ETA: 2s - loss: 0.0777 - acc: 0.974 - ETA: 2s - loss: 0.0770 - acc: 0.974 - ETA: 2s - loss: 0.0760 - acc: 0.974 - ETA: 2s - loss: 0.0789 - acc: 0.974 - ETA: 2s - loss: 0.0777 - acc: 0.975 - ETA: 2s - loss: 0.0819 - acc: 0.974 - ETA: 2s - loss: 0.0809 - acc: 0.974 - ETA: 2s - loss: 0.0800 - acc: 0.975 - ETA: 2s - loss: 0.0846 - acc: 0.974 - ETA: 2s - loss: 0.0836 - acc: 0.975 - ETA: 2s - loss: 0.0864 - acc: 0.974 - ETA: 2s - loss: 0.0894 - acc: 0.974 - ETA: 2s - loss: 0.0883 - acc: 0.974 - ETA: 2s - loss: 0.0901 - acc: 0.974 - ETA: 2s - loss: 0.0894 - acc: 0.974 - ETA: 2s - loss: 0.0880 - acc: 0.974 - ETA: 1s - loss: 0.0871 - acc: 0.974 - ETA: 1s - loss: 0.0865 - acc: 0.973 - ETA: 1s - loss: 0.0853 - acc: 0.974 - ETA: 1s - loss: 0.0863 - acc: 0.974 - ETA: 1s - loss: 0.0884 - acc: 0.973 - ETA: 1s - loss: 0.0893 - acc: 0.973 - ETA: 1s - loss: 0.0886 - acc: 0.973 - ETA: 1s - loss: 0.0932 - acc: 0.972 - ETA: 1s - loss: 0.0931 - acc: 0.972 - ETA: 1s - loss: 0.0923 - acc: 0.972 - ETA: 1s - loss: 0.0915 - acc: 0.972 - ETA: 1s - loss: 0.0912 - acc: 0.972 - ETA: 1s - loss: 0.0931 - acc: 0.972 - ETA: 1s - loss: 0.0943 - acc: 0.972 - ETA: 1s - loss: 0.0934 - acc: 0.972 - ETA: 1s - loss: 0.0927 - acc: 0.972 - ETA: 1s - loss: 0.0930 - acc: 0.972 - ETA: 1s - loss: 0.0929 - acc: 0.971 - ETA: 1s - loss: 0.0917 - acc: 0.972 - ETA: 0s - loss: 0.0918 - acc: 0.971 - ETA: 0s - loss: 0.0908 - acc: 0.971 - ETA: 0s - loss: 0.0910 - acc: 0.971 - ETA: 0s - loss: 0.0912 - acc: 0.971 - ETA: 0s - loss: 0.0909 - acc: 0.971 - ETA: 0s - loss: 0.0932 - acc: 0.971 - ETA: 0s - loss: 0.0952 - acc: 0.971 - ETA: 0s - loss: 0.0942 - acc: 0.971 - ETA: 0s - loss: 0.0931 - acc: 0.972 - ETA: 0s - loss: 0.0925 - acc: 0.972 - ETA: 0s - loss: 0.0944 - acc: 0.971 - ETA: 0s - loss: 0.0937 - acc: 0.972 - ETA: 0s - loss: 0.0943 - acc: 0.971 - ETA: 0s - loss: 0.0932 - acc: 0.972 - ETA: 0s - loss: 0.0923 - acc: 0.972 - ETA: 0s - loss: 0.0918 - acc: 0.972 - ETA: 0s - loss: 0.0922 - acc: 0.972 - ETA: 0s - loss: 0.0931 - acc: 0.9721Epoch 00015: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0937 - acc: 0.9719 - val_loss: 0.6427 - val_acc: 0.8563\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6620/6680 [============================>.] - ETA: 4s - loss: 0.0090 - acc: 1.000 - ETA: 4s - loss: 0.1261 - acc: 0.970 - ETA: 4s - loss: 0.1022 - acc: 0.972 - ETA: 4s - loss: 0.0972 - acc: 0.973 - ETA: 4s - loss: 0.0772 - acc: 0.979 - ETA: 4s - loss: 0.0655 - acc: 0.983 - ETA: 4s - loss: 0.0676 - acc: 0.980 - ETA: 4s - loss: 0.0752 - acc: 0.979 - ETA: 3s - loss: 0.0709 - acc: 0.979 - ETA: 3s - loss: 0.0801 - acc: 0.974 - ETA: 3s - loss: 0.0827 - acc: 0.973 - ETA: 3s - loss: 0.0777 - acc: 0.976 - ETA: 3s - loss: 0.0815 - acc: 0.973 - ETA: 3s - loss: 0.0792 - acc: 0.974 - ETA: 3s - loss: 0.0780 - acc: 0.974 - ETA: 3s - loss: 0.0824 - acc: 0.973 - ETA: 3s - loss: 0.0795 - acc: 0.974 - ETA: 3s - loss: 0.0764 - acc: 0.975 - ETA: 3s - loss: 0.0779 - acc: 0.975 - ETA: 3s - loss: 0.0758 - acc: 0.975 - ETA: 3s - loss: 0.0776 - acc: 0.974 - ETA: 3s - loss: 0.0904 - acc: 0.973 - ETA: 3s - loss: 0.0879 - acc: 0.974 - ETA: 3s - loss: 0.0863 - acc: 0.974 - ETA: 3s - loss: 0.0840 - acc: 0.975 - ETA: 2s - loss: 0.0814 - acc: 0.975 - ETA: 2s - loss: 0.0795 - acc: 0.976 - ETA: 2s - loss: 0.0774 - acc: 0.977 - ETA: 2s - loss: 0.0775 - acc: 0.977 - ETA: 2s - loss: 0.0770 - acc: 0.977 - ETA: 2s - loss: 0.0819 - acc: 0.975 - ETA: 2s - loss: 0.0829 - acc: 0.974 - ETA: 2s - loss: 0.0836 - acc: 0.974 - ETA: 2s - loss: 0.0874 - acc: 0.974 - ETA: 2s - loss: 0.0860 - acc: 0.974 - ETA: 2s - loss: 0.0894 - acc: 0.974 - ETA: 2s - loss: 0.0894 - acc: 0.973 - ETA: 2s - loss: 0.0880 - acc: 0.974 - ETA: 2s - loss: 0.0869 - acc: 0.974 - ETA: 2s - loss: 0.0855 - acc: 0.975 - ETA: 2s - loss: 0.0844 - acc: 0.975 - ETA: 2s - loss: 0.0834 - acc: 0.975 - ETA: 2s - loss: 0.0871 - acc: 0.975 - ETA: 2s - loss: 0.0868 - acc: 0.975 - ETA: 1s - loss: 0.0878 - acc: 0.974 - ETA: 1s - loss: 0.0872 - acc: 0.975 - ETA: 1s - loss: 0.0862 - acc: 0.975 - ETA: 1s - loss: 0.0860 - acc: 0.975 - ETA: 1s - loss: 0.0871 - acc: 0.974 - ETA: 1s - loss: 0.0879 - acc: 0.974 - ETA: 1s - loss: 0.0871 - acc: 0.974 - ETA: 1s - loss: 0.0875 - acc: 0.973 - ETA: 1s - loss: 0.0863 - acc: 0.974 - ETA: 1s - loss: 0.0855 - acc: 0.974 - ETA: 1s - loss: 0.0886 - acc: 0.974 - ETA: 1s - loss: 0.0897 - acc: 0.974 - ETA: 1s - loss: 0.0888 - acc: 0.974 - ETA: 1s - loss: 0.0878 - acc: 0.974 - ETA: 1s - loss: 0.0871 - acc: 0.974 - ETA: 1s - loss: 0.0880 - acc: 0.974 - ETA: 1s - loss: 0.0868 - acc: 0.974 - ETA: 1s - loss: 0.0857 - acc: 0.975 - ETA: 1s - loss: 0.0845 - acc: 0.975 - ETA: 1s - loss: 0.0857 - acc: 0.975 - ETA: 0s - loss: 0.0848 - acc: 0.975 - ETA: 0s - loss: 0.0860 - acc: 0.975 - ETA: 0s - loss: 0.0862 - acc: 0.975 - ETA: 0s - loss: 0.0868 - acc: 0.974 - ETA: 0s - loss: 0.0860 - acc: 0.975 - ETA: 0s - loss: 0.0855 - acc: 0.975 - ETA: 0s - loss: 0.0854 - acc: 0.974 - ETA: 0s - loss: 0.0848 - acc: 0.975 - ETA: 0s - loss: 0.0842 - acc: 0.975 - ETA: 0s - loss: 0.0831 - acc: 0.975 - ETA: 0s - loss: 0.0828 - acc: 0.975 - ETA: 0s - loss: 0.0854 - acc: 0.975 - ETA: 0s - loss: 0.0865 - acc: 0.975 - ETA: 0s - loss: 0.0859 - acc: 0.975 - ETA: 0s - loss: 0.0868 - acc: 0.975 - ETA: 0s - loss: 0.0877 - acc: 0.974 - ETA: 0s - loss: 0.0873 - acc: 0.9749Epoch 00016: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0879 - acc: 0.9746 - val_loss: 0.6428 - val_acc: 0.8623\n",
      "Epoch 18/20\n",
      "6620/6680 [============================>.] - ETA: 3s - loss: 0.0153 - acc: 1.000 - ETA: 4s - loss: 0.0697 - acc: 0.970 - ETA: 4s - loss: 0.0606 - acc: 0.972 - ETA: 4s - loss: 0.0657 - acc: 0.973 - ETA: 4s - loss: 0.0785 - acc: 0.973 - ETA: 4s - loss: 0.0859 - acc: 0.971 - ETA: 4s - loss: 0.0779 - acc: 0.972 - ETA: 4s - loss: 0.0785 - acc: 0.972 - ETA: 4s - loss: 0.0788 - acc: 0.974 - ETA: 4s - loss: 0.0709 - acc: 0.977 - ETA: 4s - loss: 0.0647 - acc: 0.979 - ETA: 3s - loss: 0.0648 - acc: 0.979 - ETA: 3s - loss: 0.0726 - acc: 0.980 - ETA: 3s - loss: 0.0886 - acc: 0.980 - ETA: 3s - loss: 0.0879 - acc: 0.980 - ETA: 3s - loss: 0.0939 - acc: 0.979 - ETA: 3s - loss: 0.0891 - acc: 0.980 - ETA: 3s - loss: 0.0908 - acc: 0.979 - ETA: 3s - loss: 0.0909 - acc: 0.979 - ETA: 3s - loss: 0.0885 - acc: 0.980 - ETA: 3s - loss: 0.0851 - acc: 0.981 - ETA: 3s - loss: 0.0819 - acc: 0.981 - ETA: 3s - loss: 0.0827 - acc: 0.982 - ETA: 3s - loss: 0.0820 - acc: 0.981 - ETA: 3s - loss: 0.0833 - acc: 0.980 - ETA: 3s - loss: 0.0855 - acc: 0.978 - ETA: 3s - loss: 0.0862 - acc: 0.978 - ETA: 3s - loss: 0.0855 - acc: 0.979 - ETA: 2s - loss: 0.0842 - acc: 0.978 - ETA: 2s - loss: 0.0826 - acc: 0.979 - ETA: 2s - loss: 0.0808 - acc: 0.979 - ETA: 2s - loss: 0.0788 - acc: 0.980 - ETA: 2s - loss: 0.0780 - acc: 0.980 - ETA: 2s - loss: 0.0769 - acc: 0.980 - ETA: 2s - loss: 0.0803 - acc: 0.979 - ETA: 2s - loss: 0.0805 - acc: 0.979 - ETA: 2s - loss: 0.0815 - acc: 0.979 - ETA: 2s - loss: 0.0812 - acc: 0.979 - ETA: 2s - loss: 0.0831 - acc: 0.979 - ETA: 2s - loss: 0.0822 - acc: 0.979 - ETA: 2s - loss: 0.0828 - acc: 0.978 - ETA: 2s - loss: 0.0819 - acc: 0.979 - ETA: 2s - loss: 0.0822 - acc: 0.978 - ETA: 2s - loss: 0.0813 - acc: 0.978 - ETA: 2s - loss: 0.0821 - acc: 0.978 - ETA: 2s - loss: 0.0806 - acc: 0.978 - ETA: 1s - loss: 0.0805 - acc: 0.978 - ETA: 1s - loss: 0.0808 - acc: 0.978 - ETA: 1s - loss: 0.0800 - acc: 0.977 - ETA: 1s - loss: 0.0838 - acc: 0.976 - ETA: 1s - loss: 0.0858 - acc: 0.976 - ETA: 1s - loss: 0.0852 - acc: 0.975 - ETA: 1s - loss: 0.0850 - acc: 0.975 - ETA: 1s - loss: 0.0844 - acc: 0.975 - ETA: 1s - loss: 0.0833 - acc: 0.976 - ETA: 1s - loss: 0.0831 - acc: 0.976 - ETA: 1s - loss: 0.0837 - acc: 0.975 - ETA: 1s - loss: 0.0840 - acc: 0.975 - ETA: 1s - loss: 0.0843 - acc: 0.975 - ETA: 1s - loss: 0.0885 - acc: 0.974 - ETA: 1s - loss: 0.0871 - acc: 0.975 - ETA: 1s - loss: 0.0858 - acc: 0.975 - ETA: 1s - loss: 0.0856 - acc: 0.975 - ETA: 1s - loss: 0.0848 - acc: 0.976 - ETA: 0s - loss: 0.0851 - acc: 0.976 - ETA: 0s - loss: 0.0846 - acc: 0.976 - ETA: 0s - loss: 0.0841 - acc: 0.976 - ETA: 0s - loss: 0.0842 - acc: 0.975 - ETA: 0s - loss: 0.0848 - acc: 0.975 - ETA: 0s - loss: 0.0837 - acc: 0.976 - ETA: 0s - loss: 0.0837 - acc: 0.976 - ETA: 0s - loss: 0.0837 - acc: 0.975 - ETA: 0s - loss: 0.0849 - acc: 0.975 - ETA: 0s - loss: 0.0847 - acc: 0.975 - ETA: 0s - loss: 0.0838 - acc: 0.976 - ETA: 0s - loss: 0.0842 - acc: 0.976 - ETA: 0s - loss: 0.0838 - acc: 0.976 - ETA: 0s - loss: 0.0859 - acc: 0.975 - ETA: 0s - loss: 0.0851 - acc: 0.976 - ETA: 0s - loss: 0.0848 - acc: 0.976 - ETA: 0s - loss: 0.0852 - acc: 0.9757Epoch 00017: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0852 - acc: 0.9756 - val_loss: 0.6738 - val_acc: 0.8575\n",
      "Epoch 19/20\n",
      "6600/6680 [============================>.] - ETA: 3s - loss: 0.0147 - acc: 1.000 - ETA: 4s - loss: 0.0215 - acc: 1.000 - ETA: 4s - loss: 0.0317 - acc: 0.994 - ETA: 4s - loss: 0.0330 - acc: 0.992 - ETA: 4s - loss: 0.0502 - acc: 0.988 - ETA: 4s - loss: 0.0474 - acc: 0.988 - ETA: 4s - loss: 0.0520 - acc: 0.986 - ETA: 4s - loss: 0.0466 - acc: 0.987 - ETA: 4s - loss: 0.0432 - acc: 0.989 - ETA: 4s - loss: 0.0473 - acc: 0.987 - ETA: 4s - loss: 0.0442 - acc: 0.989 - ETA: 4s - loss: 0.0408 - acc: 0.990 - ETA: 4s - loss: 0.0407 - acc: 0.989 - ETA: 4s - loss: 0.0413 - acc: 0.989 - ETA: 3s - loss: 0.0424 - acc: 0.988 - ETA: 3s - loss: 0.0405 - acc: 0.989 - ETA: 3s - loss: 0.0440 - acc: 0.986 - ETA: 3s - loss: 0.0450 - acc: 0.986 - ETA: 3s - loss: 0.0478 - acc: 0.986 - ETA: 3s - loss: 0.0553 - acc: 0.985 - ETA: 3s - loss: 0.0547 - acc: 0.985 - ETA: 3s - loss: 0.0576 - acc: 0.983 - ETA: 3s - loss: 0.0597 - acc: 0.983 - ETA: 3s - loss: 0.0591 - acc: 0.984 - ETA: 3s - loss: 0.0578 - acc: 0.984 - ETA: 3s - loss: 0.0567 - acc: 0.985 - ETA: 3s - loss: 0.0554 - acc: 0.986 - ETA: 3s - loss: 0.0548 - acc: 0.985 - ETA: 3s - loss: 0.0541 - acc: 0.985 - ETA: 3s - loss: 0.0543 - acc: 0.985 - ETA: 2s - loss: 0.0557 - acc: 0.985 - ETA: 2s - loss: 0.0553 - acc: 0.985 - ETA: 2s - loss: 0.0551 - acc: 0.985 - ETA: 2s - loss: 0.0582 - acc: 0.984 - ETA: 2s - loss: 0.0583 - acc: 0.983 - ETA: 2s - loss: 0.0575 - acc: 0.983 - ETA: 2s - loss: 0.0574 - acc: 0.983 - ETA: 2s - loss: 0.0628 - acc: 0.982 - ETA: 2s - loss: 0.0624 - acc: 0.981 - ETA: 2s - loss: 0.0625 - acc: 0.981 - ETA: 2s - loss: 0.0626 - acc: 0.981 - ETA: 2s - loss: 0.0621 - acc: 0.981 - ETA: 2s - loss: 0.0645 - acc: 0.981 - ETA: 2s - loss: 0.0666 - acc: 0.980 - ETA: 2s - loss: 0.0661 - acc: 0.981 - ETA: 2s - loss: 0.0650 - acc: 0.981 - ETA: 2s - loss: 0.0667 - acc: 0.981 - ETA: 1s - loss: 0.0668 - acc: 0.981 - ETA: 1s - loss: 0.0659 - acc: 0.981 - ETA: 1s - loss: 0.0649 - acc: 0.982 - ETA: 1s - loss: 0.0655 - acc: 0.981 - ETA: 1s - loss: 0.0651 - acc: 0.981 - ETA: 1s - loss: 0.0683 - acc: 0.981 - ETA: 1s - loss: 0.0677 - acc: 0.981 - ETA: 1s - loss: 0.0684 - acc: 0.980 - ETA: 1s - loss: 0.0675 - acc: 0.980 - ETA: 1s - loss: 0.0703 - acc: 0.980 - ETA: 1s - loss: 0.0710 - acc: 0.980 - ETA: 1s - loss: 0.0721 - acc: 0.980 - ETA: 1s - loss: 0.0716 - acc: 0.980 - ETA: 1s - loss: 0.0724 - acc: 0.980 - ETA: 1s - loss: 0.0763 - acc: 0.979 - ETA: 1s - loss: 0.0765 - acc: 0.979 - ETA: 0s - loss: 0.0758 - acc: 0.979 - ETA: 0s - loss: 0.0765 - acc: 0.979 - ETA: 0s - loss: 0.0772 - acc: 0.979 - ETA: 0s - loss: 0.0777 - acc: 0.978 - ETA: 0s - loss: 0.0772 - acc: 0.979 - ETA: 0s - loss: 0.0797 - acc: 0.978 - ETA: 0s - loss: 0.0786 - acc: 0.979 - ETA: 0s - loss: 0.0793 - acc: 0.978 - ETA: 0s - loss: 0.0783 - acc: 0.978 - ETA: 0s - loss: 0.0786 - acc: 0.978 - ETA: 0s - loss: 0.0778 - acc: 0.978 - ETA: 0s - loss: 0.0769 - acc: 0.979 - ETA: 0s - loss: 0.0770 - acc: 0.978 - ETA: 0s - loss: 0.0767 - acc: 0.978 - ETA: 0s - loss: 0.0761 - acc: 0.979 - ETA: 0s - loss: 0.0766 - acc: 0.979 - ETA: 0s - loss: 0.0769 - acc: 0.9788Epoch 00018: val_loss did not improve\n",
      "6680/6680 [==============================] - 4s - loss: 0.0778 - acc: 0.9786 - val_loss: 0.6782 - val_acc: 0.8527\n",
      "Epoch 20/20\n",
      "6620/6680 [============================>.] - ETA: 3s - loss: 0.1281 - acc: 0.900 - ETA: 4s - loss: 0.0282 - acc: 0.980 - ETA: 4s - loss: 0.0222 - acc: 0.988 - ETA: 4s - loss: 0.0321 - acc: 0.988 - ETA: 4s - loss: 0.0401 - acc: 0.988 - ETA: 4s - loss: 0.0481 - acc: 0.988 - ETA: 4s - loss: 0.0483 - acc: 0.984 - ETA: 4s - loss: 0.0454 - acc: 0.984 - ETA: 4s - loss: 0.0560 - acc: 0.981 - ETA: 4s - loss: 0.0508 - acc: 0.983 - ETA: 4s - loss: 0.0478 - acc: 0.984 - ETA: 4s - loss: 0.0526 - acc: 0.983 - ETA: 4s - loss: 0.0567 - acc: 0.980 - ETA: 3s - loss: 0.0556 - acc: 0.980 - ETA: 3s - loss: 0.0546 - acc: 0.980 - ETA: 3s - loss: 0.0518 - acc: 0.982 - ETA: 3s - loss: 0.0497 - acc: 0.982 - ETA: 3s - loss: 0.0511 - acc: 0.981 - ETA: 3s - loss: 0.0525 - acc: 0.980 - ETA: 3s - loss: 0.0503 - acc: 0.981 - ETA: 3s - loss: 0.0536 - acc: 0.980 - ETA: 3s - loss: 0.0531 - acc: 0.981 - ETA: 3s - loss: 0.0522 - acc: 0.980 - ETA: 3s - loss: 0.0505 - acc: 0.981 - ETA: 3s - loss: 0.0503 - acc: 0.981 - ETA: 3s - loss: 0.0512 - acc: 0.981 - ETA: 3s - loss: 0.0513 - acc: 0.981 - ETA: 3s - loss: 0.0510 - acc: 0.981 - ETA: 3s - loss: 0.0525 - acc: 0.981 - ETA: 3s - loss: 0.0515 - acc: 0.981 - ETA: 3s - loss: 0.0518 - acc: 0.981 - ETA: 2s - loss: 0.0508 - acc: 0.981 - ETA: 2s - loss: 0.0495 - acc: 0.982 - ETA: 2s - loss: 0.0483 - acc: 0.983 - ETA: 2s - loss: 0.0512 - acc: 0.982 - ETA: 2s - loss: 0.0528 - acc: 0.981 - ETA: 2s - loss: 0.0531 - acc: 0.981 - ETA: 2s - loss: 0.0522 - acc: 0.982 - ETA: 2s - loss: 0.0537 - acc: 0.981 - ETA: 2s - loss: 0.0527 - acc: 0.982 - ETA: 2s - loss: 0.0575 - acc: 0.981 - ETA: 2s - loss: 0.0569 - acc: 0.981 - ETA: 2s - loss: 0.0596 - acc: 0.981 - ETA: 2s - loss: 0.0624 - acc: 0.980 - ETA: 2s - loss: 0.0618 - acc: 0.980 - ETA: 2s - loss: 0.0608 - acc: 0.981 - ETA: 2s - loss: 0.0605 - acc: 0.981 - ETA: 2s - loss: 0.0607 - acc: 0.980 - ETA: 2s - loss: 0.0596 - acc: 0.981 - ETA: 1s - loss: 0.0586 - acc: 0.981 - ETA: 1s - loss: 0.0612 - acc: 0.981 - ETA: 1s - loss: 0.0651 - acc: 0.980 - ETA: 1s - loss: 0.0675 - acc: 0.980 - ETA: 1s - loss: 0.0665 - acc: 0.980 - ETA: 1s - loss: 0.0661 - acc: 0.980 - ETA: 1s - loss: 0.0650 - acc: 0.980 - ETA: 1s - loss: 0.0661 - acc: 0.980 - ETA: 1s - loss: 0.0654 - acc: 0.980 - ETA: 1s - loss: 0.0655 - acc: 0.980 - ETA: 1s - loss: 0.0648 - acc: 0.980 - ETA: 1s - loss: 0.0643 - acc: 0.980 - ETA: 1s - loss: 0.0639 - acc: 0.980 - ETA: 1s - loss: 0.0693 - acc: 0.980 - ETA: 1s - loss: 0.0693 - acc: 0.980 - ETA: 1s - loss: 0.0692 - acc: 0.980 - ETA: 1s - loss: 0.0696 - acc: 0.980 - ETA: 0s - loss: 0.0693 - acc: 0.980 - ETA: 0s - loss: 0.0693 - acc: 0.980 - ETA: 0s - loss: 0.0697 - acc: 0.980 - ETA: 0s - loss: 0.0688 - acc: 0.980 - ETA: 0s - loss: 0.0698 - acc: 0.980 - ETA: 0s - loss: 0.0709 - acc: 0.980 - ETA: 0s - loss: 0.0708 - acc: 0.980 - ETA: 0s - loss: 0.0702 - acc: 0.980 - ETA: 0s - loss: 0.0721 - acc: 0.980 - ETA: 0s - loss: 0.0719 - acc: 0.979 - ETA: 0s - loss: 0.0719 - acc: 0.979 - ETA: 0s - loss: 0.0738 - acc: 0.979 - ETA: 0s - loss: 0.0742 - acc: 0.979 - ETA: 0s - loss: 0.0734 - acc: 0.979 - ETA: 0s - loss: 0.0737 - acc: 0.979 - ETA: 0s - loss: 0.0739 - acc: 0.979 - ETA: 0s - loss: 0.0746 - acc: 0.979 - ETA: 0s - loss: 0.0741 - acc: 0.9792Epoch 00019: val_loss did not improve\n",
      "6680/6680 [==============================] - 5s - loss: 0.0742 - acc: 0.9790 - val_loss: 0.6751 - val_acc: 0.8539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20ea3d87518>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO: Train the model.\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.Xception.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "Xception_model.fit(train_Xception, train_targets, \n",
    "          validation_data=(valid_Xception, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Load the model weights with the best validation loss.\n",
    "Xception_model.load_weights('saved_models/weights.best.Xception.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 83.3732%\n"
     ]
    }
   ],
   "source": [
    "### TODO: Calculate classification accuracy on the test dataset.\n",
    "Xception_predictions = [np.argmax(Xception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Xception]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(Xception_predictions)==np.argmax(test_targets, axis=1))/len(Xception_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Predict Dog Breed with the Model\n",
    "\n",
    "Write a function that takes an image path as input and returns the dog breed (`Affenpinscher`, `Afghan_hound`, etc) that is predicted by your model.  \n",
    "\n",
    "Similar to the analogous function in Step 5, your function should have three steps:\n",
    "1. Extract the bottleneck features corresponding to the chosen CNN model.\n",
    "2. Supply the bottleneck features as input to the model to return the predicted vector.  Note that the argmax of this prediction vector gives the index of the predicted dog breed.\n",
    "3. Use the `dog_names` array defined in Step 0 of this notebook to return the corresponding breed.\n",
    "\n",
    "The functions to extract the bottleneck features can be found in `extract_bottleneck_features.py`, and they have been imported in an earlier code cell.  To obtain the bottleneck features corresponding to your chosen CNN architecture, you need to use the function\n",
    "\n",
    "    extract_{network}\n",
    "    \n",
    "where `{network}`, in the above filename, should be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Write a function that takes a path to an image as input\n",
    "### and returns the dog breed that is predicted by the model.\n",
    "def Xception_predict_breed(img_path):\n",
    "    bottleneck_feature = extract_Xception(path_to_tensor(img_path))\n",
    "    predicted_vector = Xception_model.predict(bottleneck_feature)\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step6'></a>\n",
    "## Step 6: Write your Algorithm\n",
    "\n",
    "Write an algorithm that accepts a file path to an image and first determines whether the image contains a human, dog, or neither.  Then,\n",
    "- if a __dog__ is detected in the image, return the predicted breed.\n",
    "- if a __human__ is detected in the image, return the resembling dog breed.\n",
    "- if __neither__ is detected in the image, provide output that indicates an error.\n",
    "\n",
    "You are welcome to write your own functions for detecting humans and dogs in images, but feel free to use the `face_detector` and `dog_detector` functions developed above.  You are __required__ to use your CNN from Step 5 to predict dog breed.  \n",
    "\n",
    "Some sample output for our algorithm is provided below, but feel free to design your own user experience!\n",
    "\n",
    "![Sample Human Output](images/sample_human_output.png)\n",
    "\n",
    "\n",
    "### (IMPLEMENTATION) Write your Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Write your algorithm.\n",
    "### Feel free to use as many code cells as needed.\n",
    "from PIL import Image # I commented this out so my notebook doesn't require image to run\n",
    "from matplotlib.pyplot import imshow, show\n",
    "\n",
    "def resembling_dog_breed(img_path):\n",
    "    #image = Image.open(img_path)\n",
    "    #imshow(np.asarray(image))\n",
    "    #show()\n",
    "    \n",
    "    if (dog_detector(img_path)):\n",
    "        breed = Xception_predict_breed(img_path)\n",
    "        print('Detected a dog which looks like a', breed)\n",
    "    elif (face_detector(img_path)):\n",
    "        breed = Xception_predict_breed(img_path)\n",
    "        print('Detected a human who resembles a ', breed)\n",
    "    else:\n",
    "        print('Unable to detect a human nor dog in the input image.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step7'></a>\n",
    "## Step 7: Test Your Algorithm\n",
    "\n",
    "In this section, you will take your new algorithm for a spin!  What kind of dog does the algorithm think that __you__ look like?  If you have a dog, does it predict your dog's breed accurately?  If you have a cat, does it mistakenly think that your cat is a dog?\n",
    "\n",
    "### (IMPLEMENTATION) Test Your Algorithm on Sample Images!\n",
    "\n",
    "Test your algorithm at least six images on your computer.  Feel free to use any images you like.  Use at least two human and two dog images.  \n",
    "\n",
    "__Question 6:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm.\n",
    "\n",
    "__Answer:__ The output is actually better than I expected. It misclassified my dog as a human but is able to identify the correct breed (golden retriever). It is also able to tell when the input is neither a human nor dog.\n",
    "- I could have augmented the training data such that the dog breed prediction may have a higher accuracy. \n",
    "- Evaluate both the probability of the image containing a human or dog to improve the chances to correctly identifying the subject within the input image (e.g. there could be cases where both detectors return true).\n",
    "- Train the network with images containing a human with other animals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected a dog which looks like a Great_pyrenees\n",
      "Detected a dog which looks like a Golden_retriever\n",
      "Detected a dog which looks like a Bichon_frise\n",
      "Unable to detect a human nor dog in the input image.\n",
      "Detected a dog which looks like a Pembroke_welsh_corgi\n",
      "Unable to detect a human nor dog in the input image.\n",
      "Detected a human who resembles a  Smooth_fox_terrier\n",
      "Detected a human who resembles a  Afghan_hound\n"
     ]
    }
   ],
   "source": [
    "## TODO: Execute your algorithm from Step 6 on\n",
    "## at least 6 images on your computer.\n",
    "## Feel free to use as many code cells as needed.\n",
    "img_paths = [\n",
    "    'images/nova_bb.jpg', # golden retriever\n",
    "    'images/nova.jpg', # golden retriever\n",
    "    'images/alex.jpg', # poddle\n",
    "    'images/lamb.jpg', # toy\n",
    "    'images/corgi.jpg', # corgi\n",
    "    'images/peanut.jpg', # cat\n",
    "    'images/lya.jpg', # human\n",
    "    'images/kty.jpg' # human\n",
    "]\n",
    "for path in img_paths:\n",
    "    resembling_dog_breed(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nova_scotia_duck_tolling_retriever'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xception_predict_breed('images/peanut.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "dog-project",
   "language": "python",
   "name": "dog-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
